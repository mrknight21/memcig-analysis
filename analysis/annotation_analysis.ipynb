{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd55fb76-832a-4c1b-a51e-9710d515bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score     # pairwise κ\n",
    "import krippendorff \n",
    "import itertools as it\n",
    "# --- Add these two imports for plotting ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pymongo import MongoClient\n",
    "MONGO_URI = \"mongodb+srv://mrknight21:secret2103@infogain.gb5c5.mongodb.net/?retryWrites=true&w=majority&appName=infogain\"\n",
    "client = MongoClient(MONGO_URI, uuidRepresentation=\"standard\")\n",
    "db = client[\"infogain_annotation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6619ac47-5ba4-4722-a2eb-aacdded45ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Helper: clean annotator names\n",
    "# ----------------------------\n",
    "def _clean_annot_name(s):\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    s = str(s).strip()\n",
    "    s = s.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    # normalize typical variants\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Convert wide episode DF -> long tidy\n",
    "# ----------------------------\n",
    "def episode_wide_to_long(df_episode: pd.DataFrame, episode_id: str, value_name=\"label\"):\n",
    "    \"\"\"\n",
    "    df_episode:\n",
    "        index: utterance_id\n",
    "        columns: annotator names\n",
    "        values: labels\n",
    "    returns long DF with columns: ['episode','utterance_id','annotator','label','item_id']\n",
    "    \"\"\"\n",
    "    # ensure index named\n",
    "    if df_episode.index.name is None:\n",
    "        df_episode = df_episode.copy()\n",
    "        df_episode.index.name = \"utterance_id\"\n",
    "    # clean annotator names (columns)\n",
    "    clean_cols = [ _clean_annot_name(c) for c in df_episode.columns ]\n",
    "    df_tmp = df_episode.copy()\n",
    "    df_tmp.columns = clean_cols\n",
    "\n",
    "    # melt to long\n",
    "    long = df_tmp.reset_index().melt(\n",
    "        id_vars=[\"utterance_id\"],\n",
    "        var_name=\"annotator\",\n",
    "        value_name=value_name\n",
    "    )\n",
    "\n",
    "    # drop rows with no label\n",
    "    long = long[~long[value_name].isna()].copy()\n",
    "\n",
    "    # attach episode + global item_id\n",
    "    long[\"episode\"] = episode_id\n",
    "    long[\"item_id\"] = long[\"episode\"].astype(str) + \"||\" + long[\"utterance_id\"].astype(str)\n",
    "\n",
    "    # remove duplicate coder entries per item (keep first; change policy if needed)\n",
    "    long = long.drop_duplicates(subset=[\"item_id\", \"annotator\"], keep=\"first\")\n",
    "    return long[[\"episode\",\"utterance_id\",\"item_id\",\"annotator\",value_name]]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Concatenate many episodes\n",
    "# ----------------------------\n",
    "def concat_episodes_to_matrix(episode_dfs: dict,  # {\"ep1\": df1, \"ep2\": df2, ...}\n",
    "                              value_name=\"label\",\n",
    "                              make_numeric=True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        long_all: tidy long DF across episodes\n",
    "        R: reliability matrix (items x annotators) with NaN for missing\n",
    "    \"\"\"\n",
    "    longs = []\n",
    "    for ep_id, df_ep in episode_dfs.items():\n",
    "        longs.append(episode_wide_to_long(df_ep, ep_id, value_name=value_name))\n",
    "    long_all = pd.concat(longs, ignore_index=True)\n",
    "\n",
    "    # optional: numeric labels (for ordinal/interval alpha)\n",
    "    if make_numeric:\n",
    "        # try to coerce to numeric but keep non-convertible as NaN (warn user if many are NaN)\n",
    "        long_all[value_name] = pd.to_numeric(long_all[value_name], errors=\"coerce\")\n",
    "\n",
    "    # Pivot to items x annotators matrix\n",
    "    R = long_all.pivot_table(index=\"item_id\", columns=\"annotator\", values=value_name, aggfunc=\"first\")\n",
    "\n",
    "    # sort for reproducibility\n",
    "    R = R.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "    return long_all, R\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Krippendorff's alpha\n",
    "# ----------------------------\n",
    "# Option A: using the 'krippendorff' package (pip install krippendorff)\n",
    "def krippendorff_alpha_from_matrix(R: pd.DataFrame, level=\"ordinal\"):\n",
    "    \"\"\"\n",
    "    R: items x annotators (rows x cols), NaN allowed\n",
    "    level: 'nominal' | 'ordinal' | 'interval' | 'ratio'\n",
    "    The 'krippendorff' package expects coders x items, so transpose.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import krippendorff\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please pip install krippendorff (pip install krippendorff)\")\n",
    "\n",
    "    # krippendorff.alpha expects a 2D array-like with shape (coders, items)\n",
    "    data = np.array(R.T, dtype=float)\n",
    "    return krippendorff.alpha(reliability_data=data, level_of_measurement=level)\n",
    "\n",
    "# Option B: lightweight nominal alpha (fallback) — optional to implement if you can't install packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7039f2-ccf8-41a9-adc0-8300c4da2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_wide(df_or_dict):\n",
    "    if isinstance(df_or_dict, dict):\n",
    "        wide = pd.DataFrame(df_or_dict).sort_index()\n",
    "    else:\n",
    "        wide = df_or_dict.copy()\n",
    "    wide = wide.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return wide\n",
    "\n",
    "def pairwise_confusions(wide_df, labels=None, normalize=None):\n",
    "    df = ensure_wide(wide_df).rename_axis(index=\"item\", columns=\"annotator\")\n",
    "    if labels is None:\n",
    "        vals = df.values.ravel()\n",
    "        labels = sorted(pd.unique(vals[~pd.isna(vals)].astype(int)).tolist())\n",
    "\n",
    "    out = {}\n",
    "    for a, b in it.combinations(df.columns.tolist(), 2):\n",
    "        both = df[[a, b]].dropna()\n",
    "        if both.empty:\n",
    "            continue\n",
    "        cm = pd.crosstab(\n",
    "            both[a].astype(int), both[b].astype(int),\n",
    "            rownames=[f\"{a} rating\"], colnames=[f\"{b} rating\"], dropna=False\n",
    "        ).reindex(index=labels, columns=labels, fill_value=0)\n",
    "\n",
    "        if normalize == \"row\":\n",
    "            cm = cm.div(cm.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "        elif normalize == \"all\":\n",
    "            tot = cm.values.sum()\n",
    "            if tot > 0: cm = cm / tot\n",
    "        out[(a, b)] = cm\n",
    "    return out\n",
    "\n",
    "def pooled_confusion(wide_df, labels=None, normalize=None):\n",
    "    df = ensure_wide(wide_df).rename_axis(index=\"item\", columns=\"annotator\")\n",
    "    if labels is None:\n",
    "        vals = df.values.ravel()\n",
    "        labels = sorted(pd.unique(vals[~pd.isna(vals)].astype(int)).tolist())\n",
    "    idx = {lab: i for i, lab in enumerate(labels)}\n",
    "    M = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        vals = row.dropna().astype(int).tolist()\n",
    "        for x, y in it.combinations(vals, 2):\n",
    "            i, j = idx[x], idx[y]\n",
    "            M[i, j] += 1\n",
    "            M[j, i] += 1\n",
    "\n",
    "    cm = pd.DataFrame(M, index=labels, columns=labels)\n",
    "    if normalize == \"row\":\n",
    "        cm = cm.div(cm.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "    elif normalize == \"all\":\n",
    "        tot = cm.values.sum()\n",
    "        if tot > 0: cm = cm / tot\n",
    "    return cm\n",
    "\n",
    "def majority_vote(row, min_raters=2, tie=\"skip\"):\n",
    "    s = row.dropna().astype(int)\n",
    "    if len(s) < min_raters:\n",
    "        return np.nan\n",
    "    vc = s.value_counts()\n",
    "    if len(vc) > 1 and vc.iloc[0] == vc.iloc[1]:\n",
    "        if tie == \"skip\": return np.nan\n",
    "        if tie == \"min\":  return s.min()\n",
    "        if tie == \"max\":  return s.max()\n",
    "    return vc.index[0]\n",
    "\n",
    "def consensus_confusion(wide_df, labels=None, normalize=None, min_raters=2, tie=\"skip\"):\n",
    "    # <<< KEY CHANGE: force axis names >>>\n",
    "    df = ensure_wide(wide_df).rename_axis(index=\"item\", columns=\"annotator\")\n",
    "\n",
    "    maj = df.apply(lambda r: majority_vote(r, min_raters=min_raters, tie=tie), axis=1).rename(\"majority\")\n",
    "    tmp = pd.concat([df, maj], axis=1).dropna(subset=[\"majority\"])\n",
    "    tmp[\"majority\"] = tmp[\"majority\"].astype(int)\n",
    "\n",
    "    long = (tmp.drop(columns=[\"majority\"])\n",
    "              .stack()                       # drops NaNs\n",
    "              .reset_index(name=\"rating\"))   # cols: item, annotator, rating\n",
    "    long = long.merge(tmp[[\"majority\"]], left_on=\"item\", right_index=True, how=\"left\")\n",
    "    long[\"rating\"] = long[\"rating\"].astype(int)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = sorted(pd.unique(long[\"rating\"]).tolist())\n",
    "\n",
    "    cm = pd.crosstab(\n",
    "        long[\"rating\"], long[\"majority\"],\n",
    "        rownames=[\"annotator rating\"], colnames=[\"majority vote\"], dropna=False\n",
    "    ).reindex(index=labels, columns=labels, fill_value=0)\n",
    "\n",
    "    if normalize == \"row\":\n",
    "        cm = cm.div(cm.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "    elif normalize == \"all\":\n",
    "        tot = cm.values.sum()\n",
    "        if tot > 0: cm = cm / tot\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "810ba389-5598-4f12-b71b-569bc83c470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotator_vs_groundtruth_confusions(df, gt_col=\"bryan\", labels=(1,2,3,4), normalize=None):\n",
    "    \"\"\"\n",
    "    Build confusion matrices of each annotator vs ground truth (gt_col).\n",
    "    Returns:\n",
    "      per_annotator: dict[str -> DataFrame]  # rows=annotator rating, cols=ground truth\n",
    "      pooled_micro:  DataFrame               # sum across annotators (micro-average)\n",
    "    normalize: None | \"row\" | \"col\" | \"all\"\n",
    "      - \"row\":   P(rating | GT column aggregated later when you flip)\n",
    "      - \"col\":   P(annotator rating | ground-truth class)  <-- GREAT to see deviations from GT\n",
    "      - \"all\":   global proportion\n",
    "    \"\"\"\n",
    "    if gt_col not in df.columns:\n",
    "        raise ValueError(f\"Ground truth column '{gt_col}' not found in df.columns = {list(df.columns)}\")\n",
    "\n",
    "    labels = list(labels)\n",
    "    per_annotator = {}\n",
    "    pooled_counts = np.zeros((len(labels), len(labels)), dtype=int)  # rows=ann, cols=GT\n",
    "\n",
    "    for ann in df.columns:\n",
    "        if ann == gt_col:\n",
    "            continue\n",
    "        pair = df[[ann, gt_col]].dropna()\n",
    "        if pair.empty:\n",
    "            continue\n",
    "        # ensure ints\n",
    "        pair = pair.astype(int)\n",
    "\n",
    "        cm = pd.crosstab(\n",
    "            index=pair[ann], columns=pair[gt_col],\n",
    "            rownames=[f\"{ann} rating\"], colnames=[f\"{gt_col} (GT)\"], dropna=False\n",
    "        ).reindex(index=labels, columns=labels, fill_value=0)\n",
    "\n",
    "        per_annotator[ann] = cm\n",
    "        pooled_counts += cm.values\n",
    "\n",
    "    pooled_micro = pd.DataFrame(pooled_counts, index=labels, columns=labels)\n",
    "\n",
    "    def _normalize(mat: pd.DataFrame) -> pd.DataFrame:\n",
    "        if normalize is None:\n",
    "            return mat\n",
    "        if normalize == \"row\":\n",
    "            return mat.div(mat.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "        if normalize == \"col\":\n",
    "            return mat.div(mat.sum(axis=0).replace(0, np.nan), axis=1)\n",
    "        if normalize == \"all\":\n",
    "            tot = mat.values.sum()\n",
    "            return mat / tot if tot > 0 else mat\n",
    "        raise ValueError(\"normalize must be one of {None,'row','col','all'}\")\n",
    "\n",
    "    per_annotator = {k: _normalize(v) for k, v in per_annotator.items()}\n",
    "    pooled_micro = _normalize(pooled_micro)\n",
    "    return per_annotator, pooled_micro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f38a72-c11b-41cd-a649-946b2990848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import krippendorff\n",
    "\n",
    "# ---- the robust helpers from earlier (paste them once in your notebook) ----\n",
    "# ensure_wide / pairwise_confusions / pooled_confusion / majority_vote / consensus_confusion\n",
    "# (use the latest versions I shared that handle ragged coverage)\n",
    "\n",
    "def calculate_simple_agreement(annotation, aspct, labels=(1,2,3,4), k=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Build utterance_id x annotator matrix, optionally restrict to k annotators by\n",
    "    removing outliers based on mean pairwise Cohen's kappa (quadratic), then compute:\n",
    "      - Pairwise κ among retained annotators\n",
    "      - Krippendorff's α (ordinal) on retained annotators\n",
    "      - Ratings matrix (utterance x retained annotators)\n",
    "\n",
    "    Outlier detection: iteratively remove the annotator with the lowest\n",
    "    overlap-weighted mean κ to others (ties -> lower total overlap, then name).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build annotator -> {utt_id: rating}\n",
    "    ann2dict = {}\n",
    "    for a in annotation:\n",
    "        author = a[\"annotator\"]\n",
    "        if author not in ann2dict:\n",
    "            ann2dict[author] = {}\n",
    "        for utt in a[\"target_utterances\"]:\n",
    "            if utt.get(\"skipped\", False):\n",
    "                continue\n",
    "            utt_id = utt[\"utterance_id\"]\n",
    "            ann2dict[author][utt_id] = utt[\"labels\"][aspct]\n",
    "\n",
    "    # 2) Wide DF (union of utterance_ids, columns are annotators)\n",
    "    df = pd.DataFrame(ann2dict).apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df.index.name = \"utterance_id\"\n",
    "\n",
    "    annotators = list(df.columns)\n",
    "\n",
    "    def _pairwise_stats(cols):\n",
    "        \"\"\"Compute pairwise κ and per-annotator overlap-weighted mean κ.\"\"\"\n",
    "        pairwise_kappa = {}\n",
    "        pairwise_overlap = {}\n",
    "        # accumulators per annotator\n",
    "        wsum = {c: 0.0 for c in cols}       # sum(kappa * overlap)\n",
    "        wcnt = {c: 0   for c in cols}       # sum(overlap)\n",
    "\n",
    "        for a, b in combinations(cols, 2):\n",
    "            ab = df[[a, b]].dropna()\n",
    "            n = len(ab)\n",
    "            if n == 0:\n",
    "                kappa = np.nan\n",
    "            else:\n",
    "                kappa = cohen_kappa_score(\n",
    "                    ab[a].astype(int),\n",
    "                    ab[b].astype(int),\n",
    "                    weights=\"quadratic\"\n",
    "                )\n",
    "            pairwise_kappa[f\"{a}–{b}\"] = kappa\n",
    "            pairwise_overlap[(a, b)] = n\n",
    "\n",
    "            # accumulate weights both ways\n",
    "            for x, y in ((a, b), (b, a)):\n",
    "                wcnt[x] += n\n",
    "                if not np.isnan(kappa):\n",
    "                    wsum[x] += kappa * n\n",
    "\n",
    "        # mean κ per annotator (weighted by overlap count)\n",
    "        mean_kappa = {}\n",
    "        for c in cols:\n",
    "            if wcnt[c] > 0:\n",
    "                mean_kappa[c] = wsum[c] / max(wcnt[c], 1)\n",
    "            else:\n",
    "                # No overlap with anyone -> treat as extreme outlier\n",
    "                mean_kappa[c] = -np.inf\n",
    "\n",
    "        return pairwise_kappa, pairwise_overlap, mean_kappa, wcnt\n",
    "\n",
    "    removed_log = []\n",
    "    if k is not None and isinstance(k, int) and k > 0 and len(annotators) > k:\n",
    "        # Iteratively prune worst annotator by mean κ until |annotators| == k\n",
    "        current = annotators.copy()\n",
    "        while len(current) > k:\n",
    "            _, _, mean_kappa, overlaps = _pairwise_stats(current)\n",
    "\n",
    "            # select worst (lowest mean κ); tie-break: lower overlap, then name\n",
    "            def key_func(c):\n",
    "                mk = mean_kappa[c]\n",
    "                # ensure NaN behaves like -inf (worst)\n",
    "                if isinstance(mk, float) and np.isnan(mk):\n",
    "                    mk_key = -np.inf\n",
    "                else:\n",
    "                    mk_key = mk\n",
    "                return (mk_key, overlaps[c], str(c))  # ascending -> worst first\n",
    "\n",
    "            worst = sorted(current, key=key_func)[0]\n",
    "            removed_log.append((worst, mean_kappa[worst], overlaps[worst]))\n",
    "            current.remove(worst)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Outlier removal (until top-k remain):\")\n",
    "            if removed_log:\n",
    "                for name, mk, ov in removed_log:\n",
    "                    mk_str = f\"{mk:.3f}\" if np.isfinite(mk) else str(mk)\n",
    "                    print(f\"  - Removed {name} | mean κ={mk_str} | total overlap={ov}\")\n",
    "            else:\n",
    "                print(\"  - No removals needed\")\n",
    "\n",
    "        # filter df to retained annotators\n",
    "        df = df[current]\n",
    "\n",
    "    # Final agreement on the retained set\n",
    "    final_annots = list(df.columns)\n",
    "\n",
    "    # Pairwise κ among retained\n",
    "    pairwise_kappa = {}\n",
    "    for a, b in combinations(final_annots, 2):\n",
    "        ab = df[[a, b]].dropna()\n",
    "        pairwise_kappa[f\"{a}–{b}\"] = (\n",
    "            cohen_kappa_score(ab[a].astype(int), ab[b].astype(int), weights=\"quadratic\")\n",
    "            if not ab.empty else np.nan\n",
    "        )\n",
    "\n",
    "    # Krippendorff α (ordinal); expects observers x items\n",
    "    alpha = krippendorff.alpha(\n",
    "        reliability_data=df.T.values,\n",
    "        level_of_measurement=\"ordinal\"\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Pair-wise κ (quadratic):\", pairwise_kappa)\n",
    "        alpha_str = f\"{alpha:.3f}\" if np.isfinite(alpha) else str(alpha)\n",
    "        print(f\"Krippendorff α (ordinal): {alpha_str}\")\n",
    "\n",
    "    return pairwise_kappa, alpha, df\n",
    "    \n",
    "\n",
    "def calculate_agreement(annotation, task_type, aspct,include_gt=False, include_llm=False, labels=(1,2,3,4)):\n",
    "    # 1) Collect per-annotator -> {utterance_id: rating}\n",
    "    annotation = [att for att in annotation if att[\"task_type\"] == task_type]\n",
    "    ann2dict = {}  # annotator -> {utt_id: rating}\n",
    "\n",
    "    for a in annotation:\n",
    "        author = a[\"annotator\"]\n",
    "        if author not in ann2dict:\n",
    "            ann2dict[author] = {}\n",
    "        for utt in a[\"target_utterances\"]:\n",
    "            if utt.get(\"skipped\", False):\n",
    "                continue\n",
    "            utt_id = utt[\"utterance_id\"]\n",
    "            ann2dict[author][utt_id] = utt[\"labels\"][aspct]\n",
    "\n",
    "    if include_gt:\n",
    "        # 2) Ground truth (bryan) by utterance_id (align via dict, not list)\n",
    "        #    - Filter hm_df to only the utterances seen anywhere above\n",
    "        all_utts = set().union(*[set(d.keys()) for d in ann2dict.values()]) if ann2dict else set()\n",
    "        hm_sub = hm_df[hm_df[\"utterance_id\"].isin(all_utts)].copy()\n",
    "        # if duplicates, keep last; adjust as you need:\n",
    "        hm_sub = hm_sub.drop_duplicates(\"utterance_id\", keep=\"last\")\n",
    "\n",
    "        aspect_dict = {\"informativeness\": \"info\", \"novelty\": \"novo\", \"relevance\":\"relv\", \"scope\":\"imsc\"}\n",
    "        bryan_map = dict(zip(hm_sub[\"utterance_id\"], hm_sub[aspect_dict[aspct]]))\n",
    "        ann2dict[\"bryan\"] = bryan_map\n",
    "\n",
    "    # 3) Optional LLM: map from utterance_index -> value, then convert to utterance_id\n",
    "    if include_llm:\n",
    "        # Build index->id map from hm_df for the items we care about\n",
    "        idx2id = dict(zip(hm_sub[\"utterance_index\"], hm_sub[\"utterance_id\"]))\n",
    "\n",
    "        llm_idx_map = {}\n",
    "        for seg in llm_anno:\n",
    "            for u in seg.get(\"full_context_rating\", []):\n",
    "                ui = u.get(\"utterance_index\")\n",
    "                if ui in idx2id:  # only keep those we can align\n",
    "                    llm_idx_map[ui] = (u[\"implication_scope\"] if aspct == \"scope\" else u[aspct])\n",
    "\n",
    "        llm_map = {idx2id[i]: v for i, v in llm_idx_map.items() if i in idx2id}\n",
    "        ann2dict[\"llm\"] = llm_map\n",
    "\n",
    "    # 4) Build wide DF by aligning on utterance_id automatically\n",
    "    df = pd.DataFrame(ann2dict)  # index: utterance_id (union), columns: annotators\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")  # keep NaN for missing\n",
    "    df.index.name = \"utterance_id\"\n",
    "\n",
    "    # 5) Pair-wise weighted Cohen κ (quadratic), dropping NaNs per pair\n",
    "    pairwise_kappa = {}\n",
    "    for a, b in combinations(df.columns, 2):\n",
    "        ab = df[[a, b]].dropna()\n",
    "        pairwise_kappa[f\"{a}–{b}\"] = (\n",
    "            cohen_kappa_score(ab[a].astype(int), ab[b].astype(int), weights=\"quadratic\")\n",
    "            if not ab.empty else np.nan\n",
    "        )\n",
    "\n",
    "    # 6) Krippendorff α (ordinal); df.T is annotators x items\n",
    "    alpha = krippendorff.alpha(\n",
    "        reliability_data=df.T.values,\n",
    "        level_of_measurement=\"ordinal\"\n",
    "    )\n",
    "\n",
    "    print(\"Pair-wise κ (quadratic):\", pairwise_kappa)\n",
    "    print(f\"Krippendorff α (ordinal): {alpha:.3f}\")\n",
    "\n",
    "    # 7) Confusion matrices (ragged-aware helpers)\n",
    "    #    If you pasted the robust versions earlier, they accept df directly.\n",
    "    pooled = pooled_confusion(df, labels=list(labels))          # global label confusions\n",
    "    consensus = consensus_confusion(df, labels=list(labels), min_raters=2)  # vs majority\n",
    "\n",
    "    print(\"\\nPooled confusion (counts):\")\n",
    "    print(pooled)\n",
    "\n",
    "    print(\"\\nConsensus confusion vs majority (counts):\")\n",
    "    print(consensus)\n",
    "\n",
    "    \n",
    "    if include_gt:\n",
    "    # 8) Confusion vs ground truth (Bryan)\n",
    "        per_ann_cm, pooled_vs_gt = annotator_vs_groundtruth_confusions(\n",
    "            df, gt_col=\"bryan\", labels=labels, normalize=None  # try normalize=\"col\" to see deviations by GT class\n",
    "        )\n",
    "\n",
    "        print(\"\\nPooled (micro) confusion vs Bryan (counts):\")\n",
    "        print(pooled_vs_gt)\n",
    "\n",
    "        print(\"\\nPer-annotator confusion vs Bryan (counts):\")\n",
    "        for ann, cm in per_ann_cm.items():\n",
    "            print(f\"\\n{ann} vs Bryan:\")\n",
    "            print(cm)\n",
    "\n",
    "\n",
    "    # If you specifically want to see which GT class deviates most, column-normalize:\n",
    "        _, pooled_vs_gt_colnorm = annotator_vs_groundtruth_confusions(\n",
    "            df, gt_col=\"bryan\", labels=labels, normalize=\"col\"\n",
    "        )\n",
    "        print(\"\\nPooled vs Bryan (column-normalized: P(annotator rating | GT)):\")\n",
    "        print(pooled_vs_gt_colnorm)\n",
    "    \n",
    "    return df.sort_index()  # wide matrix: utterance_id x annotator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fb7b9-124e-4691-9ea7-817c023cd077",
   "metadata": {},
   "source": [
    "## Annotation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f660931-0e98-4491-b012-66a4f5a4a09b",
   "metadata": {},
   "source": [
    "### load annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5237dce4-07a4-4870-96e3-04b6e178adfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "SSL handshake failed: infogain-shard-00-00.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer,SSL handshake failed: infogain-shard-00-02.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer,SSL handshake failed: infogain-shard-00-01.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer, Timeout: 30s, Topology Description: <TopologyDescription id: 692dc011ea12aeb99ae86ebb, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('infogain-shard-00-00.gb5c5.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: infogain-shard-00-00.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer')>, <ServerDescription ('infogain-shard-00-01.gb5c5.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: infogain-shard-00-01.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer')>, <ServerDescription ('infogain-shard-00-02.gb5c5.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: infogain-shard-00-02.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer')>]>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m annotation \u001b[38;5;241m=\u001b[39m db[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfind({})\n\u001b[0;32m----> 2\u001b[0m annotation \u001b[38;5;241m=\u001b[39m [att \u001b[38;5;28;01mfor\u001b[39;00m att \u001b[38;5;129;01min\u001b[39;00m annotation \u001b[38;5;28;01mif\u001b[39;00m att[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotator\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maso\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5f1c4b9e5b420902880d1e98\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfora\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m att[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m annotation \u001b[38;5;241m=\u001b[39m db[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfind({})\n\u001b[0;32m----> 2\u001b[0m annotation \u001b[38;5;241m=\u001b[39m [att \u001b[38;5;28;01mfor\u001b[39;00m att \u001b[38;5;129;01min\u001b[39;00m annotation \u001b[38;5;28;01mif\u001b[39;00m att[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotator\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maso\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5f1c4b9e5b420902880d1e98\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfora\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m att[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.11/site-packages/pymongo/cursor.py:1251\u001b[0m, in \u001b[0;36mCursor.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__empty:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refresh():\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.11/site-packages/pymongo/cursor.py:1142\u001b[0m, in \u001b[0;36mCursor._refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__session:\n\u001b[0;32m-> 1142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__collection\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_ensure_session()\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Query\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__min \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__max) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__hint:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.11/site-packages/pymongo/mongo_client.py:1758\u001b[0m, in \u001b[0;36mMongoClient._ensure_session\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# Don't make implicit sessions causally consistent. Applications\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# should always opt-in.\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__start_session(\u001b[38;5;28;01mTrue\u001b[39;00m, causal_consistency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ConfigurationError, InvalidOperation):\n\u001b[1;32m   1760\u001b[0m     \u001b[38;5;66;03m# Sessions not supported.\u001b[39;00m\n\u001b[1;32m   1761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.11/site-packages/pymongo/mongo_client.py:1703\u001b[0m, in \u001b[0;36mMongoClient.__start_session\u001b[0;34m(self, implicit, **kwargs)\u001b[0m\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__start_session\u001b[39m(\u001b[38;5;28mself\u001b[39m, implicit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;66;03m# Raises ConfigurationError if sessions are not supported.\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m implicit:\n\u001b[0;32m-> 1703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_topology\u001b[38;5;241m.\u001b[39m_check_implicit_session_support()\n\u001b[1;32m   1704\u001b[0m         server_session \u001b[38;5;241m=\u001b[39m _EmptyServerSession()\n\u001b[1;32m   1705\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.11/site-packages/pymongo/topology.py:538\u001b[0m, in \u001b[0;36mTopology._check_implicit_session_support\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_implicit_session_support\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 538\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_session_support()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.11/site-packages/pymongo/topology.py:554\u001b[0m, in \u001b[0;36mTopology._check_session_support\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_servers_loop(\n\u001b[1;32m    551\u001b[0m             any_server_selector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_server_selection_timeout(), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m         )\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_description\u001b[38;5;241m.\u001b[39mreadable_servers:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_servers_loop(\n\u001b[1;32m    555\u001b[0m         readable_server_selector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_server_selection_timeout(), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m session_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_description\u001b[38;5;241m.\u001b[39mlogical_session_timeout_minutes\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.11/site-packages/pymongo/topology.py:238\u001b[0m, in \u001b[0;36mTopology._select_servers_loop\u001b[0;34m(self, selector, timeout, address)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m server_descriptions:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# No suitable servers.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m now \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m--> 238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServerSelectionTimeoutError(\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(selector)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Timeout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, Topology Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_opened()\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_check_all()\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: SSL handshake failed: infogain-shard-00-00.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer,SSL handshake failed: infogain-shard-00-02.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer,SSL handshake failed: infogain-shard-00-01.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer, Timeout: 30s, Topology Description: <TopologyDescription id: 692dc011ea12aeb99ae86ebb, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('infogain-shard-00-00.gb5c5.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: infogain-shard-00-00.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer')>, <ServerDescription ('infogain-shard-00-01.gb5c5.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: infogain-shard-00-01.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer')>, <ServerDescription ('infogain-shard-00-02.gb5c5.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: infogain-shard-00-02.gb5c5.mongodb.net:27017: [Errno 54] Connection reset by peer')>]>"
     ]
    }
   ],
   "source": [
    "annotation = db[\"annotations\"].find({})\n",
    "annotation = [att for att in annotation if att[\"annotator\"] not in [\"aso\", \"5f1c4b9e5b420902880d1e98\"] and \"fora\" in att[\"task_id\"]]\n",
    "\n",
    "# [\"675330fb23080b9ae3ec8239\", \"5e9ffa9ee172201b04f1a4c6\", 67cf6dfb019a3a7d53e9dc7e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd414b83-b124-477d-980d-4f8c14406b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_id_annt = {\"info\":{}, \"mix\":{}}\n",
    "for annt in annotation:\n",
    "    type_ = annt[\"type\"]\n",
    "    conv_id = annt[\"conversation_id\"]\n",
    "    if conv_id not in conv_id_annt[type_]:\n",
    "        conv_id_annt[type_][conv_id] = []\n",
    "    conv_id_annt[type_][conv_id].append(annt)\n",
    "info_annt = conv_id_annt[\"info\"]\n",
    "mix_annt = conv_id_annt[\"mix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8838f6-c337-4833-94fe-c6efe45313c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_id, segs in mix_annt.items():\n",
    "    for seg in segs:\n",
    "        for utt in seg[\"target_utterances\"]:\n",
    "            if \"skipped\" in utt:\n",
    "                continue\n",
    "            mix_annt_df.append(utt[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d500e05-7271-4c58-b35e-530550aa821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df = pd.DataFrame(mix_annt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde8de2-aa13-4c36-81aa-e737abda50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = mix_df.corr(method=\"spearman\")\n",
    "# Visualize the correlation matrix with a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Spearman Correlation Matrix of Constructs and Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4e35c-6ce1-4f9b-9596-e79117c72574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ac676-3286-4129-8b2f-8a36de88e65d",
   "metadata": {},
   "source": [
    "## INSQ Agreement analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384732e-c276-4bbc-aaad-7cd45f7577e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = db[\"annotations\"].find({})\n",
    "annotation = [att for att in annotation if att[\"annotator\"] not in [\"aso\", \"5f1c4b9e5b420902880d1e98\"] and \"insq\" in att[\"task_id\"]]\n",
    "\n",
    "# [\"675330fb23080b9ae3ec8239\", \"5e9ffa9ee172201b04f1a4c6\", 67cf6dfb019a3a7d53e9dc7e]\n",
    "conv_id_annt = {\"info\":{}, \"mix\":{}}\n",
    "for annt in annotation:\n",
    "    type_ = annt[\"type\"]\n",
    "    conv_id = annt[\"conversation_id\"]\n",
    "    if conv_id not in conv_id_annt[type_]:\n",
    "        conv_id_annt[type_][conv_id] = []\n",
    "    conv_id_annt[type_][conv_id].append(annt)\n",
    "info_annt = conv_id_annt[\"info\"]\n",
    "mix_annt = conv_id_annt[\"mix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c8ae7-52eb-4be3-a94f-ee10b45962b5",
   "metadata": {},
   "source": [
    "### Conversastional Information Gain Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ecfc0-7c7e-4329-a3cf-b8195646a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in info_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    \n",
    "    pairwise_kappa, alpha, conv_annt = calculate_simple_agreement(annts, \"informativeness\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = conv_annt\n",
    "    \n",
    "# print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ac039-536a-486b-ac3b-d7cf930deb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905caa9-c974-4eee-b2a7-a64b9d0f5651",
   "metadata": {},
   "source": [
    "#### Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f41200-694f-4906-b933-99bafefbbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in mix_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    annotators = set([a[\"annotator\"] for a in annts])\n",
    "    print(annotators)\n",
    "    if len(annotators) < 2:\n",
    "        print(\"less than 2 annotators\")\n",
    "        continue\n",
    "    \n",
    "    pairwise_kappa, alpha, eps_df = calculate_simple_agreement(annts, \"novelty\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = eps_df\n",
    "    \n",
    "# print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")\n",
    "\n",
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63f2e3-3262-4162-87f1-f77cfc8e9dd5",
   "metadata": {},
   "source": [
    "#### Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03428d80-7b85-4d5a-86dd-2459c4a95e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in mix_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    annotators = set([a[\"annotator\"] for a in annts])\n",
    "    print(annotators)\n",
    "    if len(annotators) < 2:\n",
    "        print(\"less than 2 annotators\")\n",
    "        continue\n",
    "    \n",
    "    pairwise_kappa, alpha, eps_df = calculate_simple_agreement(annts, \"relevance\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = eps_df\n",
    "    \n",
    "# print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")\n",
    "\n",
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "# print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051e113-e881-4c30-b60b-bc9676e960e3",
   "metadata": {},
   "source": [
    "#### Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73064436-17d8-435b-899a-38f0047cf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in mix_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    annotators = set([a[\"annotator\"] for a in annts])\n",
    "    print(annotators)\n",
    "    if len(annotators) < 2:\n",
    "        print(\"less than 2 annotators\")\n",
    "        continue\n",
    "    \n",
    "    pairwise_kappa, alpha, eps_df = calculate_simple_agreement(annts, \"scope\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = eps_df\n",
    "    \n",
    "print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")\n",
    "\n",
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "# print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5d9f6-44e9-4743-8984-a5abdb8b3f8f",
   "metadata": {},
   "source": [
    "## Fora Agreement analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1b3b4-2f92-4309-8867-ca4057d7b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = db[\"annotations\"].find({})\n",
    "annotation = [att for att in annotation if att[\"annotator\"] not in [\"aso\", \"5f1c4b9e5b420902880d1e98\"] and \"fora\" in att[\"task_id\"]]\n",
    "\n",
    "# [\"675330fb23080b9ae3ec8239\", \"5e9ffa9ee172201b04f1a4c6\", 67cf6dfb019a3a7d53e9dc7e]\n",
    "conv_id_annt = {\"info\":{}, \"mix\":{}}\n",
    "for annt in annotation:\n",
    "    type_ = annt[\"type\"]\n",
    "    conv_id = annt[\"conversation_id\"]\n",
    "    if conv_id not in conv_id_annt[type_]:\n",
    "        conv_id_annt[type_][conv_id] = []\n",
    "    conv_id_annt[type_][conv_id].append(annt)\n",
    "info_annt = conv_id_annt[\"info\"]\n",
    "mix_annt = conv_id_annt[\"mix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2323a-eafb-4001-86e3-0c08620feca1",
   "metadata": {},
   "source": [
    "### Fora Conversastional Information Gain Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2f8bd-767b-403a-bf32-3c520e9870a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in info_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    \n",
    "    pairwise_kappa, alpha, conv_annt = calculate_simple_agreement(annts, \"informativeness\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = conv_annt\n",
    "    \n",
    "# print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e92507-9e58-45b4-87ac-903901b617b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e8ceb-cbad-4c28-9e1e-03abb07ccd1a",
   "metadata": {},
   "source": [
    "### Novelty Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e02541-5e6f-4c2c-86ee-a1576f110bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in mix_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    annotators = set([a[\"annotator\"] for a in annts])\n",
    "    print(annotators)\n",
    "    if len(annotators) < 2:\n",
    "        print(\"less than 2 annotators\")\n",
    "        continue\n",
    "    \n",
    "    pairwise_kappa, alpha, eps_df = calculate_simple_agreement(annts, \"novelty\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = eps_df\n",
    "    \n",
    "# print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")\n",
    "\n",
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1abc98-43c3-4e40-9a60-c157d1ac2a13",
   "metadata": {},
   "source": [
    "### Relevance Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335872f-6609-4c74-91aa-1cc4a1c92252",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in mix_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    annotators = set([a[\"annotator\"] for a in annts])\n",
    "    print(annotators)\n",
    "    if len(annotators) < 2:\n",
    "        print(\"less than 2 annotators\")\n",
    "        continue\n",
    "    \n",
    "    pairwise_kappa, alpha, eps_df = calculate_simple_agreement(annts, \"relevance\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = eps_df\n",
    "    \n",
    "# print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")\n",
    "\n",
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "# print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557f4c3-3f41-457b-af14-78657cbca34d",
   "metadata": {},
   "source": [
    "### Implication scope Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65780a48-8d65-4e73-b7a0-ad98749a06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "episodes = {}\n",
    "\n",
    "for conv_id, annts in mix_annt.items():\n",
    "    print(f\"\\ncomputing agreement for conv_id: {conv_id}\")\n",
    "    annotators = set([a[\"annotator\"] for a in annts])\n",
    "    print(annotators)\n",
    "    if len(annotators) < 2:\n",
    "        print(\"less than 2 annotators\")\n",
    "        continue\n",
    "    \n",
    "    pairwise_kappa, alpha, eps_df = calculate_simple_agreement(annts, \"scope\", labels=(1,2,3,4), k=2)\n",
    "    alphas.append(alpha)\n",
    "    episodes[conv_id] = eps_df\n",
    "    \n",
    "print(f\"\\n\\n\\nkrpdff alpha average across episodes: {np.mean(alphas)}\")\n",
    "\n",
    "# build the long table and reliability matrix\n",
    "long_all, R = concat_episodes_to_matrix(episodes, value_name=\"label\", make_numeric=True)\n",
    "\n",
    "# print(\"Items x Annotators matrix shape:\", R.shape)\n",
    "# print(R.head())\n",
    "\n",
    "# compute alpha (choose level='nominal' if labels are categories; 'ordinal' if ordered 1..4)\n",
    "alpha_ord = krippendorff_alpha_from_matrix(R, level=\"ordinal\")\n",
    "print(\"Krippendorff’s alpha (ordinal):\", alpha_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b96c0-c358-4b73-9302-15a8a026a518",
   "metadata": {},
   "source": [
    "## Pilot study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac1aea-d52b-4ee0-8864-2c0fa9cfeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_df = pd.read_csv(\"../data/human_annotated/insq_2228.csv\")\n",
    "sp_df = pd.read_csv(\"../data/entropy/insq_meta_llama_Llama_32_3B_c0.csv\")\n",
    "hm_df = hm_df.loc[hm_df.phase == 1]\n",
    "# with open('mix_pilot.json') as json_data:\n",
    "#     annotation = json.load(json_data)\n",
    "#     json_data.close()\n",
    "\n",
    "    \n",
    "with open('../data/processed_segments/gemini/insq_2228_meta_checkpoint.json') as json_data:\n",
    "    llm_anno = json.load(json_data)\n",
    "    llm_anno = llm_anno[\"segmentation\"][\"segments\"]\n",
    "    json_data.close()\n",
    "df = hm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf8a17-c9c7-4ea4-ba15-591a2c1f0db9",
   "metadata": {},
   "source": [
    "merged_df = hm_df.merge(\n",
    "    sp_df[[\"utt_id\", \"sent_avg_h\", \"length\", \"tokens_h\"]],\n",
    "    left_on=\"utterance_id\",\n",
    "    right_on=\"utt_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "merged_df['tokens_h'] = merged_df['tokens_h'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189429c-1fcb-472f-8299-8c776e088f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['logpro_max'] = merged_df['tokens_h'].apply(max)\n",
    "merged_df['logpro_min'] = merged_df['tokens_h'].apply(min)\n",
    "merged_df['logpro_std'] = merged_df['tokens_h'].apply(np.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2251f6-b710-4aec-a056-a2875fcba372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df[[\"info\", \"novo\", \"relv\", \"imsc\", \"sent_avg_h\", \"length\" ,\"logpro_max\", \"logpro_min\", \"logpro_std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc6b99-bf7a-427b-9138-55059948b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"sent_avg_h\": 'sent_surprisal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28b2ee-753f-4ac6-9ba2-b5160f4dfba5",
   "metadata": {},
   "source": [
    "### Seed dataset construct validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e0fd1-5b27-40a5-9119-89ef4d20a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables of interest\n",
    "constructs_and_target = ['info', 'novo', 'relv', 'imsc']\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "corr_matrix = df[constructs_and_target].corr(method='spearman')\n",
    "\n",
    "# Visualize the correlation matrix with a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Spearman Correlation Matrix of Constructs and Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd2df7-4a21-4f83-a08c-b8bc7dfdaa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# --- Model Comparison Setup ---\n",
    "# Define features and target\n",
    "X = df.drop('info', axis=1)\n",
    "y = df['info']\n",
    "\n",
    "base_features = ['logpro_max', 'length']\n",
    "construct_features = ['novo', 'relv', 'imsc']\n",
    "\n",
    "# We'll use a robust model that can handle interactions and non-linearities\n",
    "# without requiring feature scaling.\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use Stratified K-Fold for cross-validation to preserve class distribution\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Evaluate the Baseline Model ---\n",
    "base_scores = cross_val_score(model, X[base_features], y, cv=cv, scoring='accuracy')\n",
    "print(f\"Baseline Model (features: {base_features})\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(base_scores):.4f} (+/- {np.std(base_scores):.4f})\\n\")\n",
    "\n",
    "# --- Evaluate Each Augmented Model and Test for Significance ---\n",
    "results = {}\n",
    "for construct in construct_features:\n",
    "    augmented_features = base_features + [construct]\n",
    "    \n",
    "    # Evaluate the augmented model\n",
    "    augmented_scores = cross_val_score(model, X[augmented_features], y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Perform a paired t-test on the cross-validation scores\n",
    "    # This checks if the difference in scores between the two models is statistically significant\n",
    "    t_stat, p_value = ttest_rel(augmented_scores, base_scores)\n",
    "    \n",
    "    print(f\"--- Validating '{construct}' ---\")\n",
    "    print(f\"Augmented Model (features: {augmented_features})\")\n",
    "    print(f\"Mean CV Accuracy: {np.mean(augmented_scores):.4f} (+/- {np.std(augmented_scores):.4f})\")\n",
    "    print(f\"Improvement over Baseline: {np.mean(augmented_scores) - np.mean(base_scores):.4f}\")\n",
    "    print(f\"Paired t-test p-value: {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Result: The improvement is statistically significant. The construct adds predictive power.\\n\")\n",
    "    else:\n",
    "        print(\"Result: The improvement is NOT statistically significant.\\n\")\n",
    "        \n",
    "    results[construct] = {'scores': augmented_scores, 'p_value': p_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80e436-54fc-44ef-aa27-52f6b4b0d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full Model with All Features ---\n",
    "all_features = base_features + construct_features\n",
    "X_full = df[all_features] # Use df instead of X to be clear\n",
    "\n",
    "# Train the model on the full dataset to get importance scores\n",
    "full_model = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_full, y)\n",
    "\n",
    "# Calculate Permutation Importance\n",
    "# CHANGE IS HERE: n_jobs=1 instead of n_jobs=-1\n",
    "perm_importance = permutation_importance(\n",
    "    full_model, X_full, y, n_repeats=10, random_state=42, n_jobs=1\n",
    ")\n",
    "\n",
    "# The rest of the code remains the same\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Use .iloc/.columns to be explicit with pandas\n",
    "plt.barh(X_full.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.title(\"Feature Importance in the Full Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368b8f0-8d6d-4d05-8740-718068a3284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize constructs for stability, then make 2-way interactions\n",
    "scaler = StandardScaler()\n",
    "Z = df[['novo','relv','imsc', \"logpro_max\",\"length\"]].astype(float).copy()\n",
    "Z[['novo','relv','imsc', \"logpro_max\",\"length\"]] = scaler.fit_transform(Z[['novo','relv','imsc', \"logpro_max\",\"length\"]])\n",
    "Z['novo_relv'] = Z['novo']*Z['relv']\n",
    "Z['novo_imsc'] = Z['novo']*Z['imsc']\n",
    "Z['relv_imsc'] = Z['relv']*Z['imsc']\n",
    "\n",
    "# ---- two-way aggregations ----\n",
    "pairs = [('novo','relv'), ('novo','imsc'), ('relv','imsc')]\n",
    "\n",
    "# Create 2-way aggregations\n",
    "for a,b in pairs:\n",
    "    Z[f'min_{a}_{b}']  = df[[a,b]].min(axis=1)\n",
    "    Z[f'max_{a}_{b}']  = df[[a,b]].max(axis=1)\n",
    "    Z[f'mean_{a}_{b}'] = df[[a,b]].mean(axis=1)\n",
    "    Z[f'prod_{a}_{b}'] = (df[a] * df[b]).astype(float)\n",
    "    # geometric mean (smoother than product; handles zeros with small epsilon)\n",
    "    Z[f'gmean_{a}_{b}'] = (Z[f'prod_{a}_{b}'].replace(0, 1e-9)) ** 0.5  # sqrt(prod)\n",
    "\n",
    "\n",
    "# ---- three-way aggregations (built from UNscaled originals) ----\n",
    "Z['min_agg']  = df[['novo','relv','imsc']].min(axis=1)\n",
    "Z['max_agg']  = df[['novo','relv','imsc']].max(axis=1)\n",
    "Z['mean_agg'] = df[['novo','relv','imsc']].mean(axis=1)\n",
    "Z['prod_agg'] = (df['novo'] * df['relv'] * df['imsc']).astype(float)\n",
    "Z['gmean_agg'] = (Z['prod_agg'].replace(0, 1e-9)) ** (1/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53b6e6-91cd-4a06-916f-6286641a6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from scipy.stats import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----- fit helpers -----\n",
    "def fit_ordlogit(y, X, name, link='logit'):\n",
    "    mod = OrderedModel(y, X, distr=link)\n",
    "    res = mod.fit(method='bfgs', maxiter=200, disp=False)\n",
    "    return dict(name=name, res=res, k=X.shape[1], X=X)\n",
    "\n",
    "def lr_test(full, reduced):\n",
    "    chi2_stat = 2*(full['res'].llf - reduced['res'].llf)\n",
    "    df = full['k'] - reduced['k']\n",
    "    p = chi2.sf(chi2_stat, df)\n",
    "    return chi2_stat, df, p\n",
    "\n",
    "def comp_row(m): \n",
    "    return [m['name'], float(m['res'].llf), float(m['res'].aic), float(m['res'].bic), m['k']]\n",
    "\n",
    "# ----- CV helper (returns mean±std NLL) -----\n",
    "def cv_nll(y, X, k=5, seed=42, link='logit'):\n",
    "    y = y.reset_index(drop=True)\n",
    "    X = X.reset_index(drop=True)\n",
    "    kf = KFold(k, shuffle=True, random_state=seed)\n",
    "    nlls = []\n",
    "    eps = 1e-12\n",
    "    for tr, te in kf.split(X):\n",
    "        y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
    "        X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
    "\n",
    "        train_classes = np.sort(y_tr.unique())\n",
    "        class_to_idx = {c: i for i, c in enumerate(train_classes)}\n",
    "\n",
    "        mod = OrderedModel(y_tr, X_tr, distr=link)\n",
    "        res = mod.fit(method='bfgs', maxiter=200, disp=False)\n",
    "\n",
    "        P = res.model.predict(res.params, exog=X_te)  # (n_test x K_train)\n",
    "        mask_seen = y_te.isin(train_classes).values\n",
    "        if not mask_seen.all():\n",
    "            P = P[mask_seen]\n",
    "            y_te = y_te[mask_seen]\n",
    "\n",
    "        idx = np.array([class_to_idx[int(c)] for c in y_te.values], dtype=int)\n",
    "        probs = P[np.arange(P.shape[0], dtype=int), idx]\n",
    "        probs = np.clip(probs, eps, 1.0)\n",
    "        nlls.append(float(-np.log(probs).mean()))\n",
    "    return float(np.mean(nlls)), float(np.std(nlls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141e095-c223-4814-8707-24924711fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# y and baseline features\n",
    "y = df['info']  # assumes values in {1,2,3,4}\n",
    "X_B1 = df[[\"logpro_max\",\"length\"]]  # <- your simple baseline names here\n",
    "\n",
    "B1 = fit_ordlogit(y, X_B1, 'Base model')\n",
    "models.append(B1)\n",
    "\n",
    "M_novo = fit_ordlogit(y, Z[['novo']], 'Single(novo)')\n",
    "M_relv = fit_ordlogit(y, Z[['relv']], 'Single(relv)')\n",
    "M_imsc = fit_ordlogit(y, Z[['imsc']], 'Single(imsc)')\n",
    "models.extend([M_novo, M_relv, M_imsc])\n",
    "\n",
    "I_novo = fit_ordlogit(y, pd.concat([X_B1, Z[['novo']]], axis=1), 'Base+novo')\n",
    "I_relv = fit_ordlogit(y, pd.concat([X_B1, Z[['relv']]], axis=1), 'Base+relv')\n",
    "I_imsc = fit_ordlogit(y, pd.concat([X_B1, Z[['imsc']]], axis=1), 'Base+imsc')\n",
    "models.extend([I_novo, I_relv, I_imsc])\n",
    "\n",
    "\n",
    "# Ablation models\n",
    "Abl_novo = fit_ordlogit(y, pd.concat([X_B1, Z[['relv','imsc']]], axis=1), 'Ablation(-novo)')\n",
    "Abl_relv = fit_ordlogit(y, pd.concat([X_B1, Z[['novo','imsc']]], axis=1), 'Ablation(-relv)')\n",
    "Abl_imsc = fit_ordlogit(y, pd.concat([X_B1, Z[['novo','relv']]], axis=1), 'Ablation(-imsc)')\n",
    "models.extend([Abl_novo, Abl_relv, Abl_imsc])\n",
    "\n",
    "C1 = fit_ordlogit(y, pd.concat([X_B1, Z[['novo','relv','imsc']]], axis=1), 'base+3')\n",
    "C1_ = fit_ordlogit(y, Z[['novo','relv','imsc']], 'Only3')\n",
    "\n",
    "models.append(C1)\n",
    "models.append(C1_)\n",
    "\n",
    "comp_abl = pd.DataFrame(\n",
    "    [comp_row(m) for m in models],\n",
    "    columns=['Model','LL','AIC','BIC','Params']\n",
    ").sort_values('AIC')\n",
    "print(comp_abl)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959731f4-47b4-473a-b373-66b297ecf744",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "models.append(C1_)\n",
    "\n",
    "pro2 = fit_ordlogit(y, Z[['novo','relv','imsc','prod_novo_relv','prod_novo_imsc','prod_relv_imsc']],\n",
    "                  '3+prod2ways')\n",
    "\n",
    "min2 = fit_ordlogit(y, Z[['novo','relv','imsc','min_novo_relv','min_novo_imsc','min_relv_imsc']],\n",
    "                  '3+min2ways')\n",
    "\n",
    "max2 = fit_ordlogit(y, Z[['novo','relv','imsc','max_novo_relv','max_novo_imsc','max_relv_imsc']],\n",
    "                  '3+max2ways')\n",
    "\n",
    "pro3 = fit_ordlogit(y, Z[['novo','relv','imsc', \"prod_agg\"]],\n",
    "                  '3+prod3ways')\n",
    "\n",
    "min3 = fit_ordlogit(y, Z[['novo','relv','imsc', \"min_agg\"]],\n",
    "                  '3+min3ways')\n",
    "\n",
    "max3 = fit_ordlogit(y, Z[['novo','relv','imsc',\"max_agg\"]],\n",
    "                  '3+max3ways')\n",
    "\n",
    "pro3only = fit_ordlogit(y, Z[[\"prod_agg\"]],\n",
    "                  'prod3waysonly')\n",
    "\n",
    "min3only = fit_ordlogit(y, Z[[\"min_agg\"]],\n",
    "                  'min3waysonly')\n",
    "\n",
    "max3only = fit_ordlogit(y, Z[[\"max_agg\"]],\n",
    "                  'max3waysonly')\n",
    "\n",
    "models.extend([pro2, min2, max2, pro3, min3, max3, pro3only, min3only, max3only])\n",
    "\n",
    "comp_abl = pd.DataFrame(\n",
    "    [comp_row(m) for m in models],\n",
    "    columns=['Model','LL','AIC','BIC','Params']\n",
    ").sort_values('AIC')\n",
    "print(comp_abl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce355f89-9504-488c-928f-ed419c8c8510",
   "metadata": {},
   "source": [
    "### Conversational Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69459e9b-7e38-4dfd-863a-1bd26c9e74bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rena time spent: ~22 minutes 10 seconds\n",
      "ZW time spent: ~25 minutes 20 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Rena time spent: ~22 minutes 10 seconds\")\n",
    "print(\"ZW time spent: ~25 minutes 20 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc569fe-31c1-48ad-8e92-0fdc2f6c9ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair-wise κ (quadratic): {'Rena–zw': 0.6555090655509065, 'Rena–5b406fc7c2e38100017613c4': 0.5326797385620914, 'Rena–667d6327bd395da87f80a249': 0.6312056737588652, 'Rena–67294473dbb580a0ca93f60f': 0.5145228215767634, 'Rena–bryan': 0.43111831442463544, 'zw–5b406fc7c2e38100017613c4': 0.7464239271781534, 'zw–667d6327bd395da87f80a249': 0.7018943170488534, 'zw–67294473dbb580a0ca93f60f': 0.4072538860103626, 'zw–bryan': 0.7329910141206675, '5b406fc7c2e38100017613c4–667d6327bd395da87f80a249': 0.7394209354120267, '5b406fc7c2e38100017613c4–67294473dbb580a0ca93f60f': 0.5470967741935484, '5b406fc7c2e38100017613c4–bryan': 0.6696562032884903, '667d6327bd395da87f80a249–67294473dbb580a0ca93f60f': 0.574826560951437, '667d6327bd395da87f80a249–bryan': 0.43853820598006654, '67294473dbb580a0ca93f60f–bryan': 0.3866317169069463}\n",
      "Krippendorff α (ordinal): 0.573\n",
      "\n",
      "Pooled confusion (counts):\n",
      "     1   2   3   4\n",
      "1  104  69  23   4\n",
      "2   69  94  54  23\n",
      "3   23  54  54  44\n",
      "4    4  23  44  94\n",
      "\n",
      "Consensus confusion vs majority (counts):\n",
      "majority vote      1   2   3   4\n",
      "annotator rating                \n",
      "1                 26   7   2   0\n",
      "2                  8  26   4   3\n",
      "3                  2   7  15   7\n",
      "4                  0   2   3  26\n",
      "\n",
      "Pooled (micro) confusion vs Bryan (counts):\n",
      "    1   2   3   4\n",
      "1  16  14   3   1\n",
      "2  10  16  10   3\n",
      "3   3   9  12   4\n",
      "4   1   6  10  12\n",
      "\n",
      "Per-annotator confusion vs Bryan (counts):\n",
      "\n",
      "Rena vs Bryan:\n",
      "bryan (GT)   1  2  3  4\n",
      "Rena rating            \n",
      "1            3  2  0  0\n",
      "2            2  4  3  2\n",
      "3            1  2  3  1\n",
      "4            0  1  1  1\n",
      "\n",
      "zw vs Bryan:\n",
      "bryan (GT)  1  2  3  4\n",
      "zw rating             \n",
      "1           4  7  0  0\n",
      "2           2  1  2  0\n",
      "3           0  1  3  2\n",
      "4           0  0  2  2\n",
      "\n",
      "5b406fc7c2e38100017613c4 vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "5b406fc7c2e38100017613c4 rating            \n",
      "1                                3  2  0  0\n",
      "2                                3  6  3  1\n",
      "3                                0  0  3  0\n",
      "4                                0  1  1  3\n",
      "\n",
      "667d6327bd395da87f80a249 vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "667d6327bd395da87f80a249 rating            \n",
      "1                                4  3  3  1\n",
      "2                                1  3  0  0\n",
      "3                                1  2  0  0\n",
      "4                                0  1  4  3\n",
      "\n",
      "67294473dbb580a0ca93f60f vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "67294473dbb580a0ca93f60f rating            \n",
      "1                                2  0  0  0\n",
      "2                                2  2  2  0\n",
      "3                                1  4  3  1\n",
      "4                                1  3  2  3\n",
      "\n",
      "Pooled vs Bryan (column-normalized: P(annotator rating | GT)):\n",
      "          1         2         3     4\n",
      "1  0.533333  0.311111  0.085714  0.05\n",
      "2  0.333333  0.355556  0.285714  0.15\n",
      "3  0.100000  0.200000  0.342857  0.20\n",
      "4  0.033333  0.133333  0.285714  0.60\n"
     ]
    }
   ],
   "source": [
    "df = calculate_agreement(annotation, \"info\", \"informativeness\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2065769-8084-45ce-adbc-31a918ea4e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rena</th>\n",
       "      <th>zw</th>\n",
       "      <th>5b406fc7c2e38100017613c4</th>\n",
       "      <th>667d6327bd395da87f80a249</th>\n",
       "      <th>67294473dbb580a0ca93f60f</th>\n",
       "      <th>bryan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utterance_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Rena  zw  5b406fc7c2e38100017613c4  667d6327bd395da87f80a249  \\\n",
       "utterance_id                                                                 \n",
       "2243             3   4                         3                         4   \n",
       "2244             2   1                         1                         1   \n",
       "2246             2   1                         1                         1   \n",
       "2247             1   1                         1                         1   \n",
       "2248             2   2                         2                         2   \n",
       "2249             1   1                         2                         2   \n",
       "2251             3   1                         2                         3   \n",
       "2252             3   3                         2                         1   \n",
       "2253             2   2                         3                         1   \n",
       "2254             4   4                         3                         4   \n",
       "2306             2   2                         2                         1   \n",
       "2308             2   1                         2                         2   \n",
       "2310             3   3                         2                         4   \n",
       "2312             2   3                         4                         4   \n",
       "2314             1   1                         2                         1   \n",
       "2316             2   2                         2                         3   \n",
       "2318             4   3                         4                         4   \n",
       "2365             1   1                         2                         1   \n",
       "2366             3   2                         2                         3   \n",
       "2369             3   1                         2                         2   \n",
       "2374             2   1                         1                         1   \n",
       "2375             1   1                         1                         1   \n",
       "2377             2   3                         2                         1   \n",
       "2379             4   4                         4                         4   \n",
       "2381             3   4                         4                         4   \n",
       "2383             2   3                         4                         4   \n",
       "\n",
       "              67294473dbb580a0ca93f60f  bryan  \n",
       "utterance_id                                   \n",
       "2243                                 3    3.0  \n",
       "2244                                 2    2.0  \n",
       "2246                                 3    2.0  \n",
       "2247                                 1    1.0  \n",
       "2248                                 4    1.0  \n",
       "2249                                 3    2.0  \n",
       "2251                                 4    2.0  \n",
       "2252                                 3    3.0  \n",
       "2253                                 2    3.0  \n",
       "2254                                 4    3.0  \n",
       "2306                                 2    3.0  \n",
       "2308                                 2    2.0  \n",
       "2310                                 3    3.0  \n",
       "2312                                 4    3.0  \n",
       "2314                                 2    1.0  \n",
       "2316                                 3    2.0  \n",
       "2318                                 4    2.0  \n",
       "2365                                 3    2.0  \n",
       "2366                                 3    1.0  \n",
       "2369                                 4    2.0  \n",
       "2374                                 2    1.0  \n",
       "2375                                 1    1.0  \n",
       "2377                                 3    4.0  \n",
       "2379                                 4    4.0  \n",
       "2381                                 4    4.0  \n",
       "2383                                 4    4.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8da88d-957b-4aef-859d-a4bd6e8a3bf4",
   "metadata": {},
   "source": [
    "## Mixed aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d43e63cf-5ec9-4e53-9444-859a00305ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZW time spent: ~28 minutes 20 seconds\n",
      "Aso time spent: ~44 minutes 20 seconds (only pre annotation)\n",
      "68109d22b0ba84c17333ecab time spent: ~28 minutes 10 seconds\n",
      "6007421356226920359b5f3e time spent: ~41 minutes 10 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"ZW time spent: ~28 minutes 20 seconds\")\n",
    "print(\"Aso time spent: ~44 minutes 20 seconds (only pre annotation)\")\n",
    "print(\"68109d22b0ba84c17333ecab time spent: ~28 minutes 10 seconds\")\n",
    "print(\"6007421356226920359b5f3e time spent: ~41 minutes 10 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019f8a3-e13d-48b9-b3e7-9db7c7b63fcd",
   "metadata": {},
   "source": [
    "### Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6994a39-2cb9-4e57-88b0-2767ea7fac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair-wise κ (quadratic): {'zw–68109d22b0ba84c17333ecab': 0.6119402985074627, 'zw–6007421356226920359b5f3e': 0.39344262295081966, 'zw–5f1c4b9e5b420902880d1e98': 0.4021024967148489, 'zw–bryan': 0.5976190476190476, '68109d22b0ba84c17333ecab–6007421356226920359b5f3e': 0.5490196078431373, '68109d22b0ba84c17333ecab–5f1c4b9e5b420902880d1e98': 0.478330658105939, '68109d22b0ba84c17333ecab–bryan': 0.4522471910112359, '6007421356226920359b5f3e–5f1c4b9e5b420902880d1e98': 0.5111111111111111, '6007421356226920359b5f3e–bryan': 0.6037735849056604, '5f1c4b9e5b420902880d1e98–bryan': 0.29017160686427457}\n",
      "Krippendorff α (ordinal): 0.496\n",
      "\n",
      "Pooled confusion (counts):\n",
      "    1   2   3   4\n",
      "1  50  43  18   5\n",
      "2  43  44  41  12\n",
      "3  18  41  84  29\n",
      "4   5  12  29  46\n",
      "\n",
      "Consensus confusion vs majority (counts):\n",
      "majority vote      1   2   3   4\n",
      "annotator rating                \n",
      "1                 19   1   0   0\n",
      "2                  8  10   4   0\n",
      "3                  1   1  26   2\n",
      "4                  2   3   5  13\n",
      "\n",
      "Pooled (micro) confusion vs Bryan (counts):\n",
      "    1   2   3  4\n",
      "1  10  10   3  0\n",
      "2   8  10   6  4\n",
      "3   4   7  16  8\n",
      "4   2   1   7  8\n",
      "\n",
      "Per-annotator confusion vs Bryan (counts):\n",
      "\n",
      "zw vs Bryan:\n",
      "bryan (GT)  1  2  3  4\n",
      "zw rating             \n",
      "1           4  5  1  0\n",
      "2           0  2  1  0\n",
      "3           1  0  5  2\n",
      "4           1  0  1  3\n",
      "\n",
      "68109d22b0ba84c17333ecab vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "68109d22b0ba84c17333ecab rating            \n",
      "1                                1  2  2  0\n",
      "2                                4  5  0  2\n",
      "3                                1  0  3  1\n",
      "4                                0  0  3  2\n",
      "\n",
      "6007421356226920359b5f3e vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "6007421356226920359b5f3e rating            \n",
      "1                                3  2  0  0\n",
      "2                                3  1  2  1\n",
      "3                                0  3  5  2\n",
      "4                                0  1  1  2\n",
      "\n",
      "5f1c4b9e5b420902880d1e98 vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "5f1c4b9e5b420902880d1e98 rating            \n",
      "1                                2  1  0  0\n",
      "2                                1  2  3  1\n",
      "3                                2  4  3  3\n",
      "4                                1  0  2  1\n",
      "\n",
      "Pooled vs Bryan (column-normalized: P(annotator rating | GT)):\n",
      "          1         2        3    4\n",
      "1  0.416667  0.357143  0.09375  0.0\n",
      "2  0.333333  0.357143  0.18750  0.2\n",
      "3  0.166667  0.250000  0.50000  0.4\n",
      "4  0.083333  0.035714  0.21875  0.4\n"
     ]
    }
   ],
   "source": [
    "df = calculate_agreement(annotation, \"mix\", \"novelty\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9125c901-7571-4567-9f1c-df5774952220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zw</th>\n",
       "      <th>68109d22b0ba84c17333ecab</th>\n",
       "      <th>6007421356226920359b5f3e</th>\n",
       "      <th>5f1c4b9e5b420902880d1e98</th>\n",
       "      <th>bryan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utterance_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              zw  68109d22b0ba84c17333ecab  6007421356226920359b5f3e  \\\n",
       "utterance_id                                                           \n",
       "2243           2                         3                         4   \n",
       "2244           1                         2                         3   \n",
       "2246           2                         1                         4   \n",
       "2247           1                         1                         1   \n",
       "2248           3                         3                         3   \n",
       "2249           1                         1                         3   \n",
       "2251           2                         3                         4   \n",
       "2252           2                         2                         3   \n",
       "2253           1                         2                         2   \n",
       "2254           3                         3                         4   \n",
       "2306           2                         3                         3   \n",
       "2308           2                         3                         1   \n",
       "2310           2                         3                         2   \n",
       "2312           3                         3                         4   \n",
       "2314           2                         2                         2   \n",
       "2316           2                         3                         3   \n",
       "2318           3                         3                         3   \n",
       "2365           2                         2                         1   \n",
       "2366           3                         2                         2   \n",
       "2369           2                         3                         3   \n",
       "2374           1                         2                         1   \n",
       "2375           1                         2                         1   \n",
       "2377           3                         3                         2   \n",
       "2379           3                         3                         4   \n",
       "2381           3                         3                         4   \n",
       "2383           3                         3                         3   \n",
       "\n",
       "              5f1c4b9e5b420902880d1e98  bryan  \n",
       "utterance_id                                   \n",
       "2243                                 3    3.0  \n",
       "2244                                 2    1.0  \n",
       "2246                                 3    2.0  \n",
       "2247                                 1    1.0  \n",
       "2248                                 3    3.0  \n",
       "2249                                 3    2.0  \n",
       "2251                                 2    2.0  \n",
       "2252                                 2    2.0  \n",
       "2253                                 2    2.0  \n",
       "2254                                 2    3.0  \n",
       "2306                                 3    3.0  \n",
       "2308                                 2    2.0  \n",
       "2310                                 2    3.0  \n",
       "2312                                 3    4.0  \n",
       "2314                                 3    3.0  \n",
       "2316                                 3    4.0  \n",
       "2318                                 4    4.0  \n",
       "2365                                 1    3.0  \n",
       "2366                                 4    3.0  \n",
       "2369                                 3    2.0  \n",
       "2374                                 1    1.0  \n",
       "2375                                 3    1.0  \n",
       "2377                                 4    4.0  \n",
       "2379                                 3    3.0  \n",
       "2381                                 2    4.0  \n",
       "2383                                 2    3.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad79bd-e9eb-41f0-adf6-454a3ac420c7",
   "metadata": {},
   "source": [
    "### Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95afb389-d460-48f0-90e3-63f245df44f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair-wise κ (quadratic): {'zw–68109d22b0ba84c17333ecab': 0.6034858387799564, 'zw–6007421356226920359b5f3e': 0.43295638126009695, 'zw–5f1c4b9e5b420902880d1e98': 0.5438596491228069, 'zw–bryan': 0.6269315673289183, '68109d22b0ba84c17333ecab–6007421356226920359b5f3e': 0.4155038759689922, '68109d22b0ba84c17333ecab–5f1c4b9e5b420902880d1e98': 0.6141001855287569, '68109d22b0ba84c17333ecab–bryan': 0.48434237995824636, '6007421356226920359b5f3e–5f1c4b9e5b420902880d1e98': 0.4414814814814815, '6007421356226920359b5f3e–bryan': 0.38095238095238093, '5f1c4b9e5b420902880d1e98–bryan': 0.47665056360708535}\n",
      "Krippendorff α (ordinal): 0.450\n",
      "\n",
      "Pooled confusion (counts):\n",
      "    1   2    3   4\n",
      "1  26  14    7   5\n",
      "2  14  26   46   6\n",
      "3   7  46  122  65\n",
      "4   5   6   65  60\n",
      "\n",
      "Consensus confusion vs majority (counts):\n",
      "majority vote     1   2   3   4\n",
      "annotator rating               \n",
      "1                 8   2   1   1\n",
      "2                 2  11   7   0\n",
      "3                 0   6  44   6\n",
      "4                 0   1  13  18\n",
      "\n",
      "Pooled (micro) confusion vs Bryan (counts):\n",
      "   1  2   3   4\n",
      "1  4  3   3   2\n",
      "2  0  1  20   1\n",
      "3  0  0  22  25\n",
      "4  0  0   7  16\n",
      "\n",
      "Per-annotator confusion vs Bryan (counts):\n",
      "\n",
      "zw vs Bryan:\n",
      "bryan (GT)  1  2  3  4\n",
      "zw rating             \n",
      "1           1  1  0  0\n",
      "2           0  0  4  0\n",
      "3           0  0  8  7\n",
      "4           0  0  1  4\n",
      "\n",
      "68109d22b0ba84c17333ecab vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "68109d22b0ba84c17333ecab rating            \n",
      "1                                1  0  1  0\n",
      "2                                0  1  4  0\n",
      "3                                0  0  5  8\n",
      "4                                0  0  3  3\n",
      "\n",
      "6007421356226920359b5f3e vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "6007421356226920359b5f3e rating            \n",
      "1                                1  1  2  1\n",
      "2                                0  0  3  1\n",
      "3                                0  0  5  4\n",
      "4                                0  0  3  5\n",
      "\n",
      "5f1c4b9e5b420902880d1e98 vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "5f1c4b9e5b420902880d1e98 rating            \n",
      "1                                1  1  0  1\n",
      "2                                0  0  9  0\n",
      "3                                0  0  4  6\n",
      "4                                0  0  0  4\n",
      "\n",
      "Pooled vs Bryan (column-normalized: P(annotator rating | GT)):\n",
      "     1     2         3         4\n",
      "1  1.0  0.75  0.057692  0.045455\n",
      "2  0.0  0.25  0.384615  0.022727\n",
      "3  0.0  0.00  0.423077  0.568182\n",
      "4  0.0  0.00  0.134615  0.363636\n"
     ]
    }
   ],
   "source": [
    "df = calculate_agreement(annotation, \"mix\", \"relevance\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6dc89-9fa9-48ab-b9b8-4562c5ae830c",
   "metadata": {},
   "source": [
    "### Implication Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe0c32ac-1754-44a0-b9e8-4f22d70ce868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair-wise κ (quadratic): {'zw–68109d22b0ba84c17333ecab': 0.5699745547073791, 'zw–6007421356226920359b5f3e': 0.37554585152838427, 'zw–5f1c4b9e5b420902880d1e98': 0.3910386965376783, 'zw–bryan': 0.6689303904923599, '68109d22b0ba84c17333ecab–6007421356226920359b5f3e': 0.3085106382978724, '68109d22b0ba84c17333ecab–5f1c4b9e5b420902880d1e98': 0.2277227722772277, '68109d22b0ba84c17333ecab–bryan': 0.5702479338842974, '6007421356226920359b5f3e–5f1c4b9e5b420902880d1e98': 0.300632911392405, '6007421356226920359b5f3e–bryan': 0.4057142857142857, '5f1c4b9e5b420902880d1e98–bryan': 0.485611510791367}\n",
      "Krippendorff α (ordinal): 0.410\n",
      "\n",
      "Pooled confusion (counts):\n",
      "    1   2    3   4\n",
      "1  44  26   13   1\n",
      "2  26  58   62  14\n",
      "3  13  62  104  37\n",
      "4   1  14   37   8\n",
      "\n",
      "Consensus confusion vs majority (counts):\n",
      "majority vote      1   2   3  4\n",
      "annotator rating               \n",
      "1                 12   3   0  0\n",
      "2                  2  22   7  0\n",
      "3                  1   8  35  0\n",
      "4                  0   2   8  0\n",
      "\n",
      "Pooled (micro) confusion vs Bryan (counts):\n",
      "    1   2   3   4\n",
      "1  10   5   2   0\n",
      "2   4  12  14   3\n",
      "3   2   9  20  13\n",
      "4   0   2   4   4\n",
      "\n",
      "Per-annotator confusion vs Bryan (counts):\n",
      "\n",
      "zw vs Bryan:\n",
      "bryan (GT)  1  2  3  4\n",
      "zw rating             \n",
      "1           4  2  0  0\n",
      "2           0  5  5  1\n",
      "3           0  0  5  4\n",
      "4           0  0  0  0\n",
      "\n",
      "68109d22b0ba84c17333ecab vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "68109d22b0ba84c17333ecab rating            \n",
      "1                                1  2  0  0\n",
      "2                                3  2  3  0\n",
      "3                                0  3  7  5\n",
      "4                                0  0  0  0\n",
      "\n",
      "6007421356226920359b5f3e vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "6007421356226920359b5f3e rating            \n",
      "1                                3  1  1  0\n",
      "2                                0  1  3  1\n",
      "3                                1  3  3  2\n",
      "4                                0  2  3  2\n",
      "\n",
      "5f1c4b9e5b420902880d1e98 vs Bryan:\n",
      "bryan (GT)                       1  2  3  4\n",
      "5f1c4b9e5b420902880d1e98 rating            \n",
      "1                                2  0  1  0\n",
      "2                                1  4  3  1\n",
      "3                                1  3  5  2\n",
      "4                                0  0  1  2\n",
      "\n",
      "Pooled vs Bryan (column-normalized: P(annotator rating | GT)):\n",
      "       1         2     3     4\n",
      "1  0.625  0.178571  0.05  0.00\n",
      "2  0.250  0.428571  0.35  0.15\n",
      "3  0.125  0.321429  0.50  0.65\n",
      "4  0.000  0.071429  0.10  0.20\n"
     ]
    }
   ],
   "source": [
    "df = calculate_agreement(annotation, \"mix\", \"scope\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba11176-4c78-4705-a434-73baaa89f0ee",
   "metadata": {},
   "source": [
    "### LLM memory based summary comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "119cfc9c-f11d-4174-baee-9c55b23e1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sum_rating = {}\n",
    "model_fulcon_rating = {}\n",
    "for model in [\"gemini\", \"flash\", \"openai\"]:\n",
    "    with open(f\"../data/processed_segments/{model}/insq_2228_meta_checkpoint.json\") as json_data:\n",
    "        data = json.load(json_data)\n",
    "        segments = data[\"segmentation\"][\"segments\"]\n",
    "        for seg in segments:\n",
    "            model_sum_rating[model] = seg[\"summary_ratings\"][\"memory_summary\"]\n",
    "            model_fulcon_rating[model] = seg[\"full_context_rating\"]\n",
    "        json_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c1a1f13-f58b-49f4-bef4-4870015546be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Sequence, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    cohen_kappa_score,\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "def _min_max_labels(y_true: Sequence[int], preds_dict: Dict[str, Sequence[int]]) -> Tuple[int, int]:\n",
    "    lo = min(y_true) if len(y_true) else 0\n",
    "    hi = max(y_true) if len(y_true) else 0\n",
    "    for v in preds_dict.values():\n",
    "        if v:\n",
    "            lo = min(lo, min(v))\n",
    "            hi = max(hi, max(v))\n",
    "    return int(lo), int(hi)\n",
    "\n",
    "def compare_models_ordinal(\n",
    "    preds_dict: Dict[str, Sequence[int]],\n",
    "    y_true: Sequence[int],\n",
    "    weight_scheme: str = \"quadratic\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare ordinal predictions against y_true using sklearn:\n",
    "      - qwk: weighted Cohen's kappa ('linear' or 'quadratic')\n",
    "      - mae, mse\n",
    "      - accuracy\n",
    "    Returns a ranked DataFrame.\n",
    "    \"\"\"\n",
    "    y_true = list(map(int, y_true))\n",
    "    n = len(y_true)\n",
    "    for name, preds in preds_dict.items():\n",
    "        if len(preds) != n:\n",
    "            raise ValueError(f\"Model '{name}' has {len(preds)} predictions, but y_true has {n}.\")\n",
    "\n",
    "    # Ensure labels are a contiguous range so κ distances are ordinal-correct\n",
    "    lo, hi = _min_max_labels(y_true, preds_dict)\n",
    "    labels_full = list(range(lo, hi + 1))\n",
    "\n",
    "    rows = []\n",
    "    for model, y_pred in preds_dict.items():\n",
    "        y_pred = list(map(int, y_pred))\n",
    "        # Metrics (sklearn)\n",
    "        acc = accuracy_score(y_true, y_pred) if n else 0.0\n",
    "        mae = mean_absolute_error(y_true, y_pred) if n else 0.0\n",
    "        mse = mean_squared_error(y_true, y_pred) if n else 0.0\n",
    "        qwk = cohen_kappa_score(y_true, y_pred, weights=weight_scheme, labels=labels_full) if n else 0.0\n",
    "\n",
    "        rows.append(\n",
    "            {\"model\": model, \"qwk\": qwk, \"mae\": mae, \"mse\": mse, \"accuracy\": acc}\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Primary: higher qwk; Tiebreakers: lower mae; then higher accuracy; then lower mse\n",
    "    df = df.sort_values(by=[\"qwk\", \"mae\", \"accuracy\", \"mse\"],\n",
    "                        ascending=[False, True, False, True]).reset_index(drop=True)\n",
    "    df.insert(0, \"rank\", range(1, len(df) + 1))\n",
    "    df.attrs[\"task_type\"] = \"ordinal (discrete)\"\n",
    "    df.attrs[\"labels_range\"] = (lo, hi)\n",
    "    df.attrs[\"weight_scheme\"] = weight_scheme\n",
    "    return df\n",
    "\n",
    "def compare_model_performance(ratings, aspect, hm_df):\n",
    "    colnames = {\"informativeness\": \"info\", \"novelty\":\"novo\", \"relevance\": \"relv\", \"implication_scope\": \"imsc\"}\n",
    "\n",
    "    model_aspect_rating = {}\n",
    "    utt_indexes = set()\n",
    "    for model, m_ratings in ratings.items():\n",
    "        rs = []\n",
    "        for r in m_ratings:\n",
    "            rs.append(int(r[aspect]))\n",
    "            utt_indexes.add(r[\"utterance_index\"])\n",
    "        model_aspect_rating[model] = rs\n",
    "\n",
    "    utt_indexes = sorted(utt_indexes)\n",
    "    col = colnames[aspect]\n",
    "    hm_rating = hm_df.loc[hm_df[\"utterance_index\"].isin(utt_indexes), col].astype(int).tolist()\n",
    "\n",
    "    result = compare_models_ordinal(model_aspect_rating, hm_rating, weight_scheme=\"quadratic\")\n",
    "    print(result.to_string(index=False))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a382de9-3e41-43a9-89df-023d8e5fcb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1 openai 0.823322 0.16 0.16      0.84\n",
      "    2 gemini 0.760994 0.20 0.20      0.80\n",
      "    3  flash 0.594257 0.36 0.52      0.72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.823322</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.760994</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.594257</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1  openai  0.823322  0.16  0.16      0.84\n",
       "1     2  gemini  0.760994  0.20  0.20      0.80\n",
       "2     3   flash  0.594257  0.36  0.52      0.72"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_sum_rating, \"informativeness\", hm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cda1f400-6a2b-4fdb-97df-486d297875d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1 openai 0.776386 0.20 0.20      0.80\n",
      "    2 gemini 0.658574 0.28 0.36      0.76\n",
      "    3  flash 0.324324 0.40 0.72      0.72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.776386</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.658574</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1  openai  0.776386  0.20  0.20      0.80\n",
       "1     2  gemini  0.658574  0.28  0.36      0.76\n",
       "2     3   flash  0.324324  0.40  0.72      0.72"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_fulcon_rating, \"informativeness\", hm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c91f3dc-00ff-426c-bb18-dfa3e402ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1 openai 0.680851 0.28 0.36      0.76\n",
      "    2 gemini 0.596774 0.32 0.40      0.72\n",
      "    3  flash 0.523810 0.40 0.64      0.72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1  openai  0.680851  0.28  0.36      0.76\n",
       "1     2  gemini  0.596774  0.32  0.40      0.72\n",
       "2     3   flash  0.523810  0.40  0.64      0.72"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_sum_rating, \"novelty\", hm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b460ff33-14a2-4530-85aa-9a3c4a282221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1 openai 0.592593 0.36 0.44      0.68\n",
      "    2 gemini 0.523810 0.40 0.48      0.64\n",
      "    3  flash 0.210526 0.60 0.84      0.52\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1  openai  0.592593  0.36  0.44      0.68\n",
       "1     2  gemini  0.523810  0.40  0.48      0.64\n",
       "2     3   flash  0.210526  0.60  0.84      0.52"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_fulcon_rating, \"novelty\", hm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d72d69a7-9f70-47ec-93aa-7dbb56ef3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1 gemini 0.575835 0.68 1.32      0.56\n",
      "    2 openai 0.570354 0.72 1.28      0.52\n",
      "    3  flash 0.261201 1.04 2.48      0.48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.575835</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.570354</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.261201</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1  gemini  0.575835  0.68  1.32      0.56\n",
       "1     2  openai  0.570354  0.72  1.28      0.52\n",
       "2     3   flash  0.261201  1.04  2.48      0.48"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_sum_rating, \"relevance\", hm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dc6f67f-5f6a-4944-8181-8a0a29c15116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1 gemini 0.644550 0.60 1.08      0.60\n",
      "    2 openai 0.561404 0.76 1.32      0.48\n",
      "    3  flash 0.325674 1.04 2.16      0.36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.644550</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.325674</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1  gemini  0.644550  0.60  1.08      0.60\n",
       "1     2  openai  0.561404  0.76  1.32      0.48\n",
       "2     3   flash  0.325674  1.04  2.16      0.36"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_fulcon_rating, \"relevance\", hm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1da34415-1a48-470a-a6f7-01f0fefd1291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1  flash 0.730769 0.48 0.56      0.56\n",
      "    2 gemini 0.666667 0.52 0.68      0.56\n",
      "    3 openai 0.638298 0.52 0.68      0.56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1   flash  0.730769  0.48  0.56      0.56\n",
       "1     2  gemini  0.666667  0.52  0.68      0.56\n",
       "2     3  openai  0.638298  0.52  0.68      0.56"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_sum_rating, \"implication_scope\", hm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9c89155-7f90-4e97-ace7-859c9cb62d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank  model      qwk  mae  mse  accuracy\n",
      "    1 gemini 0.736842 0.44 0.60      0.64\n",
      "    2  flash 0.695652 0.48 0.56      0.56\n",
      "    3 openai 0.638298 0.52 0.68      0.56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>model</th>\n",
       "      <th>qwk</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>flash</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>openai</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   model       qwk   mae   mse  accuracy\n",
       "0     1  gemini  0.736842  0.44  0.60      0.64\n",
       "1     2   flash  0.695652  0.48  0.56      0.56\n",
       "2     3  openai  0.638298  0.52  0.68      0.56"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_performance(model_fulcon_rating, \"implication_scope\", hm_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
