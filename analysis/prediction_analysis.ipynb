{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad57d36-9ddf-4fde-9451-50a9cbe869a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b2b9bd5-84ae-4511-8b8b-669ca4c0fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original \n",
    "ratings = [ load_json(F\"../data/ratings/tasks_ratings_{i}.json\") for i in range(3)]\n",
    "\n",
    "# with human ratings\n",
    "rating_hm = load_json(\"../data/ratings/tasks_ratings_hm.json\")\n",
    "rating_gpt_ex = load_json(\"../data/ratings/tasks_ratings_gptex.json\")\n",
    "\n",
    "# final post-processed labels for each utterance.\n",
    "final_labels = pd.read_csv(\"../scripts/final_labels.csv\")\n",
    "final_labels = final_labels.astype({'utterance_id': 'str'})\n",
    "\n",
    "# rating with claim labels\n",
    "claim_ratings = load_json(\"../data/ratings/tasks_ratings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7423bbc-023c-4de4-9b53-660fc6b623ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get utterances information df\n",
    "utts_info = []\n",
    "for corpus in [\"fora\", \"insq\"]:\n",
    "    for task in ratings[0]:\n",
    "        if corpus not in task[\"task_id\"]:\n",
    "            continue\n",
    "        utts = task[\"target_utterances\"]\n",
    "        for utt in utts:\n",
    "            if utt[\"skipped\"]:\n",
    "                continue\n",
    "            text = utt[\"utterance_text\"]\n",
    "            length = len(text.split(\" \"))\n",
    "            conv_id = corpus + \"_\" + str(utt['conversation_id']).split(\"_\")[-1]\n",
    "            utts_info.append({'corpus': corpus, \"conversation_id\": conv_id,'utterance_index': utt['utterance_index'], 'utterance_id': str(utt['utterance_id']), \"text\": text, \"length\": length})\n",
    "utts_df = pd.DataFrame(utts_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fe4248f-a9ce-4807-93c0-ca0c73ba160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the human soft and hard labels.\n",
    "\n",
    "# Pivot so that aspects are columns\n",
    "final_df = final_labels.pivot_table(\n",
    "    index=[\"corpus\", \"conv_id\", \"utterance_id\"],   # keep utterance identifiers\n",
    "    columns=\"aspect\",                              # spread aspects across columns\n",
    "    values=[\"n_raters\",\"y_hard\",\"w_conf\",\"p1\",\"p2\",\"p3\",\"p4\",\"y_cont\",\"lo\",\"hi\",\n",
    "            \"mask_t1\",\"mask_t2\",\"mask_t3\"],        # values to pivot\n",
    "    aggfunc=\"first\"                                # if duplicates, take first\n",
    ")\n",
    "\n",
    "# Flatten the multi-level column index (e.g. ('y_hard','Novelty') â†’ 'y_hard_Novelty')\n",
    "final_df.columns = [f\"{val}_{aspect}\" for val, aspect in final_df.columns]\n",
    "\n",
    "# Reset index so corpus/conv_id/utterance_id are normal columns\n",
    "final_df = final_df.reset_index()\n",
    "final_df = pd.merge(final_df, utts_df[[\"utterance_id\", 'utterance_index', \"text\", \"length\"]], how=\"left\", on=[\"utterance_id\"])\n",
    "# use soft label\n",
    "soft_df = final_df[['corpus', 'conv_id', 'utterance_id', 'utterance_index', 'y_cont_CIG',\n",
    "       'y_cont_Novelty', 'y_cont_Relevance', 'y_cont_Scope', \"text\", \"length\"]]\n",
    "soft_df = soft_df.rename(columns={'y_cont_CIG': 'info', 'y_cont_Novelty': 'novo', 'y_cont_Relevance':\"relv\", \"y_cont_Scope\": \"imsc\"})\n",
    "# use hard labels\n",
    "hard_df = final_df[['corpus', 'conv_id', 'utterance_id', 'utterance_index', 'y_hard_CIG',\n",
    "       'y_hard_Novelty', 'y_hard_Relevance', 'y_hard_Scope', \"text\", \"length\"]]\n",
    "\n",
    "hard_df = hard_df.rename(columns={'y_hard_CIG': 'info', 'y_hard_Novelty': 'novo', 'y_hard_Relevance':\"relv\", \"y_hard_Scope\": \"imsc\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0dd5a13c-5667-4e72-a0a2-eaf1cf016952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "\n",
    "ASPECTS = [\"info\", \"novo\", \"relv\", \"imsc\"]\n",
    "HARD_LABELS = [1, 2, 3, 4]\n",
    "\n",
    "aspect_decoder = {\n",
    "    \"info\": \"informativeness\",\n",
    "    \"novo\": \"novelty\",\n",
    "    \"relv\": \"relevance\",\n",
    "    \"imsc\": \"implication_scope\",\n",
    "}\n",
    "\n",
    "def get_groundtruth(\n",
    "    ratings: List[Dict[str, Any]],\n",
    "    soft_df: pd.DataFrame,\n",
    "    hard_df: pd.DataFrame,\n",
    "    corpus: str | None = None,\n",
    "    aspects: Sequence[str] = ASPECTS,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a ground_truth labels dict from preprocessed soft_df / hard_df.\n",
    "\n",
    "    soft_df, hard_df columns:\n",
    "      ['corpus', 'conv_id', 'utterance_id', 'utterance_index',\n",
    "       'info', 'novo', 'relv', 'imsc', 'text', 'length']\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"ids\": [utt_id1, utt_id2, ...],\n",
    "        \"soft\": {aspect: [float, ...], ...},\n",
    "        \"hard\": {aspect: [int, ...], ...},\n",
    "      }\n",
    "    \"\"\"\n",
    "    labels: Dict[str, Any] = {\"ids\": [], \"hard\": {}, \"soft\": {}}\n",
    "\n",
    "    for task in ratings:\n",
    "        conv_id = task[\"conversation_id\"]\n",
    "        if corpus is not None and task.get(\"corpus_id\") != corpus:\n",
    "            continue\n",
    "\n",
    "        # target span for this task\n",
    "        start = task[\"target_utterances\"][0][\"utterance_index\"]\n",
    "        end = task[\"target_utterances\"][-1][\"utterance_index\"] + 1\n",
    "\n",
    "        sub_soft = soft_df[\n",
    "            (soft_df[\"conv_id\"] == conv_id)\n",
    "            & (soft_df[\"utterance_index\"] >= start)\n",
    "            & (soft_df[\"utterance_index\"] < end)\n",
    "        ]\n",
    "        sub_hard = hard_df[\n",
    "            (hard_df[\"conv_id\"] == conv_id)\n",
    "            & (hard_df[\"utterance_index\"] >= start)\n",
    "            & (hard_df[\"utterance_index\"] < end)\n",
    "        ]\n",
    "\n",
    "        utt_ids = sub_soft[\"utterance_id\"].tolist()\n",
    "        labels[\"ids\"].extend(utt_ids)\n",
    "\n",
    "        for aspect in aspects:\n",
    "            for label_type, df in [(\"hard\", sub_hard), (\"soft\", sub_soft)]:\n",
    "                if aspect not in labels[label_type]:\n",
    "                    labels[label_type][aspect] = []\n",
    "                labels[label_type][aspect].extend(df[aspect].tolist())\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_predictions(\n",
    "    ratings: List[Dict[str, Any]],\n",
    "    soft_df: pd.DataFrame,\n",
    "    corpus: str | None = None,\n",
    "    models: Sequence[str] | None = None,\n",
    "    aspects: Sequence[str] = ASPECTS,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract prediction labels dict from ratings (which contain 'predictions').\n",
    "\n",
    "    Assumes predictions are stored in each task as:\n",
    "      task[\"predictions\"][model][method] = [\n",
    "          {\n",
    "            \"utterance_index\": int,\n",
    "            \"informativeness\": int,\n",
    "            \"novelty\": int,\n",
    "            \"relevance\": int,\n",
    "            \"implication_scope\": int,\n",
    "          }, ...\n",
    "      ]\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"ids\": [...],\n",
    "        \"<model>_<method>\": {\n",
    "            \"info\": [...],\n",
    "            \"novo\": [...],\n",
    "            ...\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    if models is None:\n",
    "        models = [\"gpt-5\", \"gpt-5-mini\"]\n",
    "\n",
    "    labels: Dict[str, Any] = {\"ids\": []}\n",
    "\n",
    "    for task in ratings:\n",
    "        conv_id = task[\"conversation_id\"]\n",
    "        if corpus is not None and task.get(\"corpus_id\") != corpus:\n",
    "            continue\n",
    "\n",
    "        start = task[\"target_utterances\"][0][\"utterance_index\"]\n",
    "        end = task[\"target_utterances\"][-1][\"utterance_index\"] + 1\n",
    "\n",
    "        sub_soft = soft_df[\n",
    "            (soft_df[\"conv_id\"] == conv_id)\n",
    "            & (soft_df[\"utterance_index\"] >= start)\n",
    "            & (soft_df[\"utterance_index\"] < end)\n",
    "        ]\n",
    "\n",
    "        utt_indexes = sub_soft[\"utterance_index\"].tolist()\n",
    "        utt_ids = sub_soft[\"utterance_id\"].tolist()\n",
    "        labels[\"ids\"].extend(utt_ids)\n",
    "\n",
    "        if \"predictions\" not in task:\n",
    "            continue\n",
    "\n",
    "        predictions = task[\"predictions\"]\n",
    "        for model, model_preds in predictions.items():\n",
    "            if model not in models:\n",
    "                continue\n",
    "\n",
    "            for method, method_pred in model_preds.items():\n",
    "                method_tag = f\"{model}_{method}\"\n",
    "                if method_tag not in labels:\n",
    "                    labels[method_tag] = {asp: [] for asp in aspects}\n",
    "\n",
    "                # We expect one prediction per utterance in utt_indexes\n",
    "                addition = 0\n",
    "                for pred in method_pred:\n",
    "                    if pred[\"utterance_index\"] in utt_indexes:\n",
    "                        addition += 1\n",
    "                        for aspect in aspects:\n",
    "                            labels[method_tag][aspect].append(\n",
    "                                pred[aspect_decoder[aspect]]\n",
    "                            )\n",
    "                # Sanity check: we saw exactly all utt_indexes\n",
    "                assert addition == len(utt_indexes), (\n",
    "                    f\"Method {method_tag} had {addition} matching preds, \"\n",
    "                    f\"expected {len(utt_indexes)} for conv_id={conv_id}\"\n",
    "                )\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d106cc3f-320f-4e6e-9a13-015e9ebac21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- metrics --------------------------- #\n",
    "\n",
    "def _safe_spearman(a: pd.Series, b: pd.Series) -> float:\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return np.nan\n",
    "    return a.corr(b, method=\"spearman\")\n",
    "\n",
    "\n",
    "def _safe_pearson(a: pd.Series, b: pd.Series) -> float:\n",
    "    if a.nunique() < 2 or b.nunique() < 2:\n",
    "        return np.nan\n",
    "    return a.corr(b, method=\"pearson\")\n",
    "\n",
    "\n",
    "def _confusion_matrix(y_true, y_pred, labels):\n",
    "    L = len(labels)\n",
    "    idx = {lab: i for i, lab in enumerate(labels)}\n",
    "    cm = np.zeros((L, L), dtype=float)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        if t in idx and p in idx:\n",
    "            cm[idx[t], idx[p]] += 1.0\n",
    "    return cm\n",
    "\n",
    "\n",
    "def _macro_f1(y_true, y_pred, labels=HARD_LABELS):\n",
    "    cm = _confusion_matrix(y_true, y_pred, labels)\n",
    "    tp = np.diag(cm)\n",
    "    fp = cm.sum(axis=0) - tp\n",
    "    fn = cm.sum(axis=1) - tp\n",
    "    f1s = []\n",
    "    for k in range(len(labels)):\n",
    "        denom = (2 * tp[k] + fp[k] + fn[k])\n",
    "        f1 = 0.0 if denom == 0 else (2 * tp[k]) / denom\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s)) if len(f1s) else np.nan\n",
    "\n",
    "\n",
    "def _quadratic_weighted_kappa(y_true, y_pred, labels=HARD_LABELS):\n",
    "    \"\"\"\n",
    "    Cohen's kappa with quadratic weights for ordinal labels.\n",
    "    \"\"\"\n",
    "    L = len(labels)\n",
    "    cm = _confusion_matrix(y_true, y_pred, labels)  # O_ij\n",
    "    n = cm.sum()\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Weight matrix: w_ij = (i-j)^2 / (L-1)^2\n",
    "    grid = np.arange(L)\n",
    "    W = (grid[:, None] - grid[None, :]) ** 2 / float((L - 1) ** 2)\n",
    "\n",
    "    # Row/col marginals\n",
    "    r = cm.sum(axis=1)\n",
    "    c = cm.sum(axis=0)\n",
    "\n",
    "    # Expected matrix under independence: E_ij = r_i * c_j / n\n",
    "    E = (r[:, None] * c[None, :]) / n if n > 0 else np.zeros_like(cm)\n",
    "\n",
    "    # Observed vs expected weighted sums\n",
    "    O_w = (W * cm).sum() / n\n",
    "    E_w = (W * E).sum() / n\n",
    "    if E_w == 0:\n",
    "        return 1.0 if O_w == 0 else 0.0\n",
    "\n",
    "    kappa = 1.0 - (O_w / E_w)\n",
    "    return float(kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f84bfc6-0479-463d-ad7f-b5c557242abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_ground_truth_labels(\n",
    "    gt_labels: Dict[str, Any],\n",
    "    aspects: Sequence[str],\n",
    ") -> Tuple[List[Any], int]:\n",
    "    \"\"\"\n",
    "    ground_truth_labels is expected to be the output of get_groundtruth(...).\n",
    "    \"\"\"\n",
    "    if \"ids\" not in gt_labels:\n",
    "        raise ValueError(\"ground_truth must contain key 'ids'.\")\n",
    "    gt_ids = gt_labels[\"ids\"]\n",
    "    gt_len = len(gt_ids)\n",
    "\n",
    "    soft = gt_labels.get(\"soft\", {})\n",
    "    hard = gt_labels.get(\"hard\", {})\n",
    "\n",
    "    for asp in aspects:\n",
    "        if asp not in soft:\n",
    "            raise ValueError(f\"ground_truth['soft'] missing aspect {asp}.\")\n",
    "        if asp not in hard:\n",
    "            raise ValueError(f\"ground_truth['hard'] missing aspect {asp}.\")\n",
    "        if len(soft[asp]) != gt_len:\n",
    "            raise ValueError(f\"ground_truth['soft'][{asp}] length != ids length.\")\n",
    "        if len(hard[asp]) != gt_len:\n",
    "            raise ValueError(f\"ground_truth['hard'][{asp}] length != ids length.\")\n",
    "\n",
    "    return gt_ids, gt_len\n",
    "\n",
    "\n",
    "def _ground_truth_labels_to_df(\n",
    "    gt_labels: Dict[str, Any],\n",
    "    aspects: Sequence[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Turn get_groundtruth(...) output into a long DataFrame:\n",
    "      id, aspect, soft_gt, hard_gt\n",
    "    \"\"\"\n",
    "    gt_ids = gt_labels[\"ids\"]\n",
    "    rows = []\n",
    "    for i, ex_id in enumerate(gt_ids):\n",
    "        for asp in aspects:\n",
    "            rows.append(\n",
    "                (\n",
    "                    ex_id,\n",
    "                    asp,\n",
    "                    float(gt_labels[\"soft\"][asp][i]),\n",
    "                    int(gt_labels[\"hard\"][asp][i]),\n",
    "                )\n",
    "            )\n",
    "    return pd.DataFrame(rows, columns=[\"id\", \"aspect\", \"soft_gt\", \"hard_gt\"])\n",
    "\n",
    "\n",
    "def _build_predictions_df(\n",
    "    llm_runs: List[Dict[str, Any]],\n",
    "    gt_ids: Sequence[Any],\n",
    "    gt_len: int,\n",
    "    aspects: Sequence[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    llm_runs is a list of label dicts as returned by get_predictions(...),\n",
    "    one per run.\n",
    "    \"\"\"\n",
    "    pred_rows = []\n",
    "    for run_idx, run in enumerate(llm_runs):\n",
    "        if \"ids\" not in run:\n",
    "            raise ValueError(f\"Run {run_idx} missing 'ids'.\")\n",
    "        run_ids = run[\"ids\"]\n",
    "\n",
    "        if len(run_ids) != gt_len or any(a != b for a, b in zip(run_ids, gt_ids)):\n",
    "            raise ValueError(f\"Run {run_idx} ids not aligned with ground truth.\")\n",
    "\n",
    "        methods = [k for k in run.keys() if k != \"ids\"]\n",
    "        for method in methods:\n",
    "            block = run[method]\n",
    "            for asp in aspects:\n",
    "                if asp not in block:\n",
    "                    raise ValueError(\n",
    "                        f\"Run {run_idx}, method {method} missing aspect {asp}.\"\n",
    "                    )\n",
    "                vals = block[asp]\n",
    "                if len(vals) != gt_len:\n",
    "                    raise ValueError(\n",
    "                        f\"Run {run_idx}, method {method}, aspect {asp} len {len(vals)} != ids len {gt_len}\"\n",
    "                    )\n",
    "                for i, ex_id in enumerate(run_ids):\n",
    "                    pred_rows.append((run_idx, method, asp, ex_id, int(vals[i])))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        pred_rows,\n",
    "        columns=[\"run\", \"method\", \"aspect\", \"id\", \"pred\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def _compute_metrics_for_group(\n",
    "    g: pd.DataFrame,\n",
    "    hard_labels: Sequence[int],\n",
    ") -> Dict[str, Any]:\n",
    "    pred = pd.to_numeric(g[\"pred\"], errors=\"coerce\")\n",
    "    soft = pd.to_numeric(g[\"soft_gt\"], errors=\"coerce\")\n",
    "    hard = pd.to_numeric(g[\"hard_gt\"], errors=\"coerce\")\n",
    "\n",
    "    # Soft metrics\n",
    "    mae_soft = (pred - soft).abs().mean()\n",
    "    pearson_soft = _safe_pearson(pred, soft)\n",
    "\n",
    "    # Hard metrics\n",
    "    mae_to_hard = (pred - hard).abs().mean()\n",
    "    macro_f1 = _macro_f1(hard.tolist(), pred.tolist(), labels=hard_labels)\n",
    "    qwk = _quadratic_weighted_kappa(hard.tolist(), pred.tolist(), labels=hard_labels)\n",
    "\n",
    "    return {\n",
    "        \"mae_soft\": float(mae_soft),\n",
    "        \"pearson_soft\": float(pearson_soft) if pd.notna(pearson_soft) else np.nan,\n",
    "        \"mae_to_hard\": float(mae_to_hard),\n",
    "        \"macro_f1_hard\": float(macro_f1),\n",
    "        \"qwk_hard\": float(qwk) if pd.notna(qwk) else np.nan,\n",
    "        \"n_items\": len(g),\n",
    "    }\n",
    "\n",
    "# ------------------------ main evaluators ---------------------- #\n",
    "\n",
    "def evaluate_llm_runs(\n",
    "    llm_runs: List[Dict[str, Any]],\n",
    "    ground_truth_labels: Dict[str, Any],\n",
    "    aspects: Sequence[str] = ASPECTS,\n",
    "    hard_labels: Sequence[int] = HARD_LABELS,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple LLM runs against ground truth ratings.\n",
    "\n",
    "    - ground_truth_labels: output of get_groundtruth(...)\n",
    "    - llm_runs           : list of outputs from get_predictions(...), 1 dict per run\n",
    "\n",
    "    Returns:\n",
    "      per_run_metrics: DataFrame with one row per run/method/aspect\n",
    "      agg: DataFrame aggregated over runs (mean/std per method/aspect)\n",
    "    \"\"\"\n",
    "    # validate & build GT df\n",
    "    gt_ids, gt_len = _validate_ground_truth_labels(ground_truth_labels, aspects)\n",
    "    gt_df = _ground_truth_labels_to_df(ground_truth_labels, aspects)\n",
    "\n",
    "    # predictions long df\n",
    "    pred_df = _build_predictions_df(llm_runs, gt_ids, gt_len, aspects)\n",
    "\n",
    "    # join\n",
    "    df = pred_df.merge(gt_df, on=[\"id\", \"aspect\"], how=\"inner\")\n",
    "\n",
    "    # per run x method x aspect metrics\n",
    "    metrics = []\n",
    "    for (run, method, aspect), g in df.groupby([\"run\", \"method\", \"aspect\"]):\n",
    "        m = _compute_metrics_for_group(g, hard_labels)\n",
    "        row = {\"run\": run, \"method\": method, \"aspect\": aspect}\n",
    "        row.update(m)\n",
    "        metrics.append(row)\n",
    "\n",
    "    per_run_metrics = pd.DataFrame(metrics).sort_values([\"method\", \"aspect\", \"run\"])\n",
    "\n",
    "    # aggregated metrics across runs\n",
    "    agg = (\n",
    "        per_run_metrics.groupby([\"method\", \"aspect\"])\n",
    "        .agg(\n",
    "            mae_soft_mean=(\"mae_soft\", \"mean\"),\n",
    "            mae_soft_std=(\"mae_soft\", \"std\"),\n",
    "            pearson_soft_mean=(\"pearson_soft\", \"mean\"),\n",
    "            pearson_soft_std=(\"pearson_soft\", \"std\"),\n",
    "            mae_to_hard_mean=(\"mae_to_hard\", \"mean\"),\n",
    "            mae_to_hard_std=(\"mae_to_hard\", \"std\"),\n",
    "            macro_f1_hard_mean=(\"macro_f1_hard\", \"mean\"),\n",
    "            macro_f1_hard_std=(\"macro_f1_hard\", \"std\"),\n",
    "            qwk_hard_mean=(\"qwk_hard\", \"mean\"),\n",
    "            qwk_hard_std=(\"qwk_hard\", \"std\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"method\", \"aspect\"])\n",
    "    )\n",
    "\n",
    "    return per_run_metrics, agg\n",
    "\n",
    "\n",
    "def evaluate_llm_runs_for_corpus(\n",
    "    ratings_per_run: List[List[Dict[str, Any]]],\n",
    "    soft_df: pd.DataFrame,\n",
    "    hard_df: pd.DataFrame,\n",
    "    corpus: str | None = None,\n",
    "    aspects: Sequence[str] = ASPECTS,\n",
    "    hard_labels: Sequence[int] = HARD_LABELS,\n",
    "    models_per_run: List[Sequence[str]] | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper that handles the corpus filter and building\n",
    "    ground truth + predictions from dfs.\n",
    "\n",
    "    - ratings_per_run: list of ratings lists, one per run\n",
    "    - soft_df, hard_df: preprocessed dfs with columns:\n",
    "        ['corpus', 'conv_id', 'utterance_id', 'utterance_index',\n",
    "         'info', 'novo', 'relv', 'imsc', 'text', 'length']\n",
    "    - corpus: if not None, only tasks with task['corpus_id'] == corpus are used\n",
    "    - models_per_run: optional list of 'models' argument for get_predictions.\n",
    "                      If None, get_predictions uses its default models.\n",
    "\n",
    "    Returns:\n",
    "        per_run_metrics, agg\n",
    "    \"\"\"\n",
    "    if not ratings_per_run:\n",
    "        raise ValueError(\"ratings_per_run must be a non-empty list.\")\n",
    "\n",
    "    # 1) Build ground truth once from the first run's ratings\n",
    "    ratings_for_gt = ratings_per_run[0]\n",
    "    ground_truth_labels = get_groundtruth(\n",
    "        ratings_for_gt,\n",
    "        soft_df=soft_df,\n",
    "        hard_df=hard_df,\n",
    "        corpus=corpus,\n",
    "        aspects=aspects,\n",
    "    )\n",
    "\n",
    "    # 2) Build per-run prediction label dicts\n",
    "    llm_runs: List[Dict[str, Any]] = []\n",
    "    for run_idx, ratings in enumerate(ratings_per_run):\n",
    "        if models_per_run is not None:\n",
    "            if run_idx >= len(models_per_run):\n",
    "                raise ValueError(\n",
    "                    f\"models_per_run has length {len(models_per_run)} \"\n",
    "                    f\"but there is run index {run_idx}.\"\n",
    "                )\n",
    "            models = models_per_run[run_idx]\n",
    "        else:\n",
    "            models = None  # will fall back to default in get_predictions\n",
    "        preds = get_predictions(\n",
    "            ratings,\n",
    "            soft_df=soft_df,\n",
    "            corpus=corpus,\n",
    "            models=models,\n",
    "            aspects=aspects,\n",
    "        )\n",
    "        llm_runs.append(preds)\n",
    "\n",
    "    # 3) Run the generic evaluation\n",
    "    per_run_metrics, agg = evaluate_llm_runs(\n",
    "        llm_runs=llm_runs,\n",
    "        ground_truth_labels=ground_truth_labels,\n",
    "        aspects=aspects,\n",
    "        hard_labels=hard_labels,\n",
    "    )\n",
    "    return per_run_metrics, agg, llm_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8191e7f8-2df5-4d3a-8e89-3c6a49b85635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- NEW: \"gpt-5_summary\" as soft reference ------------- #\n",
    "\n",
    "def build_summary_reference_soft(\n",
    "    llm_runs: List[Dict[str, Any]],\n",
    "    ref_method: str = \"gpt-5_summary\",\n",
    "    aspects: Sequence[str] = ASPECTS,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use the soft predictions of one method (e.g. 'gpt-5_summary')\n",
    "    across multiple runs as a *reference* by averaging them.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"ids\": [...],\n",
    "        \"soft\": {aspect: [mean_over_runs, ...]}\n",
    "      }\n",
    "    \"\"\"\n",
    "    if not llm_runs:\n",
    "        raise ValueError(\"llm_runs must be non-empty.\")\n",
    "\n",
    "    # Check ids alignment across runs\n",
    "    base_ids = llm_runs[0].get(\"ids\")\n",
    "    if base_ids is None:\n",
    "        raise ValueError(\"Each run must contain 'ids'.\")\n",
    "\n",
    "    N = len(base_ids)\n",
    "    for run_idx, run in enumerate(llm_runs):\n",
    "        run_ids = run.get(\"ids\")\n",
    "        if run_ids is None:\n",
    "            raise ValueError(f\"Run {run_idx} missing 'ids'.\")\n",
    "        if len(run_ids) != N or any(a != b for a, b in zip(run_ids, base_ids)):\n",
    "            raise ValueError(f\"Run {run_idx} ids not aligned with reference run 0.\")\n",
    "\n",
    "        if ref_method not in run:\n",
    "            raise ValueError(f\"Run {run_idx} missing reference method '{ref_method}'.\")\n",
    "\n",
    "    # Stack and average per aspect\n",
    "    ref_soft: Dict[str, List[float]] = {}\n",
    "    for asp in aspects:\n",
    "        all_vals = []\n",
    "        for run_idx, run in enumerate(llm_runs):\n",
    "            block = run[ref_method]\n",
    "            if asp not in block:\n",
    "                raise ValueError(\n",
    "                    f\"Run {run_idx}, method {ref_method} missing aspect {asp}.\"\n",
    "                )\n",
    "            vals = block[asp]\n",
    "            if len(vals) != N:\n",
    "                raise ValueError(\n",
    "                    f\"Run {run_idx}, method {ref_method}, aspect {asp} \"\n",
    "                    f\"len {len(vals)} != ids len {N}\"\n",
    "                )\n",
    "            all_vals.append(np.array(vals, dtype=float))\n",
    "\n",
    "        stacked = np.stack(all_vals, axis=0)  # shape: (num_runs, N)\n",
    "        mean_vals = stacked.mean(axis=0)      # shape: (N,)\n",
    "        ref_soft[asp] = mean_vals.tolist()\n",
    "\n",
    "    return {\"ids\": base_ids, \"soft\": ref_soft}\n",
    "\n",
    "\n",
    "def evaluate_against_summary_reference(\n",
    "    llm_runs: List[Dict[str, Any]],\n",
    "    ref_method: str = \"gpt-5_summary\",\n",
    "    aspects: Sequence[str] = ASPECTS,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Supervisor-requested analysis:\n",
    "\n",
    "    1. Take the method `ref_method` (e.g. 'gpt-5_summary') across all runs.\n",
    "    2. Build a *soft* reference ground truth by averaging its predictions\n",
    "       over runs (per id, per aspect).\n",
    "    3. For every OTHER method in the runs, compute:\n",
    "         - mae_soft (per run)\n",
    "         - pearson_soft (per run)\n",
    "       against this reference.\n",
    "    4. Aggregate over runs:\n",
    "         - mae_soft_mean, mae_soft_std\n",
    "         - pearson_soft_mean, pearson_soft_std\n",
    "\n",
    "    Returns:\n",
    "      per_run_metrics_soft: run x method x aspect\n",
    "      agg_soft: aggregated per method x aspect\n",
    "    \"\"\"\n",
    "    # 1) build reference from ref_method\n",
    "    ref = build_summary_reference_soft(llm_runs, ref_method=ref_method, aspects=aspects)\n",
    "    ref_ids = ref[\"ids\"]\n",
    "    N = len(ref_ids)\n",
    "\n",
    "    # 2) build predictions df for all OTHER methods\n",
    "    pred_rows = []\n",
    "    for run_idx, run in enumerate(llm_runs):\n",
    "        run_ids = run.get(\"ids\")\n",
    "        if run_ids is None:\n",
    "            raise ValueError(f\"Run {run_idx} missing 'ids'.\")\n",
    "        if len(run_ids) != N or any(a != b for a, b in zip(run_ids, ref_ids)):\n",
    "            raise ValueError(f\"Run {run_idx} ids not aligned with reference ids.\")\n",
    "\n",
    "        methods = [m for m in run.keys() if m not in (\"ids\", ref_method)]\n",
    "        for method in methods:\n",
    "            block = run[method]\n",
    "            for asp in aspects:\n",
    "                if asp not in block:\n",
    "                    raise ValueError(\n",
    "                        f\"Run {run_idx}, method {method} missing aspect {asp}.\"\n",
    "                    )\n",
    "                vals = block[asp]\n",
    "                if len(vals) != N:\n",
    "                    raise ValueError(\n",
    "                        f\"Run {run_idx}, method {method}, aspect {asp} \"\n",
    "                        f\"len {len(vals)} != ids len {N}\"\n",
    "                    )\n",
    "                for i, ex_id in enumerate(run_ids):\n",
    "                    pred_rows.append((run_idx, method, asp, ex_id, float(vals[i])))\n",
    "\n",
    "    if not pred_rows:\n",
    "        raise ValueError(\"No methods found to compare against the reference.\")\n",
    "\n",
    "    pred_df = pd.DataFrame(\n",
    "        pred_rows,\n",
    "        columns=[\"run\", \"method\", \"aspect\", \"id\", \"pred\"],\n",
    "    )\n",
    "\n",
    "    # 3) build reference DF (soft only, no hard)\n",
    "    ref_rows = []\n",
    "    for i, ex_id in enumerate(ref_ids):\n",
    "        for asp in aspects:\n",
    "            ref_rows.append((ex_id, asp, float(ref[\"soft\"][asp][i])))\n",
    "    ref_df = pd.DataFrame(ref_rows, columns=[\"id\", \"aspect\", \"soft_ref\"])\n",
    "\n",
    "    # 4) join and compute soft metrics per run/method/aspect\n",
    "    df = pred_df.merge(ref_df, on=[\"id\", \"aspect\"], how=\"inner\")\n",
    "\n",
    "    metrics = []\n",
    "    for (run, method, aspect), g in df.groupby([\"run\", \"method\", \"aspect\"]):\n",
    "        pred = pd.to_numeric(g[\"pred\"], errors=\"coerce\")\n",
    "        soft_ref = pd.to_numeric(g[\"soft_ref\"], errors=\"coerce\")\n",
    "\n",
    "        mae_soft = (pred - soft_ref).abs().mean()\n",
    "        pearson_soft = _safe_pearson(pred, soft_ref)\n",
    "\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"run\": run,\n",
    "                \"method\": method,\n",
    "                \"aspect\": aspect,\n",
    "                \"mae_soft\": float(mae_soft),\n",
    "                \"pearson_soft\": float(pearson_soft)\n",
    "                if pd.notna(pearson_soft)\n",
    "                else np.nan,\n",
    "                \"n_items\": len(g),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    per_run_metrics_soft = pd.DataFrame(metrics).sort_values(\n",
    "        [\"method\", \"aspect\", \"run\"]\n",
    "    )\n",
    "\n",
    "    # 5) aggregate across runs: only mae_soft & pearson_soft, as requested\n",
    "    agg_soft = (\n",
    "        per_run_metrics_soft.groupby([\"method\", \"aspect\"])\n",
    "        .agg(\n",
    "            mae_soft_mean=(\"mae_soft\", \"mean\"),\n",
    "            mae_soft_std=(\"mae_soft\", \"std\"),\n",
    "            pearson_soft_mean=(\"pearson_soft\", \"mean\"),\n",
    "            pearson_soft_std=(\"pearson_soft\", \"std\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"method\", \"aspect\"])\n",
    "    )\n",
    "\n",
    "    return per_run_metrics_soft, agg_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a286b9-3267-4758-a273-657c6b6f919a",
   "metadata": {},
   "source": [
    "## INSQ prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7f28a59f-85d0-499a-9bf6-df041843e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_run_metrics, summary_metrics, llm_runs = evaluate_llm_runs_for_corpus(\n",
    "    ratings_per_run=ratings,\n",
    "    soft_df=soft_df,\n",
    "    hard_df=hard_df,\n",
    "    corpus=\"insq\",   # or None for all\n",
    "    aspects=ASPECTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6ba2f5b1-e769-4390-85d6-05abd4976f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.470877</td>\n",
       "      <td>0.014102</td>\n",
       "      <td>0.764229</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.490885</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.513113</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.693793</td>\n",
       "      <td>0.002630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.494141</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.730311</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.540365</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>0.477249</td>\n",
       "      <td>0.026663</td>\n",
       "      <td>0.649928</td>\n",
       "      <td>0.011366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.529036</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.812272</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.475173</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.662282</td>\n",
       "      <td>0.006118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.550477</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.776533</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.606771</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>0.467829</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.012404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>info</td>\n",
       "      <td>0.540582</td>\n",
       "      <td>0.032335</td>\n",
       "      <td>0.682746</td>\n",
       "      <td>0.027261</td>\n",
       "      <td>0.558594</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.470858</td>\n",
       "      <td>0.029675</td>\n",
       "      <td>0.622592</td>\n",
       "      <td>0.016425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.408550</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.809369</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>0.397135</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.591118</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.751823</td>\n",
       "      <td>0.007017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.445182</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.783179</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>0.466146</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.541944</td>\n",
       "      <td>0.005897</td>\n",
       "      <td>0.716484</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.445269</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>0.826468</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550756</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.731736</td>\n",
       "      <td>0.006764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.439540</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.816468</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.463542</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>0.555847</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>0.731608</td>\n",
       "      <td>0.009172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>info</td>\n",
       "      <td>0.450738</td>\n",
       "      <td>0.019755</td>\n",
       "      <td>0.760566</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.440104</td>\n",
       "      <td>0.027437</td>\n",
       "      <td>0.557945</td>\n",
       "      <td>0.016870</td>\n",
       "      <td>0.703239</td>\n",
       "      <td>0.008660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "1           gpt-5-mini_full   info       0.470877      0.014102   \n",
       "5         gpt-5-mini_memory   info       0.494141      0.004968   \n",
       "9   gpt-5-mini_no_knowledge   info       0.529036      0.007333   \n",
       "13   gpt-5-mini_short_prior   info       0.550477      0.002904   \n",
       "17       gpt-5-mini_summary   info       0.540582      0.032335   \n",
       "21               gpt-5_full   info       0.408550      0.007143   \n",
       "25             gpt-5_memory   info       0.445182      0.003646   \n",
       "29       gpt-5_no_knowledge   info       0.445269      0.004818   \n",
       "33        gpt-5_short_prior   info       0.439540      0.006175   \n",
       "37            gpt-5_summary   info       0.450738      0.019755   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "1            0.764229          0.003995          0.490885         0.009021   \n",
       "5            0.730311          0.004205          0.540365         0.013718   \n",
       "9            0.812272          0.009476          0.593750         0.014084   \n",
       "13           0.776533          0.008843          0.606771         0.021514   \n",
       "17           0.682746          0.027261          0.558594         0.027344   \n",
       "21           0.809369          0.006790          0.397135         0.004511   \n",
       "25           0.783179          0.011682          0.466146         0.009831   \n",
       "29           0.826468          0.010900          0.472656         0.000000   \n",
       "33           0.816468          0.009793          0.463542         0.013718   \n",
       "37           0.760566          0.012496          0.440104         0.027437   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "1             0.513113           0.011423       0.693793      0.002630  \n",
       "5             0.477249           0.026663       0.649928      0.011366  \n",
       "9             0.475173           0.012675       0.662282      0.006118  \n",
       "13            0.467829           0.017372       0.637117      0.012404  \n",
       "17            0.470858           0.029675       0.622592      0.016425  \n",
       "21            0.591118           0.002435       0.751823      0.007017  \n",
       "25            0.541944           0.005897       0.716484      0.002233  \n",
       "29            0.550756           0.005265       0.731736      0.006764  \n",
       "33            0.555847           0.015486       0.731608      0.009172  \n",
       "37            0.557945           0.016870       0.703239      0.008660  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4ddf2758-473a-4e04-8c82-024ccd96dd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.723481</td>\n",
       "      <td>0.031651</td>\n",
       "      <td>0.638575</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.591146</td>\n",
       "      <td>0.026589</td>\n",
       "      <td>0.451260</td>\n",
       "      <td>0.006107</td>\n",
       "      <td>0.571709</td>\n",
       "      <td>0.014658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.684418</td>\n",
       "      <td>0.049129</td>\n",
       "      <td>0.588371</td>\n",
       "      <td>0.028666</td>\n",
       "      <td>0.600260</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.459454</td>\n",
       "      <td>0.026736</td>\n",
       "      <td>0.549857</td>\n",
       "      <td>0.034634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.597526</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.694616</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.643229</td>\n",
       "      <td>0.019269</td>\n",
       "      <td>0.451677</td>\n",
       "      <td>0.004860</td>\n",
       "      <td>0.581286</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.526432</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.697129</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.031815</td>\n",
       "      <td>0.482946</td>\n",
       "      <td>0.030921</td>\n",
       "      <td>0.634073</td>\n",
       "      <td>0.022165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.799870</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.528916</td>\n",
       "      <td>0.029543</td>\n",
       "      <td>0.694010</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.394345</td>\n",
       "      <td>0.036135</td>\n",
       "      <td>0.468360</td>\n",
       "      <td>0.030664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.577127</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.702800</td>\n",
       "      <td>0.012404</td>\n",
       "      <td>0.513021</td>\n",
       "      <td>0.014789</td>\n",
       "      <td>0.518710</td>\n",
       "      <td>0.022540</td>\n",
       "      <td>0.676709</td>\n",
       "      <td>0.014048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.567491</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.695081</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.545573</td>\n",
       "      <td>0.023546</td>\n",
       "      <td>0.490467</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>0.660849</td>\n",
       "      <td>0.013093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.021931</td>\n",
       "      <td>0.717685</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>0.785156</td>\n",
       "      <td>0.033829</td>\n",
       "      <td>0.398282</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>0.542672</td>\n",
       "      <td>0.015641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.639280</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.719965</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.700521</td>\n",
       "      <td>0.020045</td>\n",
       "      <td>0.421067</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>0.589428</td>\n",
       "      <td>0.004694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.580859</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>0.671735</td>\n",
       "      <td>0.015670</td>\n",
       "      <td>0.522135</td>\n",
       "      <td>0.037127</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.041368</td>\n",
       "      <td>0.652520</td>\n",
       "      <td>0.023440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "2           gpt-5-mini_full   novo       0.723481      0.031651   \n",
       "6         gpt-5-mini_memory   novo       0.684418      0.049129   \n",
       "10  gpt-5-mini_no_knowledge   novo       0.597526      0.017059   \n",
       "14   gpt-5-mini_short_prior   novo       0.526432      0.011449   \n",
       "18       gpt-5-mini_summary   novo       0.799870      0.028773   \n",
       "22               gpt-5_full   novo       0.577127      0.001734   \n",
       "26             gpt-5_memory   novo       0.567491      0.008203   \n",
       "30       gpt-5_no_knowledge   novo       0.704905      0.021931   \n",
       "34        gpt-5_short_prior   novo       0.639280      0.022222   \n",
       "38            gpt-5_summary   novo       0.580859      0.024067   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "2            0.638575          0.004038          0.591146         0.026589   \n",
       "6            0.588371          0.028666          0.600260         0.041401   \n",
       "10           0.694616          0.008421          0.643229         0.019269   \n",
       "14           0.697129          0.012954          0.541667         0.031815   \n",
       "18           0.528916          0.029543          0.694010         0.026009   \n",
       "22           0.702800          0.012404          0.513021         0.014789   \n",
       "26           0.695081          0.006389          0.545573         0.023546   \n",
       "30           0.717685          0.006507          0.785156         0.033829   \n",
       "34           0.719965          0.009912          0.700521         0.020045   \n",
       "38           0.671735          0.015670          0.522135         0.037127   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "2             0.451260           0.006107       0.571709      0.014658  \n",
       "6             0.459454           0.026736       0.549857      0.034634  \n",
       "10            0.451677           0.004860       0.581286      0.010200  \n",
       "14            0.482946           0.030921       0.634073      0.022165  \n",
       "18            0.394345           0.036135       0.468360      0.030664  \n",
       "22            0.518710           0.022540       0.676709      0.014048  \n",
       "26            0.490467           0.024873       0.660849      0.013093  \n",
       "30            0.398282           0.016788       0.542672      0.015641  \n",
       "34            0.421067           0.014449       0.589428      0.004694  \n",
       "38            0.509434           0.041368       0.652520      0.023440  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"novo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bf0b466a-4b2c-43d1-96af-658dae11ef98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.675819</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.608073</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>0.022913</td>\n",
       "      <td>0.535901</td>\n",
       "      <td>0.009949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.532595</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.691653</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.623698</td>\n",
       "      <td>0.008132</td>\n",
       "      <td>0.378773</td>\n",
       "      <td>0.015027</td>\n",
       "      <td>0.517904</td>\n",
       "      <td>0.016006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.517405</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.691798</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.595052</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.445642</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>0.016031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.549523</td>\n",
       "      <td>0.021409</td>\n",
       "      <td>0.655711</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>0.628906</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.383909</td>\n",
       "      <td>0.024460</td>\n",
       "      <td>0.499288</td>\n",
       "      <td>0.023726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.544314</td>\n",
       "      <td>0.028058</td>\n",
       "      <td>0.680052</td>\n",
       "      <td>0.031809</td>\n",
       "      <td>0.630208</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.372562</td>\n",
       "      <td>0.035624</td>\n",
       "      <td>0.513479</td>\n",
       "      <td>0.032178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.442578</td>\n",
       "      <td>0.004442</td>\n",
       "      <td>0.742377</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.467448</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>0.551933</td>\n",
       "      <td>0.027038</td>\n",
       "      <td>0.692062</td>\n",
       "      <td>0.003865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.428429</td>\n",
       "      <td>0.009952</td>\n",
       "      <td>0.759530</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.459635</td>\n",
       "      <td>0.017614</td>\n",
       "      <td>0.534062</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>0.703909</td>\n",
       "      <td>0.018736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.436936</td>\n",
       "      <td>0.014755</td>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>0.471354</td>\n",
       "      <td>0.026589</td>\n",
       "      <td>0.517399</td>\n",
       "      <td>0.025558</td>\n",
       "      <td>0.701863</td>\n",
       "      <td>0.018627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.444401</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.745444</td>\n",
       "      <td>0.019660</td>\n",
       "      <td>0.483073</td>\n",
       "      <td>0.023546</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.016079</td>\n",
       "      <td>0.678664</td>\n",
       "      <td>0.032018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.453082</td>\n",
       "      <td>0.017726</td>\n",
       "      <td>0.738147</td>\n",
       "      <td>0.027082</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.020670</td>\n",
       "      <td>0.506650</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>0.668565</td>\n",
       "      <td>0.030916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "3           gpt-5-mini_full   relv       0.525217      0.001504   \n",
       "7         gpt-5-mini_memory   relv       0.532595      0.007171   \n",
       "11  gpt-5-mini_no_knowledge   relv       0.517405      0.003978   \n",
       "15   gpt-5-mini_short_prior   relv       0.549523      0.021409   \n",
       "19       gpt-5-mini_summary   relv       0.544314      0.028058   \n",
       "23               gpt-5_full   relv       0.442578      0.004442   \n",
       "27             gpt-5_memory   relv       0.428429      0.009952   \n",
       "31       gpt-5_no_knowledge   relv       0.436936      0.014755   \n",
       "35        gpt-5_short_prior   relv       0.444401      0.011113   \n",
       "39            gpt-5_summary   relv       0.453082      0.017726   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "3            0.675819          0.005317          0.608073         0.005967   \n",
       "7            0.691653          0.000707          0.623698         0.008132   \n",
       "11           0.691798          0.012350          0.595052         0.005967   \n",
       "15           0.655711          0.025620          0.628906         0.013532   \n",
       "19           0.680052          0.031809          0.630208         0.033678   \n",
       "23           0.742377          0.002555          0.467448         0.013718   \n",
       "27           0.759530          0.015646          0.459635         0.017614   \n",
       "31           0.759804          0.015654          0.471354         0.026589   \n",
       "35           0.745444          0.019660          0.483073         0.023546   \n",
       "39           0.738147          0.027082          0.500000         0.020670   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "3             0.401726           0.022913       0.535901      0.009949  \n",
       "7             0.378773           0.015027       0.517904      0.016006  \n",
       "11            0.445642           0.011472       0.541693      0.016031  \n",
       "15            0.383909           0.024460       0.499288      0.023726  \n",
       "19            0.372562           0.035624       0.513479      0.032178  \n",
       "23            0.551933           0.027038       0.692062      0.003865  \n",
       "27            0.534062           0.007157       0.703909      0.018736  \n",
       "31            0.517399           0.025558       0.701863      0.018627  \n",
       "35            0.535629           0.016079       0.678664      0.032018  \n",
       "39            0.506650           0.013530       0.668565      0.030916  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"relv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a4be0390-31c3-4cc7-b245-b42c540230ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.468142</td>\n",
       "      <td>0.019895</td>\n",
       "      <td>0.710334</td>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.467448</td>\n",
       "      <td>0.033222</td>\n",
       "      <td>0.501691</td>\n",
       "      <td>0.025077</td>\n",
       "      <td>0.666974</td>\n",
       "      <td>0.017719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.451215</td>\n",
       "      <td>0.018209</td>\n",
       "      <td>0.732123</td>\n",
       "      <td>0.019837</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.533281</td>\n",
       "      <td>0.034123</td>\n",
       "      <td>0.677389</td>\n",
       "      <td>0.020693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.731567</td>\n",
       "      <td>0.013625</td>\n",
       "      <td>0.460938</td>\n",
       "      <td>0.033375</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.034308</td>\n",
       "      <td>0.677858</td>\n",
       "      <td>0.017285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.457899</td>\n",
       "      <td>0.017347</td>\n",
       "      <td>0.727750</td>\n",
       "      <td>0.016007</td>\n",
       "      <td>0.462240</td>\n",
       "      <td>0.015787</td>\n",
       "      <td>0.508251</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>0.667404</td>\n",
       "      <td>0.015399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.471094</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.711301</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.473958</td>\n",
       "      <td>0.012557</td>\n",
       "      <td>0.515266</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>0.657397</td>\n",
       "      <td>0.008277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.534809</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.686843</td>\n",
       "      <td>0.023573</td>\n",
       "      <td>0.563802</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.471440</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.627756</td>\n",
       "      <td>0.036958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.521354</td>\n",
       "      <td>0.022154</td>\n",
       "      <td>0.704536</td>\n",
       "      <td>0.019421</td>\n",
       "      <td>0.566406</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.487975</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>0.616398</td>\n",
       "      <td>0.019704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.532465</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.699612</td>\n",
       "      <td>0.013671</td>\n",
       "      <td>0.566406</td>\n",
       "      <td>0.031005</td>\n",
       "      <td>0.466218</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>0.623989</td>\n",
       "      <td>0.023293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.532552</td>\n",
       "      <td>0.010762</td>\n",
       "      <td>0.708058</td>\n",
       "      <td>0.024328</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.475917</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>0.625784</td>\n",
       "      <td>0.014409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.521788</td>\n",
       "      <td>0.020621</td>\n",
       "      <td>0.706590</td>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.555990</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.487490</td>\n",
       "      <td>0.022907</td>\n",
       "      <td>0.629132</td>\n",
       "      <td>0.011167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "0           gpt-5-mini_full   imsc       0.468142      0.019895   \n",
       "4         gpt-5-mini_memory   imsc       0.451215      0.018209   \n",
       "8   gpt-5-mini_no_knowledge   imsc       0.456944      0.016347   \n",
       "12   gpt-5-mini_short_prior   imsc       0.457899      0.017347   \n",
       "16       gpt-5-mini_summary   imsc       0.471094      0.003263   \n",
       "20               gpt-5_full   imsc       0.534809      0.023484   \n",
       "24             gpt-5_memory   imsc       0.521354      0.022154   \n",
       "28       gpt-5_no_knowledge   imsc       0.532465      0.018826   \n",
       "32        gpt-5_short_prior   imsc       0.532552      0.010762   \n",
       "36            gpt-5_summary   imsc       0.521788      0.020621   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "0            0.710334          0.015805          0.467448         0.033222   \n",
       "4            0.732123          0.019837          0.447917         0.026009   \n",
       "8            0.731567          0.013625          0.460938         0.033375   \n",
       "12           0.727750          0.016007          0.462240         0.015787   \n",
       "16           0.711301          0.004289          0.473958         0.012557   \n",
       "20           0.686843          0.023573          0.563802         0.033678   \n",
       "24           0.704536          0.019421          0.566406         0.014084   \n",
       "28           0.699612          0.013671          0.566406         0.031005   \n",
       "32           0.708058          0.024328          0.572917         0.011276   \n",
       "36           0.706590          0.015805          0.555990         0.025114   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "0             0.501691           0.025077       0.666974      0.017719  \n",
       "4             0.533281           0.034123       0.677389      0.020693  \n",
       "8             0.509000           0.034308       0.677858      0.017285  \n",
       "12            0.508251           0.009368       0.667404      0.015399  \n",
       "16            0.515266           0.012329       0.657397      0.008277  \n",
       "20            0.471440           0.018391       0.627756      0.036958  \n",
       "24            0.487975           0.006037       0.616398      0.019704  \n",
       "28            0.466218           0.018378       0.623989      0.023293  \n",
       "32            0.475917           0.008052       0.625784      0.014409  \n",
       "36            0.487490           0.022907       0.629132      0.011167  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"imsc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d209ce5-315c-4f58-b9da-08ac33fb19da",
   "metadata": {},
   "source": [
    "## Using GPT-5_summary as the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e8891f7b-e2cc-4cc2-b341-9986732b9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_run_soft, agg_soft = evaluate_against_summary_reference(\n",
    "    llm_runs,\n",
    "    ref_method=\"gpt-5_summary\",\n",
    "    aspects=ASPECTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b42536de-01b4-4e75-ae65-07fe9fddd669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                method aspect  mae_soft_mean  mae_soft_std  pearson_soft_mean  \\\n",
      "20          gpt-5_full   imsc       0.164931      0.011226           0.922658   \n",
      "21          gpt-5_full   info       0.265191      0.021049           0.859865   \n",
      "22          gpt-5_full   novo       0.352865      0.009115           0.807290   \n",
      "23          gpt-5_full   relv       0.159288      0.025857           0.934821   \n",
      "24        gpt-5_memory   imsc       0.156250      0.013718           0.930558   \n",
      "25        gpt-5_memory   info       0.264757      0.006682           0.879990   \n",
      "26        gpt-5_memory   novo       0.321181      0.027509           0.842451   \n",
      "27        gpt-5_memory   relv       0.130642      0.012512           0.948530   \n",
      "28  gpt-5_no_knowledge   imsc       0.154514      0.015751           0.932658   \n",
      "29  gpt-5_no_knowledge   info       0.393663      0.023630           0.819374   \n",
      "30  gpt-5_no_knowledge   novo       0.763889      0.043699           0.651780   \n",
      "31  gpt-5_no_knowledge   relv       0.151042      0.025416           0.934225   \n",
      "32   gpt-5_short_prior   imsc       0.148872      0.011376           0.940559   \n",
      "33   gpt-5_short_prior   info       0.349826      0.013172           0.840327   \n",
      "34   gpt-5_short_prior   novo       0.648872      0.037880           0.694408   \n",
      "35   gpt-5_short_prior   relv       0.125434      0.006682           0.953944   \n",
      "\n",
      "    pearson_soft_std  \n",
      "20          0.007740  \n",
      "21          0.016194  \n",
      "22          0.009774  \n",
      "23          0.014365  \n",
      "24          0.012586  \n",
      "25          0.005682  \n",
      "26          0.018964  \n",
      "27          0.005614  \n",
      "28          0.013250  \n",
      "29          0.012817  \n",
      "30          0.009250  \n",
      "31          0.017328  \n",
      "32          0.010506  \n",
      "33          0.001588  \n",
      "34          0.007500  \n",
      "35          0.002081  \n"
     ]
    }
   ],
   "source": [
    "print(agg_soft[~agg_soft[\"method\"].str.contains(\"mini\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8a0140cc-5895-4477-ae38-411533bf79f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.376302</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>0.765196</td>\n",
       "      <td>0.026048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.353733</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.804881</td>\n",
       "      <td>0.002851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.546007</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.757628</td>\n",
       "      <td>0.003901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.508681</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>0.779155</td>\n",
       "      <td>0.005449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>info</td>\n",
       "      <td>0.360677</td>\n",
       "      <td>0.026717</td>\n",
       "      <td>0.795793</td>\n",
       "      <td>0.017345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.265191</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.859865</td>\n",
       "      <td>0.016194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.264757</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.879990</td>\n",
       "      <td>0.005682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.393663</td>\n",
       "      <td>0.023630</td>\n",
       "      <td>0.819374</td>\n",
       "      <td>0.012817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.349826</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>0.840327</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "1           gpt-5-mini_full   info       0.376302      0.033854   \n",
       "5         gpt-5-mini_memory   info       0.353733      0.007171   \n",
       "9   gpt-5-mini_no_knowledge   info       0.546007      0.008472   \n",
       "13   gpt-5-mini_short_prior   info       0.508681      0.010605   \n",
       "17       gpt-5-mini_summary   info       0.360677      0.026717   \n",
       "21               gpt-5_full   info       0.265191      0.021049   \n",
       "25             gpt-5_memory   info       0.264757      0.006682   \n",
       "29       gpt-5_no_knowledge   info       0.393663      0.023630   \n",
       "33        gpt-5_short_prior   info       0.349826      0.013172   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "1            0.765196          0.026048  \n",
       "5            0.804881          0.002851  \n",
       "9            0.757628          0.003901  \n",
       "13           0.779155          0.005449  \n",
       "17           0.795793          0.017345  \n",
       "21           0.859865          0.016194  \n",
       "25           0.879990          0.005682  \n",
       "29           0.819374          0.012817  \n",
       "33           0.840327          0.001588  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d679e14f-286d-40ba-aaa5-20fdb5eb2a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.547309</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.702598</td>\n",
       "      <td>0.025356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.500868</td>\n",
       "      <td>0.037203</td>\n",
       "      <td>0.696973</td>\n",
       "      <td>0.034313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.665365</td>\n",
       "      <td>0.031114</td>\n",
       "      <td>0.645999</td>\n",
       "      <td>0.021472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.524740</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.687118</td>\n",
       "      <td>0.005896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.055899</td>\n",
       "      <td>0.703920</td>\n",
       "      <td>0.050761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.352865</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.807290</td>\n",
       "      <td>0.009774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.321181</td>\n",
       "      <td>0.027509</td>\n",
       "      <td>0.842451</td>\n",
       "      <td>0.018964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.043699</td>\n",
       "      <td>0.651780</td>\n",
       "      <td>0.009250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.648872</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>0.694408</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "2           gpt-5-mini_full   novo       0.547309      0.024162   \n",
       "6         gpt-5-mini_memory   novo       0.500868      0.037203   \n",
       "10  gpt-5-mini_no_knowledge   novo       0.665365      0.031114   \n",
       "14   gpt-5-mini_short_prior   novo       0.524740      0.005676   \n",
       "18       gpt-5-mini_summary   novo       0.531250      0.055899   \n",
       "22               gpt-5_full   novo       0.352865      0.009115   \n",
       "26             gpt-5_memory   novo       0.321181      0.027509   \n",
       "30       gpt-5_no_knowledge   novo       0.763889      0.043699   \n",
       "34        gpt-5_short_prior   novo       0.648872      0.037880   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "2            0.702598          0.025356  \n",
       "6            0.696973          0.034313  \n",
       "10           0.645999          0.021472  \n",
       "14           0.687118          0.005896  \n",
       "18           0.703920          0.050761  \n",
       "22           0.807290          0.009774  \n",
       "26           0.842451          0.018964  \n",
       "30           0.651780          0.009250  \n",
       "34           0.694408          0.007500  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"novo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f656957-89d7-4834-a609-e597b53d995e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.258247</td>\n",
       "      <td>0.015203</td>\n",
       "      <td>0.852382</td>\n",
       "      <td>0.029449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.257378</td>\n",
       "      <td>0.013043</td>\n",
       "      <td>0.859764</td>\n",
       "      <td>0.012827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.867430</td>\n",
       "      <td>0.005688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.264323</td>\n",
       "      <td>0.006890</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.002971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.276042</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.840253</td>\n",
       "      <td>0.013804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.159288</td>\n",
       "      <td>0.025857</td>\n",
       "      <td>0.934821</td>\n",
       "      <td>0.014365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.130642</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>0.948530</td>\n",
       "      <td>0.005614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.025416</td>\n",
       "      <td>0.934225</td>\n",
       "      <td>0.017328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.125434</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.953944</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "3           gpt-5-mini_full   relv       0.258247      0.015203   \n",
       "7         gpt-5-mini_memory   relv       0.257378      0.013043   \n",
       "11  gpt-5-mini_no_knowledge   relv       0.246094      0.011573   \n",
       "15   gpt-5-mini_short_prior   relv       0.264323      0.006890   \n",
       "19       gpt-5-mini_summary   relv       0.276042      0.018914   \n",
       "23               gpt-5_full   relv       0.159288      0.025857   \n",
       "27             gpt-5_memory   relv       0.130642      0.012512   \n",
       "31       gpt-5_no_knowledge   relv       0.151042      0.025416   \n",
       "35        gpt-5_short_prior   relv       0.125434      0.006682   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "3            0.852382          0.029449  \n",
       "7            0.859764          0.012827  \n",
       "11           0.867430          0.005688  \n",
       "15           0.857143          0.002971  \n",
       "19           0.840253          0.013804  \n",
       "23           0.934821          0.014365  \n",
       "27           0.948530          0.005614  \n",
       "31           0.934225          0.017328  \n",
       "35           0.953944          0.002081  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"relv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f0812db4-25c7-471a-b72c-c1d5f7a2c57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.318576</td>\n",
       "      <td>0.028596</td>\n",
       "      <td>0.810826</td>\n",
       "      <td>0.014752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.008132</td>\n",
       "      <td>0.852342</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.279080</td>\n",
       "      <td>0.011074</td>\n",
       "      <td>0.835004</td>\n",
       "      <td>0.014850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.019063</td>\n",
       "      <td>0.855622</td>\n",
       "      <td>0.013018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.017027</td>\n",
       "      <td>0.843369</td>\n",
       "      <td>0.019169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.164931</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>0.922658</td>\n",
       "      <td>0.007740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.012586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.154514</td>\n",
       "      <td>0.015751</td>\n",
       "      <td>0.932658</td>\n",
       "      <td>0.013250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.148872</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.010506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "0           gpt-5-mini_full   imsc       0.318576      0.028596   \n",
       "4         gpt-5-mini_memory   imsc       0.269531      0.008132   \n",
       "8   gpt-5-mini_no_knowledge   imsc       0.279080      0.011074   \n",
       "12   gpt-5-mini_short_prior   imsc       0.263889      0.019063   \n",
       "16       gpt-5-mini_summary   imsc       0.261719      0.017027   \n",
       "20               gpt-5_full   imsc       0.164931      0.011226   \n",
       "24             gpt-5_memory   imsc       0.156250      0.013718   \n",
       "28       gpt-5_no_knowledge   imsc       0.154514      0.015751   \n",
       "32        gpt-5_short_prior   imsc       0.148872      0.011376   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "0            0.810826          0.014752  \n",
       "4            0.852342          0.008460  \n",
       "8            0.835004          0.014850  \n",
       "12           0.855622          0.013018  \n",
       "16           0.843369          0.019169  \n",
       "20           0.922658          0.007740  \n",
       "24           0.930558          0.012586  \n",
       "28           0.932658          0.013250  \n",
       "32           0.940559          0.010506  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"imsc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca097d-5710-49d7-9605-3cb9a104f24a",
   "metadata": {},
   "source": [
    "## With human context enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "11af8149-70db-4bd7-ada4-680e06d433db",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_run_metrics, summary_metrics = evaluate_llm_runs_for_corpus(\n",
    "    ratings_per_run=[rating_hm],\n",
    "    soft_df=soft_df,\n",
    "    hard_df=hard_df,\n",
    "    corpus=\"insq\",   # or None for all\n",
    "    aspects=['info'],\n",
    "    models_per_run=[[\"gpt-5_hm\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ccb96c03-feee-4e6a-a411-8e1e8d974c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5_hm_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.437370</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.566073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5_hm_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.431901</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.577087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5_hm_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.448047</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.555816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-5_hm_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.438151</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.552543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5_hm_summary</td>\n",
       "      <td>info</td>\n",
       "      <td>0.445443</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.558737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  method aspect  mae_soft_mean  mae_to_hard_mean  \\\n",
       "0          gpt-5_hm_full   info       0.437370          0.453125   \n",
       "1        gpt-5_hm_memory   info       0.431901          0.429688   \n",
       "2  gpt-5_hm_no_knowledge   info       0.448047          0.468750   \n",
       "3   gpt-5_hm_short_prior   info       0.438151          0.468750   \n",
       "4       gpt-5_hm_summary   info       0.445443          0.453125   \n",
       "\n",
       "   macro_f1_hard_mean  \n",
       "0            0.566073  \n",
       "1            0.577087  \n",
       "2            0.555816  \n",
       "3            0.552543  \n",
       "4            0.558737  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[['method', 'aspect', 'mae_soft_mean', 'mae_to_hard_mean',\n",
    "        'macro_f1_hard_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cb316-2f33-48b9-8f15-dafeb7fac524",
   "metadata": {},
   "source": [
    "## FORA prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e5a9bdb8-5491-4c4b-bb09-095a2619bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_run_metrics, summary_metrics, llm_runs = evaluate_llm_runs_for_corpus(\n",
    "    ratings_per_run=ratings,\n",
    "    soft_df=soft_df,\n",
    "    hard_df=hard_df,\n",
    "    corpus=\"fora\",   # or None for all\n",
    "    aspects=ASPECTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f0311f9f-bc89-4fc2-8c6c-3c6c26beabe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.568513</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.685212</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.589989</td>\n",
       "      <td>0.011220</td>\n",
       "      <td>0.491251</td>\n",
       "      <td>0.018457</td>\n",
       "      <td>0.613705</td>\n",
       "      <td>0.003308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.599397</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>0.676139</td>\n",
       "      <td>0.023571</td>\n",
       "      <td>0.628328</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.449885</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>0.592377</td>\n",
       "      <td>0.023539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.655485</td>\n",
       "      <td>0.019434</td>\n",
       "      <td>0.697273</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>0.711395</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.411041</td>\n",
       "      <td>0.014033</td>\n",
       "      <td>0.564846</td>\n",
       "      <td>0.030226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.626376</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>0.676095</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>0.669862</td>\n",
       "      <td>0.019521</td>\n",
       "      <td>0.436980</td>\n",
       "      <td>0.024279</td>\n",
       "      <td>0.569118</td>\n",
       "      <td>0.008052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>info</td>\n",
       "      <td>0.582357</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.653148</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.602769</td>\n",
       "      <td>0.018720</td>\n",
       "      <td>0.483622</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.593888</td>\n",
       "      <td>0.010183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.547568</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>0.555911</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>0.517994</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.648396</td>\n",
       "      <td>0.008920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.559283</td>\n",
       "      <td>0.020244</td>\n",
       "      <td>0.697597</td>\n",
       "      <td>0.024855</td>\n",
       "      <td>0.576145</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.503297</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.641557</td>\n",
       "      <td>0.030001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.613241</td>\n",
       "      <td>0.026801</td>\n",
       "      <td>0.691179</td>\n",
       "      <td>0.024199</td>\n",
       "      <td>0.647497</td>\n",
       "      <td>0.015098</td>\n",
       "      <td>0.460448</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.608384</td>\n",
       "      <td>0.014651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.599752</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.681104</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.617678</td>\n",
       "      <td>0.016081</td>\n",
       "      <td>0.486862</td>\n",
       "      <td>0.019761</td>\n",
       "      <td>0.609780</td>\n",
       "      <td>0.007181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>info</td>\n",
       "      <td>0.561413</td>\n",
       "      <td>0.026970</td>\n",
       "      <td>0.679718</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.568690</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.506363</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.635218</td>\n",
       "      <td>0.008287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "1           gpt-5-mini_full   info       0.568513      0.005033   \n",
       "5         gpt-5-mini_memory   info       0.599397      0.007993   \n",
       "9   gpt-5-mini_no_knowledge   info       0.655485      0.019434   \n",
       "13   gpt-5-mini_short_prior   info       0.626376      0.013527   \n",
       "17       gpt-5-mini_summary   info       0.582357      0.017084   \n",
       "21               gpt-5_full   info       0.547568      0.008542   \n",
       "25             gpt-5_memory   info       0.559283      0.020244   \n",
       "29       gpt-5_no_knowledge   info       0.613241      0.026801   \n",
       "33        gpt-5_short_prior   info       0.599752      0.005865   \n",
       "37            gpt-5_summary   info       0.561413      0.026970   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "1            0.685212          0.007974          0.589989         0.011220   \n",
       "5            0.676139          0.023571          0.628328         0.012912   \n",
       "9            0.697273          0.015671          0.711395         0.024401   \n",
       "13           0.676095          0.009211          0.669862         0.019521   \n",
       "17           0.653148          0.003646          0.602769         0.018720   \n",
       "21           0.693578          0.005629          0.555911         0.011067   \n",
       "25           0.697597          0.024855          0.576145         0.020290   \n",
       "29           0.691179          0.024199          0.647497         0.015098   \n",
       "33           0.681104          0.002643          0.617678         0.016081   \n",
       "37           0.679718          0.019722          0.568690         0.008453   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "1             0.491251           0.018457       0.613705      0.003308  \n",
       "5             0.449885           0.013175       0.592377      0.023539  \n",
       "9             0.411041           0.014033       0.564846      0.030226  \n",
       "13            0.436980           0.024279       0.569118      0.008052  \n",
       "17            0.483622           0.006845       0.593888      0.010183  \n",
       "21            0.517994           0.012286       0.648396      0.008920  \n",
       "25            0.503297           0.013288       0.641557      0.030001  \n",
       "29            0.460448           0.001705       0.608384      0.014651  \n",
       "33            0.486862           0.019761       0.609780      0.007181  \n",
       "37            0.506363           0.008685       0.635218      0.008287  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6064fd8b-e111-4874-bb6f-4d955f396c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.621494</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.633313</td>\n",
       "      <td>0.022967</td>\n",
       "      <td>0.551651</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.446968</td>\n",
       "      <td>0.012680</td>\n",
       "      <td>0.597366</td>\n",
       "      <td>0.025474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.598243</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>0.626714</td>\n",
       "      <td>0.020108</td>\n",
       "      <td>0.533546</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.481406</td>\n",
       "      <td>0.018169</td>\n",
       "      <td>0.591338</td>\n",
       "      <td>0.022674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.566471</td>\n",
       "      <td>0.020334</td>\n",
       "      <td>0.633079</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.552716</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.487374</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.596527</td>\n",
       "      <td>0.011283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.563987</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>0.646175</td>\n",
       "      <td>0.033136</td>\n",
       "      <td>0.533546</td>\n",
       "      <td>0.019434</td>\n",
       "      <td>0.497290</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>0.627840</td>\n",
       "      <td>0.024892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.672080</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.604794</td>\n",
       "      <td>0.027980</td>\n",
       "      <td>0.587859</td>\n",
       "      <td>0.026151</td>\n",
       "      <td>0.423503</td>\n",
       "      <td>0.028783</td>\n",
       "      <td>0.554398</td>\n",
       "      <td>0.030374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.591143</td>\n",
       "      <td>0.013155</td>\n",
       "      <td>0.690685</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.577210</td>\n",
       "      <td>0.025824</td>\n",
       "      <td>0.494407</td>\n",
       "      <td>0.015378</td>\n",
       "      <td>0.647763</td>\n",
       "      <td>0.019069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.595048</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.684864</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.593184</td>\n",
       "      <td>0.015760</td>\n",
       "      <td>0.491482</td>\n",
       "      <td>0.008661</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.007847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.714679</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.639289</td>\n",
       "      <td>0.006854</td>\n",
       "      <td>0.772098</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.391002</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.530388</td>\n",
       "      <td>0.009856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.636936</td>\n",
       "      <td>0.020515</td>\n",
       "      <td>0.670863</td>\n",
       "      <td>0.014340</td>\n",
       "      <td>0.673056</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.435121</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.589529</td>\n",
       "      <td>0.013535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.588836</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>0.684961</td>\n",
       "      <td>0.015346</td>\n",
       "      <td>0.570820</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>0.496477</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>0.646030</td>\n",
       "      <td>0.012636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "2           gpt-5-mini_full   novo       0.621494      0.027614   \n",
       "6         gpt-5-mini_memory   novo       0.598243      0.016247   \n",
       "10  gpt-5-mini_no_knowledge   novo       0.566471      0.020334   \n",
       "14   gpt-5-mini_short_prior   novo       0.563987      0.026891   \n",
       "18       gpt-5-mini_summary   novo       0.672080      0.018115   \n",
       "22               gpt-5_full   novo       0.591143      0.013155   \n",
       "26             gpt-5_memory   novo       0.595048      0.007320   \n",
       "30       gpt-5_no_knowledge   novo       0.714679      0.005694   \n",
       "34        gpt-5_short_prior   novo       0.636936      0.020515   \n",
       "38            gpt-5_summary   novo       0.588836      0.016578   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "2            0.633313          0.022967          0.551651         0.023112   \n",
       "6            0.626714          0.020108          0.533546         0.008453   \n",
       "10           0.633079          0.006013          0.552716         0.028754   \n",
       "14           0.646175          0.033136          0.533546         0.019434   \n",
       "18           0.604794          0.027980          0.587859         0.026151   \n",
       "22           0.690685          0.012696          0.577210         0.025824   \n",
       "26           0.684864          0.001142          0.593184         0.015760   \n",
       "30           0.639289          0.006854          0.772098         0.012912   \n",
       "34           0.670863          0.014340          0.673056         0.024401   \n",
       "38           0.684961          0.015346          0.570820         0.016395   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "2             0.446968           0.012680       0.597366      0.025474  \n",
       "6             0.481406           0.018169       0.591338      0.022674  \n",
       "10            0.487374           0.018890       0.596527      0.011283  \n",
       "14            0.497290           0.005314       0.627840      0.024892  \n",
       "18            0.423503           0.028783       0.554398      0.030374  \n",
       "22            0.494407           0.015378       0.647763      0.019069  \n",
       "26            0.491482           0.008661       0.628527      0.007847  \n",
       "30            0.391002           0.011461       0.530388      0.009856  \n",
       "34            0.435121           0.019638       0.589529      0.013535  \n",
       "38            0.496477           0.013274       0.646030      0.012636  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"novo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b3be638-9a15-443d-9f91-d5a2ed5aded0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.463348</td>\n",
       "      <td>0.023750</td>\n",
       "      <td>0.661619</td>\n",
       "      <td>0.052877</td>\n",
       "      <td>0.528222</td>\n",
       "      <td>0.031520</td>\n",
       "      <td>0.388462</td>\n",
       "      <td>0.039728</td>\n",
       "      <td>0.542799</td>\n",
       "      <td>0.045806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.471512</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.641784</td>\n",
       "      <td>0.035325</td>\n",
       "      <td>0.531416</td>\n",
       "      <td>0.019258</td>\n",
       "      <td>0.404373</td>\n",
       "      <td>0.060426</td>\n",
       "      <td>0.518755</td>\n",
       "      <td>0.022329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.453230</td>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.676744</td>\n",
       "      <td>0.027390</td>\n",
       "      <td>0.520767</td>\n",
       "      <td>0.024953</td>\n",
       "      <td>0.415989</td>\n",
       "      <td>0.038980</td>\n",
       "      <td>0.557608</td>\n",
       "      <td>0.031781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.458023</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.666039</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.525027</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.391506</td>\n",
       "      <td>0.025514</td>\n",
       "      <td>0.540538</td>\n",
       "      <td>0.001740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.479322</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.639119</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.542066</td>\n",
       "      <td>0.020540</td>\n",
       "      <td>0.389819</td>\n",
       "      <td>0.039554</td>\n",
       "      <td>0.512072</td>\n",
       "      <td>0.024326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.446663</td>\n",
       "      <td>0.029863</td>\n",
       "      <td>0.717888</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>0.456869</td>\n",
       "      <td>0.026151</td>\n",
       "      <td>0.510348</td>\n",
       "      <td>0.015635</td>\n",
       "      <td>0.685727</td>\n",
       "      <td>0.032421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.437433</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.706873</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.446219</td>\n",
       "      <td>0.011220</td>\n",
       "      <td>0.514102</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.677977</td>\n",
       "      <td>0.017317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.436546</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>0.736356</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>0.446219</td>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.522047</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.703597</td>\n",
       "      <td>0.008829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.431754</td>\n",
       "      <td>0.007346</td>\n",
       "      <td>0.737552</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.436635</td>\n",
       "      <td>0.016081</td>\n",
       "      <td>0.525334</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.709742</td>\n",
       "      <td>0.010022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.435659</td>\n",
       "      <td>0.015371</td>\n",
       "      <td>0.727480</td>\n",
       "      <td>0.020858</td>\n",
       "      <td>0.439830</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.512399</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.699074</td>\n",
       "      <td>0.015987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "3           gpt-5-mini_full   relv       0.463348      0.023750   \n",
       "7         gpt-5-mini_memory   relv       0.471512      0.010808   \n",
       "11  gpt-5-mini_no_knowledge   relv       0.453230      0.024158   \n",
       "15   gpt-5-mini_short_prior   relv       0.458023      0.007796   \n",
       "19       gpt-5-mini_summary   relv       0.479322      0.011387   \n",
       "23               gpt-5_full   relv       0.446663      0.029863   \n",
       "27             gpt-5_memory   relv       0.437433      0.014824   \n",
       "31       gpt-5_no_knowledge   relv       0.436546      0.010439   \n",
       "35        gpt-5_short_prior   relv       0.431754      0.007346   \n",
       "39            gpt-5_summary   relv       0.435659      0.015371   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "3            0.661619          0.052877          0.528222         0.031520   \n",
       "7            0.641784          0.035325          0.531416         0.019258   \n",
       "11           0.676744          0.027390          0.520767         0.024953   \n",
       "15           0.666039          0.008160          0.525027         0.012912   \n",
       "19           0.639119          0.016921          0.542066         0.020540   \n",
       "23           0.717888          0.037611          0.456869         0.026151   \n",
       "27           0.706873          0.024705          0.446219         0.011220   \n",
       "31           0.736356          0.009528          0.446219         0.007378   \n",
       "35           0.737552          0.006949          0.436635         0.016081   \n",
       "39           0.727480          0.020858          0.439830         0.009761   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "3             0.388462           0.039728       0.542799      0.045806  \n",
       "7             0.404373           0.060426       0.518755      0.022329  \n",
       "11            0.415989           0.038980       0.557608      0.031781  \n",
       "15            0.391506           0.025514       0.540538      0.001740  \n",
       "19            0.389819           0.039554       0.512072      0.024326  \n",
       "23            0.510348           0.015635       0.685727      0.032421  \n",
       "27            0.514102           0.013604       0.677977      0.017317  \n",
       "31            0.522047           0.001795       0.703597      0.008829  \n",
       "35            0.525334           0.018091       0.709742      0.010022  \n",
       "39            0.512399           0.010162       0.699074      0.015987  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"relv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7598b105-0641-4b37-99de-f5ac26e8d54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "      <th>mae_to_hard_mean</th>\n",
       "      <th>mae_to_hard_std</th>\n",
       "      <th>macro_f1_hard_mean</th>\n",
       "      <th>macro_f1_hard_std</th>\n",
       "      <th>qwk_hard_mean</th>\n",
       "      <th>qwk_hard_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.417909</td>\n",
       "      <td>0.026283</td>\n",
       "      <td>0.690299</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.016081</td>\n",
       "      <td>0.573227</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>0.647185</td>\n",
       "      <td>0.009105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.426251</td>\n",
       "      <td>0.016533</td>\n",
       "      <td>0.678207</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.436635</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.542215</td>\n",
       "      <td>0.017796</td>\n",
       "      <td>0.616529</td>\n",
       "      <td>0.023795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.434949</td>\n",
       "      <td>0.017109</td>\n",
       "      <td>0.661868</td>\n",
       "      <td>0.027063</td>\n",
       "      <td>0.428115</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.528815</td>\n",
       "      <td>0.039067</td>\n",
       "      <td>0.610018</td>\n",
       "      <td>0.025787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.429269</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.669742</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>0.434505</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.537667</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.614859</td>\n",
       "      <td>0.009124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.443291</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.664808</td>\n",
       "      <td>0.030221</td>\n",
       "      <td>0.447284</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>0.547736</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.608629</td>\n",
       "      <td>0.033438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.478967</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.668598</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>0.479233</td>\n",
       "      <td>0.035577</td>\n",
       "      <td>0.529025</td>\n",
       "      <td>0.026775</td>\n",
       "      <td>0.604347</td>\n",
       "      <td>0.022576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.488019</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>0.653676</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>0.519702</td>\n",
       "      <td>0.013301</td>\n",
       "      <td>0.511917</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.561474</td>\n",
       "      <td>0.008968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.488729</td>\n",
       "      <td>0.021335</td>\n",
       "      <td>0.646948</td>\n",
       "      <td>0.030257</td>\n",
       "      <td>0.489883</td>\n",
       "      <td>0.027172</td>\n",
       "      <td>0.528081</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.585701</td>\n",
       "      <td>0.038042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.493344</td>\n",
       "      <td>0.026319</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>0.040062</td>\n",
       "      <td>0.503727</td>\n",
       "      <td>0.028635</td>\n",
       "      <td>0.518001</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.579393</td>\n",
       "      <td>0.037876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>gpt-5_summary</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.474175</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.677162</td>\n",
       "      <td>0.017040</td>\n",
       "      <td>0.499468</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.513658</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.597205</td>\n",
       "      <td>0.025438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "0           gpt-5-mini_full   imsc       0.417909      0.026283   \n",
       "4         gpt-5-mini_memory   imsc       0.426251      0.016533   \n",
       "8   gpt-5-mini_no_knowledge   imsc       0.434949      0.017109   \n",
       "12   gpt-5-mini_short_prior   imsc       0.429269      0.003925   \n",
       "16       gpt-5-mini_summary   imsc       0.443291      0.017014   \n",
       "20               gpt-5_full   imsc       0.478967      0.022723   \n",
       "24             gpt-5_memory   imsc       0.488019      0.012096   \n",
       "28       gpt-5_no_knowledge   imsc       0.488729      0.021335   \n",
       "32        gpt-5_short_prior   imsc       0.493344      0.026319   \n",
       "36            gpt-5_summary   imsc       0.474175      0.008301   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  mae_to_hard_mean  mae_to_hard_std  \\\n",
       "0            0.690299          0.014815          0.407881         0.016081   \n",
       "4            0.678207          0.025441          0.436635         0.017596   \n",
       "8            0.661868          0.027063          0.428115         0.017788   \n",
       "12           0.669742          0.008691          0.434505         0.006390   \n",
       "16           0.664808          0.030221          0.447284         0.016601   \n",
       "20           0.668598          0.015714          0.479233         0.035577   \n",
       "24           0.653676          0.014616          0.519702         0.013301   \n",
       "28           0.646948          0.030257          0.489883         0.027172   \n",
       "32           0.646067          0.040062          0.503727         0.028635   \n",
       "36           0.677162          0.017040          0.499468         0.017596   \n",
       "\n",
       "    macro_f1_hard_mean  macro_f1_hard_std  qwk_hard_mean  qwk_hard_std  \n",
       "0             0.573227           0.025349       0.647185      0.009105  \n",
       "4             0.542215           0.017796       0.616529      0.023795  \n",
       "8             0.528815           0.039067       0.610018      0.025787  \n",
       "12            0.537667           0.011928       0.614859      0.009124  \n",
       "16            0.547736           0.006646       0.608629      0.033438  \n",
       "20            0.529025           0.026775       0.604347      0.022576  \n",
       "24            0.511917           0.020967       0.561474      0.008968  \n",
       "28            0.528081           0.004051       0.585701      0.038042  \n",
       "32            0.518001           0.016879       0.579393      0.037876  \n",
       "36            0.513658           0.013346       0.597205      0.025438  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics[summary_metrics.aspect == \"imsc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7812ee-0308-4048-9173-2214352c2582",
   "metadata": {},
   "source": [
    "## Using GPT-5_summary as the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9469bcc8-d6bb-4647-9aa3-6d979f0f1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_run_soft, agg_soft = evaluate_against_summary_reference(\n",
    "    llm_runs,\n",
    "    ref_method=\"gpt-5_summary\",\n",
    "    aspects=ASPECTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a077ecc3-1ddd-42aa-bbf8-ce30e9952e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                method aspect  mae_soft_mean  mae_soft_std  pearson_soft_mean  \\\n",
      "20          gpt-5_full   imsc       0.210508      0.026801           0.898222   \n",
      "21          gpt-5_full   info       0.207313      0.008064           0.912508   \n",
      "22          gpt-5_full   novo       0.279020      0.006390           0.878244   \n",
      "23          gpt-5_full   relv       0.150515      0.014769           0.941148   \n",
      "24        gpt-5_memory   imsc       0.202698      0.007480           0.897447   \n",
      "25        gpt-5_memory   info       0.220447      0.010159           0.908692   \n",
      "26        gpt-5_memory   novo       0.258786      0.019199           0.882531   \n",
      "27        gpt-5_memory   relv       0.145545      0.011875           0.940273   \n",
      "28  gpt-5_no_knowledge   imsc       0.205538      0.010159           0.896373   \n",
      "29  gpt-5_no_knowledge   info       0.335818      0.007704           0.851274   \n",
      "30  gpt-5_no_knowledge   novo       0.491658      0.024617           0.786127   \n",
      "31  gpt-5_no_knowledge   relv       0.166844      0.005360           0.931390   \n",
      "32   gpt-5_short_prior   imsc       0.194533      0.010343           0.905123   \n",
      "33   gpt-5_short_prior   info       0.292510      0.019790           0.868812   \n",
      "34   gpt-5_short_prior   novo       0.383387      0.031430           0.831401   \n",
      "35   gpt-5_short_prior   relv       0.148030      0.006390           0.936224   \n",
      "\n",
      "    pearson_soft_std  \n",
      "20          0.019346  \n",
      "21          0.001647  \n",
      "22          0.008391  \n",
      "23          0.007281  \n",
      "24          0.011110  \n",
      "25          0.003304  \n",
      "26          0.010567  \n",
      "27          0.009137  \n",
      "28          0.009741  \n",
      "29          0.002249  \n",
      "30          0.009847  \n",
      "31          0.007667  \n",
      "32          0.013098  \n",
      "33          0.006563  \n",
      "34          0.016454  \n",
      "35          0.003445  \n"
     ]
    }
   ],
   "source": [
    "print(agg_soft[~agg_soft[\"method\"].str.contains(\"mini\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3cbfe774-ef97-4192-b573-cc10ce454c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.314519</td>\n",
       "      <td>0.023194</td>\n",
       "      <td>0.843009</td>\n",
       "      <td>0.017787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.372737</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.798036</td>\n",
       "      <td>0.006630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.441605</td>\n",
       "      <td>0.036151</td>\n",
       "      <td>0.816094</td>\n",
       "      <td>0.021758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.393681</td>\n",
       "      <td>0.033834</td>\n",
       "      <td>0.804686</td>\n",
       "      <td>0.015157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>info</td>\n",
       "      <td>0.311679</td>\n",
       "      <td>0.006419</td>\n",
       "      <td>0.841817</td>\n",
       "      <td>0.003899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>info</td>\n",
       "      <td>0.207313</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.912508</td>\n",
       "      <td>0.001647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>info</td>\n",
       "      <td>0.220447</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>0.908692</td>\n",
       "      <td>0.003304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>info</td>\n",
       "      <td>0.335818</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.851274</td>\n",
       "      <td>0.002249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>info</td>\n",
       "      <td>0.292510</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.868812</td>\n",
       "      <td>0.006563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "1           gpt-5-mini_full   info       0.314519      0.023194   \n",
       "5         gpt-5-mini_memory   info       0.372737      0.004642   \n",
       "9   gpt-5-mini_no_knowledge   info       0.441605      0.036151   \n",
       "13   gpt-5-mini_short_prior   info       0.393681      0.033834   \n",
       "17       gpt-5-mini_summary   info       0.311679      0.006419   \n",
       "21               gpt-5_full   info       0.207313      0.008064   \n",
       "25             gpt-5_memory   info       0.220447      0.010159   \n",
       "29       gpt-5_no_knowledge   info       0.335818      0.007704   \n",
       "33        gpt-5_short_prior   info       0.292510      0.019790   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "1            0.843009          0.017787  \n",
       "5            0.798036          0.006630  \n",
       "9            0.816094          0.021758  \n",
       "13           0.804686          0.015157  \n",
       "17           0.841817          0.003899  \n",
       "21           0.912508          0.001647  \n",
       "25           0.908692          0.003304  \n",
       "29           0.851274          0.002249  \n",
       "33           0.868812          0.006563  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "27207f22-fbcf-49b4-8dc3-e2bdbee40245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.460774</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>0.019759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.475328</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>0.759864</td>\n",
       "      <td>0.018313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.494498</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>0.729291</td>\n",
       "      <td>0.019792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.441250</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.770361</td>\n",
       "      <td>0.012271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.474263</td>\n",
       "      <td>0.019990</td>\n",
       "      <td>0.819769</td>\n",
       "      <td>0.016421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.279020</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.878244</td>\n",
       "      <td>0.008391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.258786</td>\n",
       "      <td>0.019199</td>\n",
       "      <td>0.882531</td>\n",
       "      <td>0.010567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.491658</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>0.786127</td>\n",
       "      <td>0.009847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>novo</td>\n",
       "      <td>0.383387</td>\n",
       "      <td>0.031430</td>\n",
       "      <td>0.831401</td>\n",
       "      <td>0.016454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "2           gpt-5-mini_full   novo       0.460774      0.035358   \n",
       "6         gpt-5-mini_memory   novo       0.475328      0.020383   \n",
       "10  gpt-5-mini_no_knowledge   novo       0.494498      0.020383   \n",
       "14   gpt-5-mini_short_prior   novo       0.441250      0.004304   \n",
       "18       gpt-5-mini_summary   novo       0.474263      0.019990   \n",
       "22               gpt-5_full   novo       0.279020      0.006390   \n",
       "26             gpt-5_memory   novo       0.258786      0.019199   \n",
       "30       gpt-5_no_knowledge   novo       0.491658      0.024617   \n",
       "34        gpt-5_short_prior   novo       0.383387      0.031430   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "2            0.792424          0.019759  \n",
       "6            0.759864          0.018313  \n",
       "10           0.729291          0.019792  \n",
       "14           0.770361          0.012271  \n",
       "18           0.819769          0.016421  \n",
       "22           0.878244          0.008391  \n",
       "26           0.882531          0.010567  \n",
       "30           0.786127          0.009847  \n",
       "34           0.831401          0.016454  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"novo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "441b740a-4e0f-4123-8579-36b54540c16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.293575</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.828583</td>\n",
       "      <td>0.011709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.313099</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.821978</td>\n",
       "      <td>0.020855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.296060</td>\n",
       "      <td>0.009466</td>\n",
       "      <td>0.841546</td>\n",
       "      <td>0.001599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.323749</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.009583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.322329</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>0.811185</td>\n",
       "      <td>0.009857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.150515</td>\n",
       "      <td>0.014769</td>\n",
       "      <td>0.941148</td>\n",
       "      <td>0.007281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.145545</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>0.940273</td>\n",
       "      <td>0.009137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.166844</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.931390</td>\n",
       "      <td>0.007667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>relv</td>\n",
       "      <td>0.148030</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.936224</td>\n",
       "      <td>0.003445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "3           gpt-5-mini_full   relv       0.293575      0.004919   \n",
       "7         gpt-5-mini_memory   relv       0.313099      0.004880   \n",
       "11  gpt-5-mini_no_knowledge   relv       0.296060      0.009466   \n",
       "15   gpt-5-mini_short_prior   relv       0.323749      0.008453   \n",
       "19       gpt-5-mini_summary   relv       0.322329      0.008868   \n",
       "23               gpt-5_full   relv       0.150515      0.014769   \n",
       "27             gpt-5_memory   relv       0.145545      0.011875   \n",
       "31       gpt-5_no_knowledge   relv       0.166844      0.005360   \n",
       "35        gpt-5_short_prior   relv       0.148030      0.006390   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "3            0.828583          0.011709  \n",
       "7            0.821978          0.020855  \n",
       "11           0.841546          0.001599  \n",
       "15           0.806306          0.009583  \n",
       "19           0.811185          0.009857  \n",
       "23           0.941148          0.007281  \n",
       "27           0.940273          0.009137  \n",
       "31           0.931390          0.007667  \n",
       "35           0.936224          0.003445  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"relv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f443be7f-15f9-4041-8f6b-f5729e420650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>aspect</th>\n",
       "      <th>mae_soft_mean</th>\n",
       "      <th>mae_soft_std</th>\n",
       "      <th>pearson_soft_mean</th>\n",
       "      <th>pearson_soft_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-mini_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.319489</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.794352</td>\n",
       "      <td>0.004575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5-mini_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.321974</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.793291</td>\n",
       "      <td>0.005907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-5-mini_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.346823</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>0.786730</td>\n",
       "      <td>0.021315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt-5-mini_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.319844</td>\n",
       "      <td>0.008608</td>\n",
       "      <td>0.795313</td>\n",
       "      <td>0.014292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt-5-mini_summary</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.341143</td>\n",
       "      <td>0.033548</td>\n",
       "      <td>0.792373</td>\n",
       "      <td>0.018766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt-5_full</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.210508</td>\n",
       "      <td>0.026801</td>\n",
       "      <td>0.898222</td>\n",
       "      <td>0.019346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gpt-5_memory</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.202698</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.897447</td>\n",
       "      <td>0.011110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gpt-5_no_knowledge</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.205538</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>0.896373</td>\n",
       "      <td>0.009741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gpt-5_short_prior</td>\n",
       "      <td>imsc</td>\n",
       "      <td>0.194533</td>\n",
       "      <td>0.010343</td>\n",
       "      <td>0.905123</td>\n",
       "      <td>0.013098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     method aspect  mae_soft_mean  mae_soft_std  \\\n",
       "0           gpt-5-mini_full   imsc       0.319489      0.011859   \n",
       "4         gpt-5-mini_memory   imsc       0.321974      0.005865   \n",
       "8   gpt-5-mini_no_knowledge   imsc       0.346823      0.021834   \n",
       "12   gpt-5-mini_short_prior   imsc       0.319844      0.008608   \n",
       "16       gpt-5-mini_summary   imsc       0.341143      0.033548   \n",
       "20               gpt-5_full   imsc       0.210508      0.026801   \n",
       "24             gpt-5_memory   imsc       0.202698      0.007480   \n",
       "28       gpt-5_no_knowledge   imsc       0.205538      0.010159   \n",
       "32        gpt-5_short_prior   imsc       0.194533      0.010343   \n",
       "\n",
       "    pearson_soft_mean  pearson_soft_std  \n",
       "0            0.794352          0.004575  \n",
       "4            0.793291          0.005907  \n",
       "8            0.786730          0.021315  \n",
       "12           0.795313          0.014292  \n",
       "16           0.792373          0.018766  \n",
       "20           0.898222          0.019346  \n",
       "24           0.897447          0.011110  \n",
       "28           0.896373          0.009741  \n",
       "32           0.905123          0.013098  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_soft[agg_soft.aspect == \"imsc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66dae33-06ef-4ffb-8bc0-2ab6bf0cb868",
   "metadata": {},
   "source": [
    "## Regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "88d389cb-d134-419f-9ba1-6f95189060c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              soft_mae  soft_rmse  soft_spearman  hard_acc  \\\n",
      "Ordinal::CumulativeLink            NaN        NaN        -0.0076    0.6116   \n",
      "Regression::Ridge               0.4444     0.5638         0.8183    0.5835   \n",
      "Regression::GradientBoosting    0.4302     0.5607         0.8124    0.5641   \n",
      "Regression::RandomForest        0.4448     0.5879         0.7964    0.5624   \n",
      "\n",
      "                              hard_f1_macro  hard_qwk  hard_mae  \n",
      "Ordinal::CumulativeLink              0.6051    0.7445    0.4218  \n",
      "Regression::Ridge                    0.5663    0.7163    0.4376  \n",
      "Regression::GradientBoosting         0.5422    0.7118    0.4622  \n",
      "Regression::RandomForest             0.5427    0.7032    0.4675  \n"
     ]
    }
   ],
   "source": [
    "# Predict \"info\" using length + [novo, relv, imsc]\n",
    "# - Regression (Ridge, RandomForest & optional GradientBoosting)\n",
    "# - Ordinal Regression (mord.LogisticIT or statsmodels OrderedModel; fallback to LogisticRegression if needed)\n",
    "#\n",
    "# Outputs a summary DataFrame with CV metrics:\n",
    "#   Regression: MAE, RMSE, Spearman (soft), plus QWK/Acc on rounded preds\n",
    "#   Ordinal:    QWK, Acc, Macro-F1, MAE (hard), plus Spearman (soft)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------- utilities ----------\n",
    "def _ensure_numeric(df: pd.DataFrame, cols):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _get_length_series(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"length\" in df.columns:\n",
    "        return pd.to_numeric(df[\"length\"], errors=\"coerce\")\n",
    "    # try to derive from text if present\n",
    "    for cand in [\"current_text\", \"text\", \"utterance\"]:\n",
    "        if cand in df.columns:\n",
    "            return df[cand].fillna(\"\").astype(str).str.len()\n",
    "    raise KeyError(\"Need a numeric 'length' column or a text column ('current_text'/'text'/'utterance') to derive length.\")\n",
    "\n",
    "def _clip_round_1_4(x):\n",
    "    return np.clip(np.rint(x), 1, 4).astype(int)\n",
    "\n",
    "def _soft_scores(y_true_soft, y_pred_soft):\n",
    "    mae = mean_absolute_error(y_true_soft, y_pred_soft)\n",
    "    rmse = float(np.sqrt(np.mean((y_true_soft - y_pred_soft) ** 2)))\n",
    "    spear = float(pd.Series(y_true_soft).corr(pd.Series(y_pred_soft), method=\"spearman\"))\n",
    "    return {\"soft_mae\": mae, \"soft_rmse\": rmse, \"soft_spearman\": spear}\n",
    "\n",
    "def _hard_scores(y_true_hard, y_pred_hard):\n",
    "    return {\n",
    "        \"hard_acc\": accuracy_score(y_true_hard, y_pred_hard),\n",
    "        \"hard_f1_macro\": f1_score(y_true_hard, y_pred_hard, average=\"macro\"),\n",
    "        \"hard_qwk\": cohen_kappa_score(y_true_hard, y_pred_hard, weights=\"quadratic\"),\n",
    "        \"hard_mae\": mean_absolute_error(y_true_hard, y_pred_hard),\n",
    "    }\n",
    "\n",
    "# Try to import ordinal libs\n",
    "_have_mord = False\n",
    "_have_statsmodels = False\n",
    "try:\n",
    "    import mord  # type: ignore\n",
    "    _have_mord = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "    import statsmodels.api as sm\n",
    "    _have_statsmodels = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def _fit_predict_ordinal(X_train, y_train_hard, X_test):\n",
    "    \"\"\"Return hard preds (1..4) using preferred ordinal model available.\"\"\"\n",
    "    classes = np.sort(np.unique(y_train_hard))\n",
    "    if _have_mord:\n",
    "        import mord\n",
    "        model = mord.LogisticIT(alpha=0.0, max_iter=1000)\n",
    "        model.fit(X_train, y_train_hard)\n",
    "        return model.predict(X_test)\n",
    "\n",
    "    elif _have_statsmodels:\n",
    "        # 0..K-1 encoding for statsmodels\n",
    "        y_min = classes.min()\n",
    "        y_train_idx = (y_train_hard - y_min).astype(int)\n",
    "\n",
    "        # (Optional) scaling helps optimization stability\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(X_train)\n",
    "        Xte = scaler.transform(X_test)\n",
    "\n",
    "        # IMPORTANT: no constant term for OrderedModel\n",
    "        mod = OrderedModel(y_train_idx, Xtr, distr=\"logit\")\n",
    "        res = mod.fit(method=\"bfgs\", disp=False)\n",
    "\n",
    "        # Predict class probabilities; pick argmax\n",
    "        probs = res.model.predict(res.params, exog=Xte)  # shape (n_obs, n_classes)\n",
    "        pred_idx = np.asarray(probs).argmax(axis=1)\n",
    "        return y_min + pred_idx\n",
    "\n",
    "    else:\n",
    "        # Fallback: not truly ordinal, but works when libs aren't available\n",
    "        clf = make_pipeline(StandardScaler(with_mean=False),\n",
    "                            LogisticRegression(max_iter=200, multi_class=\"auto\"))\n",
    "        clf.fit(X_train, y_train_hard)\n",
    "        return clf.predict(X_test)\n",
    "\n",
    "def evaluate_regression_vs_ordinal(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    feats = [\"novo\", \"relv\", \"imsc\"]\n",
    "    # derive 'length'\n",
    "    df = df.copy()\n",
    "    df[\"length\"] = _get_length_series(df)\n",
    "    Xcols = [\"length\"] + feats\n",
    "\n",
    "    need = Xcols + [\"info\"]\n",
    "    df = _ensure_numeric(df, need).dropna(subset=need)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows left after cleaning. Check your columns/NaNs.\")\n",
    "\n",
    "    X = df[Xcols].to_numpy(dtype=float)\n",
    "    y_soft = df[\"info\"].to_numpy(dtype=float)\n",
    "    y_hard = _clip_round_1_4(y_soft)  # if info already int, this is a no-op\n",
    "\n",
    "    # ---------- Regression models (predict continuous then round for hard scores) ----------\n",
    "    regressors = {\n",
    "        \"Ridge\": make_pipeline(StandardScaler(), Ridge(alpha=1.0, random_state=None)),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=300, max_depth=None, random_state=42, n_jobs=-1),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rows: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for name, reg in regressors.items():\n",
    "        y_true_soft_all, y_pred_soft_all = [], []\n",
    "        y_true_hard_all, y_pred_hard_all = [], []\n",
    "        for tr, te in kf.split(X):\n",
    "            reg.fit(X[tr], y_soft[tr])\n",
    "            pred_soft = reg.predict(X[te])\n",
    "            pred_hard = _clip_round_1_4(pred_soft)\n",
    "\n",
    "            y_true_soft_all.append(y_soft[te]); y_pred_soft_all.append(pred_soft)\n",
    "            y_true_hard_all.append(y_hard[te]); y_pred_hard_all.append(pred_hard)\n",
    "\n",
    "        y_true_soft_all = np.concatenate(y_true_soft_all)\n",
    "        y_pred_soft_all = np.concatenate(y_pred_soft_all)\n",
    "        y_true_hard_all = np.concatenate(y_true_hard_all)\n",
    "        y_pred_hard_all = np.concatenate(y_pred_hard_all)\n",
    "\n",
    "        res_soft = _soft_scores(y_true_soft_all, y_pred_soft_all)\n",
    "        res_hard = _hard_scores(y_true_hard_all, y_pred_hard_all)\n",
    "        rows[f\"Regression::{name}\"] = {**res_soft, **res_hard}\n",
    "\n",
    "    # ---------- Ordinal Regression (predict hard classes directly) ----------\n",
    "    # Use stratified CV to preserve class balance\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_true_hard_all, y_pred_hard_all = [], []\n",
    "    # we also compute a \"soft\" proxy by taking the predicted class as float for Spearman\n",
    "    y_pred_soft_proxy_all = []\n",
    "\n",
    "    for tr, te in skf.split(X, y_hard):\n",
    "        y_pred = _fit_predict_ordinal(X[tr], y_hard[tr], X[te])\n",
    "        y_true_hard_all.append(y_hard[te])\n",
    "        y_pred_hard_all.append(y_pred)\n",
    "        y_pred_soft_proxy_all.append(y_pred.astype(float))\n",
    "\n",
    "    y_true_hard_all = np.concatenate(y_true_hard_all)\n",
    "    y_pred_hard_all = np.concatenate(y_pred_hard_all)\n",
    "    y_pred_soft_proxy_all = np.concatenate(y_pred_soft_proxy_all)\n",
    "\n",
    "    ord_hard = _hard_scores(y_true_hard_all, y_pred_hard_all)\n",
    "    # Spearman only (proxy soft) â€” how well the ordinal ranks align\n",
    "    ord_soft = {\"soft_spearman\": float(pd.Series(y_soft).corr(pd.Series(y_pred_soft_proxy_all), method=\"spearman\"))}\n",
    "    rows[\"Ordinal::CumulativeLink\"] = {**ord_soft, **ord_hard}\n",
    "\n",
    "    out = pd.DataFrame.from_dict(rows, orient=\"index\").sort_values(\n",
    "        by=[\"hard_qwk\", \"soft_spearman\", \"soft_mae\"], ascending=[False, False, True]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# ---- run it ----\n",
    "summary = evaluate_regression_vs_ordinal(soft_df)\n",
    "print(summary.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b8dac-806a-4683-a42f-ce0323a198fa",
   "metadata": {},
   "source": [
    "## Claim level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "5f409b24-b651-4efd-8601-7eca2c2ddaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import statistics\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _strip_html(text: str) -> str:\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text or \"\")\n",
    "\n",
    "def _tokens(text: str) -> List[str]:\n",
    "    clean = _strip_html(text)\n",
    "    return re.findall(r\"[A-Za-z0-9][A-Za-z0-9'\\-]*\", clean)\n",
    "\n",
    "def _mean(xs: List[float], default: Optional[float]=None) -> float:\n",
    "    return (sum(xs) / len(xs)) if xs else (float(\"nan\") if default is None else default)\n",
    "\n",
    "def h_mean(xs: List[float], default: Optional[float]=None) -> float:\n",
    "    return statistics.harmonic_mean(xs) if xs else (float(\"nan\") if default is None else default)\n",
    "\n",
    "def _std(xs: List[float], default: Optional[float]=None) -> float:\n",
    "    if not xs or len(xs) < 2:\n",
    "        return float(\"nan\") if default is None else default\n",
    "    m = sum(xs)/len(xs)\n",
    "    return (sum((x - m) ** 2 for x in xs) / (len(xs) - 1)) ** 0.5\n",
    "\n",
    "def _prop_at_least(xs: List[float], thr: float, default: Optional[float]=None) -> float:\n",
    "    return (sum(1 for x in xs if x >= thr) / len(xs)) if xs else (float(\"nan\") if default is None else default)\n",
    "\n",
    "def _topk_mean(xs: List[float], k: int = 1, default: Optional[float]=None) -> float:\n",
    "    if not xs:\n",
    "        return float(\"nan\") if default is None else default\n",
    "    k = max(1, min(k, len(xs)))\n",
    "    return sum(sorted(xs, reverse=True)[:k]) / k\n",
    "\n",
    "def _topquatile_mean(xs: List[float], k: int = 1, default: Optional[float] = None) -> float:\n",
    "    \"\"\"\n",
    "    Top quartile mean:\n",
    "    - Take the highest 25% of values (ceil(n * 0.25), at least 1 when xs not empty)\n",
    "    - Return their average\n",
    "    \"\"\"\n",
    "    if not xs:\n",
    "        return float(\"nan\") if default is None else default\n",
    "\n",
    "    n = len(xs)\n",
    "    q = max(1, math.ceil(0.25 * k * n))  # number of items in the top quartile slice\n",
    "    return sum(sorted(xs, reverse=True)[:q]) / q\n",
    "\n",
    "def _entropy_from_counts(counts: Dict[str, int], default: Optional[float]=None) -> float:\n",
    "    total = sum(counts.values())\n",
    "    if total == 0:\n",
    "        return float(\"nan\") if default is None else default\n",
    "    ent = 0.0\n",
    "    for c in counts.values():\n",
    "        if c:\n",
    "            p = c / total\n",
    "            ent -= p * math.log(p + 1e-12, 2)\n",
    "    return ent\n",
    "\n",
    "VALID_MEM_ACTIONS = {\"ADD\", \"UPDATE\", \"NONE\"}\n",
    "VALID_LOGREL = {\"equivalent\", \"forward entailment\", \"backward entailment\", \"contradiction\", \"neutral\"}\n",
    "\n",
    "# ---------- main ----------\n",
    "def compute_utterance_features(\n",
    "    utt: Dict[str, Any],\n",
    "    good_cut: int = 3,\n",
    "    cig_dims: Tuple[str, str, str] = (\"informativeness\", \"relevance\", \"implication_scope\"),\n",
    "    fill_when_no_claims: Optional[float] = 1.0,  # set to None to return NaNs instead\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simple, generalisable utterance features with configurable handling when no claims exist.\n",
    "    - If n_claims==0: all claim-derived features are set to `fill_when_no_claims` (default 0.0),\n",
    "      and `has_claims=0`.\n",
    "    - Otherwise, standard aggregates are computed; any empty sub-vectors use the same fill.\n",
    "    \"\"\"\n",
    "    claims: List[Dict[str, Any]] = (utt or {}).get(\"claims_rating\", []) or []\n",
    "    text: str = (utt or {}).get(\"utterance_text\", \"\") or \"\"\n",
    "\n",
    "    n_claims = len(claims)\n",
    "    n_changes = len([c for c in claims if c[\"mem_action\"] != \"NONE\"])\n",
    "    tok_count = len(_tokens(text))\n",
    "    claim_density = n_claims / max(1, tok_count)\n",
    "    change_density = n_changes / max(1, tok_count)\n",
    "    # claim_density_100 = claim_density * 100.0\n",
    "\n",
    "    feats = {\n",
    "        \"has_claims\": 1 if n_claims > 0 else 0,\n",
    "        \"n_claims\": n_claims,\n",
    "        \"n_changes\": n_changes,\n",
    "        \"p_changes\": n_changes / n_claims if n_claims > 0 else 0,\n",
    "        \"utterance_token_count\": tok_count,\n",
    "        \"claim_density\": claim_density,\n",
    "        \"change_density\": change_density\n",
    "        # \"claim_density_per_100_tokens\": claim_density_100,\n",
    "    }\n",
    "\n",
    "    # Fast path: no claims -> uniform fill for claim-derived features\n",
    "    if n_claims == 0:\n",
    "        default = fill_when_no_claims\n",
    "        for prefix in (\"info\", \"novo\", \"relv\", \"imsc\"):\n",
    "            feats[f\"mean_{prefix}\"] = default\n",
    "            # feats[f\"std_{prefix}\"] = 0\n",
    "            feats[f\"max_{prefix}\"] = default\n",
    "            feats[f\"min_{prefix}\"] = default\n",
    "            feats[f\"median_{prefix}\"] = default\n",
    "            feats[f\"harmonic_mean_{prefix}\"] = 1\n",
    "            # feats[f\"prop_{prefix}_ge_{good_cut}\"] = default\n",
    "            feats[f\"top2_{prefix}\"] = default\n",
    "            feats[f\"top_quartile_mean_{prefix}\"] = default\n",
    "            feats[f\"top_half_mean_{prefix}\"] = default\n",
    "\n",
    "        # feats[\"mean_cig_strict_prod\"] = default\n",
    "        # feats[\"agreement_rate_all_ge_cut\"] = default\n",
    "        # feats[\"var_novo\"] = default\n",
    "\n",
    "        # Memory/logic distributions\n",
    "        feats[\"mem_action_count_total\"] = 0\n",
    "        for k in sorted(VALID_MEM_ACTIONS):\n",
    "            # feats[f\"mem_action_count_{k}\"] = 0\n",
    "            feats[f\"mem_action_prop_{k}\"] = default\n",
    "        feats[\"mem_action_entropy\"] = default\n",
    "\n",
    "        feats[\"logical_relation_count_total\"] = 0\n",
    "        for k in sorted(VALID_LOGREL):\n",
    "            k2 = k.replace(\" \", \"_\")\n",
    "            # feats[f\"logical_relation_count_{k2}\"] = 0\n",
    "            feats[f\"logical_relation_prop_{k2}\"] = default\n",
    "        feats[\"logical_relation_entropy\"] = default\n",
    "\n",
    "        return feats\n",
    "\n",
    "    # Claims exist â†’ compute aggregates\n",
    "    info = [c.get(\"informativeness\") for c in claims if c.get(\"informativeness\") is not None]\n",
    "    novo = [c.get(\"novelty\") for c in claims if c.get(\"novelty\") is not None]\n",
    "    relv = [c.get(\"relevance\") for c in claims if c.get(\"relevance\") is not None]\n",
    "    imsc = [c.get(\"implication_scope\") for c in claims if c.get(\"implication_scope\") is not None]\n",
    "\n",
    "    def add_dim(prefix: str, xs: List[float]):\n",
    "        feats[f\"mean_{prefix}\"] = _mean(xs, default=fill_when_no_claims)\n",
    "        # feats[f\"std_{prefix}\"] = _std(xs, default=fill_when_no_claims)\n",
    "        feats[f\"max_{prefix}\"] = (max(xs) if xs else (fill_when_no_claims if fill_when_no_claims is not None else float(\"nan\")))\n",
    "        feats[f\"min_{prefix}\"] = (min(xs) if xs else (fill_when_no_claims if fill_when_no_claims is not None else float(\"nan\")))\n",
    "        feats[f\"median_{prefix}\"] = (np.median(xs) if xs else (fill_when_no_claims if fill_when_no_claims is not None else float(\"nan\")))\n",
    "        feats[f\"harmonic_mean_{prefix}\"] = h_mean(xs, default=fill_when_no_claims)\n",
    "        # feats[f\"prop_{prefix}_ge_{good_cut}\"] = _prop_at_least(xs, good_cut, default=fill_when_no_claims)\n",
    "        feats[f\"top2_{prefix}\"] = _topk_mean(xs, k=2, default=fill_when_no_claims)\n",
    "        feats[f\"top_quartile_mean_{prefix}\"] = _topquatile_mean(xs, k=1, default=fill_when_no_claims)\n",
    "        feats[f\"top_half_mean_{prefix}\"] = _topquatile_mean(xs, k=2, default=fill_when_no_claims)\n",
    "\n",
    "    add_dim(\"info\", info)\n",
    "    add_dim(\"novo\", novo)\n",
    "    add_dim(\"relv\", relv)\n",
    "    add_dim(\"imsc\", imsc)\n",
    "\n",
    "    # Cross-dimension strict product\n",
    "#     dimA, dimB, dimC = cig_dims\n",
    "#     per_claim_prod = []\n",
    "#     agreement_hi = 0\n",
    "#     for c in claims:\n",
    "#         a, b, d = c.get(dimA), c.get(dimB), c.get(dimC)\n",
    "#         if a is not None and b is not None and d is not None:\n",
    "#             per_claim_prod.append(a * b * d)\n",
    "#         if (\n",
    "#             (c.get(\"informativeness\", -1) >= good_cut)\n",
    "#             and (c.get(\"novelty\", -1) >= good_cut)\n",
    "#             and (c.get(\"relevance\", -1) >= good_cut)\n",
    "#             and (c.get(\"implication_scope\", -1) >= good_cut)\n",
    "#         ):\n",
    "#             agreement_hi += 1\n",
    "\n",
    "#     feats[\"mean_strict_prod\"] = _mean(per_claim_prod, default=fill_when_no_claims)\n",
    "    # feats[\"agreement_rate_all_ge_cut\"] = agreement_hi / n_claims if n_claims else (fill_when_no_claims if fill_when_no_claims is not None else float(\"nan\"))\n",
    "\n",
    "    # Novelty variance\n",
    "    # s = _std(novo, default=fill_when_no_claims)\n",
    "    # feats[\"var_novo\"] = (s**2 if (fill_when_no_claims is None and not math.isnan(s)) else (s**2 if s is not None else fill_when_no_claims))\n",
    "\n",
    "    # Memory / logical relation distributions\n",
    "    mem_vals = [c.get(\"mem_action\") for c in claims if c.get(\"mem_action\") in VALID_MEM_ACTIONS]\n",
    "    rel_vals = [c.get(\"logical_relation\") for c in claims if c.get(\"logical_relation\") in VALID_LOGREL]\n",
    "    mem_counts = Counter(mem_vals)\n",
    "    rel_counts = Counter(rel_vals)\n",
    "\n",
    "    feats[\"mem_action_count_total\"] = sum(mem_counts.values())\n",
    "    for k in sorted(VALID_MEM_ACTIONS):\n",
    "        # feats[f\"mem_action_count_{k}\"] = mem_counts.get(k, 0)\n",
    "        feats[f\"mem_action_prop_{k}\"] = mem_counts.get(k, 0) / n_claims if n_claims else (fill_when_no_claims if fill_when_no_claims is not None else float(\"nan\"))\n",
    "    feats[\"mem_action_entropy\"] = _entropy_from_counts(mem_counts, default=fill_when_no_claims)\n",
    "\n",
    "    feats[\"logical_relation_count_total\"] = sum(rel_counts.values())\n",
    "    for k in sorted(VALID_LOGREL):\n",
    "        k2 = k.replace(\" \", \"_\")\n",
    "        # feats[f\"logical_relation_count_{k2}\"] = rel_counts.get(k, 0)\n",
    "        feats[f\"logical_relation_prop_{k2}\"] = rel_counts.get(k, 0) / n_claims if n_claims else (fill_when_no_claims if fill_when_no_claims is not None else float(\"nan\"))\n",
    "    feats[\"logical_relation_entropy\"] = _entropy_from_counts(rel_counts, default=fill_when_no_claims)\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "39ccd58c-2563-4425-a73d-051748a1c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_claim_feature_table = []\n",
    "model = 'gpt-5'\n",
    "for task in claim_ratings:\n",
    "    if \"claim_predictions\" not in task:\n",
    "        continue\n",
    "    \n",
    "    tgt_utterances = task[\"target_utterances\"]\n",
    "    conv_id = task[\"conversation_id\"]\n",
    "    segment_id = task[\"segment_id\"]\n",
    "    segments = load_json(f\"../data/processed_segments/openai/{conv_id}_meta_checkpoint.json\")\n",
    "    segment = segments[\"segmentation\"][\"segments\"][segment_id]\n",
    "    start, end = segment[\"intervals\"]\n",
    "    memory_actions = segment.get(\"memory_actions\", []) or []\n",
    "    actions_dict = {}\n",
    "    local_i = 0\n",
    "    for i in range(start, end):\n",
    "        mems = memory_actions[local_i]\n",
    "        mems = {mem[\"id\"]: mem for mem in mems}\n",
    "        actions_dict[i] = mems\n",
    "        local_i += 1\n",
    "    \n",
    "    claims_pred = task[\"claim_predictions\"][model]\n",
    "    for utt in tgt_utterances:\n",
    "        if utt['skipped']:\n",
    "            continue\n",
    "        text = utt[\"utterance_text\"]\n",
    "        utt_index = utt[\"utterance_index\"]\n",
    "        utt_mems = actions_dict[int(utt_index)]\n",
    "        if str(utt_index) in claims_pred:\n",
    "            utt_cl_ratings = claims_pred[str(utt_index)]\n",
    "            for cl in utt_cl_ratings:\n",
    "                _id = cl[\"id\"]\n",
    "                cl['mem_action'] = None\n",
    "                cl['logical_relation'] = None\n",
    "                if _id in utt_mems:\n",
    "                    action = utt_mems[_id]\n",
    "                    cl['mem_action'] = action[\"event\"]\n",
    "                    cl['logical_relation'] = action[\"logical_relation\"]\n",
    "            info = {\n",
    "                \"claims_rating\": utt_cl_ratings,\n",
    "                \"utterance_text\": text\n",
    "            }\n",
    "        else:\n",
    "            info = {\n",
    "                \"claims_rating\": [],\n",
    "                \"utterance_text\": text\n",
    "            }\n",
    "        features = compute_utterance_features(info)\n",
    "        features[\"conv_id\"] = conv_id\n",
    "        features[\"utterance_id\"] = str(utt[\"utterance_id\"])\n",
    "        features[\"corpus\"] = task[\"corpus_id\"]\n",
    "        utt_claim_feature_table.append(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "723a2f54-a781-428b-ad96-f142ae2f7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _spearman(a, b):\n",
    "    r, _ = spearmanr(a, b, nan_policy=\"omit\")\n",
    "    return 0.0 if np.isnan(r) else float(r)\n",
    "\n",
    "def compare_three_features(\n",
    "    df: pd.DataFrame,\n",
    "    target=\"info\",\n",
    "    features=(\"n_claims\", \"n_changes\", \"utterance_token_count\"),\n",
    "    group_col: str | None = \"conv_id\",\n",
    "    n_splits: int = 5,\n",
    "):\n",
    "    # --- prepare data ---\n",
    "    cols = list(features)\n",
    "    X = df[cols].apply(_to_num).copy()\n",
    "    y = _to_num(df[target]).copy()\n",
    "\n",
    "    # common mask (drop rows with all-NaN among features or NaN in y)\n",
    "    m = y.notna() & X.notna().any(axis=1)\n",
    "    X, y = X.loc[m], y.loc[m]\n",
    "\n",
    "    # ---------- (1) Univariate metrics ----------\n",
    "    uni_rows = []\n",
    "    for c in cols:\n",
    "        xv = _to_num(X[c])\n",
    "        mm = np.isfinite(xv) & np.isfinite(y)\n",
    "        if mm.sum() < 5:\n",
    "            uni_rows.append({\"feature\": c, \"spearman\": np.nan, \"mae\": np.nan})\n",
    "            continue\n",
    "        uni_rows.append({\n",
    "            \"feature\": c,\n",
    "            \"spearman\": _spearman(y[mm], xv[mm]),\n",
    "            \"mae\": mean_absolute_error(y[mm], xv[mm]),\n",
    "        })\n",
    "    univariate = pd.DataFrame(uni_rows).set_index(\"feature\")\n",
    "\n",
    "    # ---------- (2) Partial / standardized linear coefs ----------\n",
    "    # Impute + standardize X; standardize y (for comparable beta magnitudes)\n",
    "    imp = SimpleImputer(strategy=\"median\").fit(X)\n",
    "    X_imp = pd.DataFrame(imp.transform(X), index=X.index, columns=cols)\n",
    "    scX = StandardScaler().fit(X_imp)\n",
    "    Xz = pd.DataFrame(scX.transform(X_imp), index=X.index, columns=cols)\n",
    "\n",
    "    y_mean, y_std = y.mean(), y.std(ddof=0)\n",
    "    yz = (y - y_mean) / (y_std if y_std > 0 else 1.0)\n",
    "\n",
    "    # OLS (standardized) -> coefficients are partial effects in SD-units\n",
    "    Xmat = np.c_[np.ones(len(Xz)), Xz.values]  # intercept + z-features\n",
    "    beta = np.linalg.lstsq(Xmat, yz.values, rcond=None)[0]  # [b0, b1, b2, b3]\n",
    "    std_coefs = pd.Series(beta[1:], index=cols, name=\"std_coef\")\n",
    "\n",
    "    # Partial Spearman for each feature: corr(resid_y, resid_xj)\n",
    "    partial_rows = []\n",
    "    for j, c in enumerate(cols):\n",
    "        others = [k for k in cols if k != c]\n",
    "        # regress y on others\n",
    "        Xo = np.c_[np.ones(len(Xz)), Xz[others].values]\n",
    "        by = np.linalg.lstsq(Xo, yz.values, rcond=None)[0]\n",
    "        resid_y = yz.values - Xo @ by\n",
    "        # regress xj on others\n",
    "        Xj = Xz[c].values\n",
    "        bj = np.linalg.lstsq(Xo, Xj, rcond=None)[0]\n",
    "        resid_x = Xj - Xo @ bj\n",
    "        partial_rows.append({\"feature\": c, \"partial_spearman\": _spearman(resid_y, resid_x)})\n",
    "    partial = pd.DataFrame(partial_rows).set_index(\"feature\")\n",
    "\n",
    "    # ---------- (3) Permutation importance (grouped CV; drop in Spearman & rise in MAE) ----------\n",
    "    if group_col is not None and group_col in df.columns:\n",
    "        groups_all = df.loc[m, group_col].astype(str).values\n",
    "        cv = GroupKFold(n_splits=n_splits)\n",
    "    else:\n",
    "        groups_all = None\n",
    "        cv = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    perm_drop_spear = {c: [] for c in cols}\n",
    "    perm_rise_mae = {c: [] for c in cols}\n",
    "\n",
    "    for tr, te in cv.split(X_imp, y, groups_all):\n",
    "        Xtr, Xte = X_imp.iloc[tr], X_imp.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        # pipeline: impute (already), scale, ridge (stable)\n",
    "        scX_fold = StandardScaler().fit(Xtr)\n",
    "        Xtrz = scX_fold.transform(Xtr)\n",
    "        Xtez = scX_fold.transform(Xte)\n",
    "\n",
    "        model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "        model.fit(Xtrz, ytr)\n",
    "        base_pred = model.predict(Xtez)\n",
    "        base_sp = _spearman(yte.values, base_pred)\n",
    "        base_mae = mean_absolute_error(yte, base_pred)\n",
    "\n",
    "        # permute each feature in test fold\n",
    "        rng = np.random.default_rng(RANDOM_STATE)\n",
    "        for j, c in enumerate(cols):\n",
    "            Xperm = Xtez.copy()\n",
    "            Xperm[:, j] = rng.permutation(Xperm[:, j])\n",
    "            p_pred = model.predict(Xperm)\n",
    "            perm_drop_spear[c].append(base_sp - _spearman(yte.values, p_pred))\n",
    "            perm_rise_mae[c].append(mean_absolute_error(yte, p_pred) - base_mae)\n",
    "\n",
    "    perm_df = pd.DataFrame({\n",
    "        \"perm_drop_spearman\": {c: float(np.mean(perm_drop_spear[c])) for c in cols},\n",
    "        \"perm_rise_mae\":      {c: float(np.mean(perm_rise_mae[c])) for c in cols},\n",
    "    })\n",
    "\n",
    "    # ---------- (4) Dominance / Shapley R^2 (exact for 3 vars) ----------\n",
    "    # average marginal R^2 gain over all orderings (Shapley value wrt R^2)\n",
    "    def r2_of(feat_list):\n",
    "        if len(feat_list) == 0:\n",
    "            return 0.0\n",
    "        Xi = X_imp[feat_list].values\n",
    "        Xi = np.c_[np.ones(len(Xi)), Xi]\n",
    "        bh = np.linalg.lstsq(Xi, y.values, rcond=None)[0]\n",
    "        yhat = Xi @ bh\n",
    "        return float(r2_score(y.values, yhat))\n",
    "\n",
    "    from itertools import permutations\n",
    "    shap_r2 = {c: 0.0 for c in cols}\n",
    "    perms = list(permutations(cols, 3))\n",
    "    for order in perms:\n",
    "        seen = []\n",
    "        prev = 0.0\n",
    "        for f in order:\n",
    "            seen = seen + [f]\n",
    "            cur = r2_of(seen)\n",
    "            shap_r2[f] += (cur - prev) / len(perms)\n",
    "            prev = cur\n",
    "    shap_series = pd.Series(shap_r2, name=\"shapley_R2\")\n",
    "\n",
    "    # ---------- assemble report ----------\n",
    "    report = pd.concat([univariate, std_coefs.to_frame(), partial, perm_df, shap_series.to_frame()], axis=1)\n",
    "    # nicer ordering\n",
    "    report = report.loc[cols, [\"spearman\", \"mae\", \"std_coef\", \"partial_spearman\",\n",
    "                               \"perm_drop_spearman\", \"perm_rise_mae\", \"shapley_R2\"]]\n",
    "    return report.sort_values([\"shapley_R2\", \"perm_drop_spearman\", \"std_coef\"], ascending=False)\n",
    "\n",
    "# ---- run it ----\n",
    "# features = [\"n_claims\", \"n_changes\", \"utterance_token_count\"]\n",
    "# report = compare_three_features(cl_soft, target=\"info\", features=features, group_col=\"conv_id\")\n",
    "# print(report.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "881306ff-237f-489d-aa8e-8cd5acb72834",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df = pd.DataFrame(utt_claim_feature_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "d434a5ae-e71f-43d9-8a30-f3c3ee4eb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_soft = pd.merge(cl_df, soft_df[[\"info\", \"novo\", \"relv\", \"imsc\", \"conv_id\", \"utterance_id\"]], how=\"left\", on=[\"conv_id\", \"utterance_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "24fdb03e-0045-4986-8bd6-e820040920b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['has_claims', 'n_claims', 'n_changes', 'p_changes',\n",
       "       'utterance_token_count', 'claim_density', 'change_density', 'mean_info',\n",
       "       'max_info', 'min_info', 'median_info', 'harmonic_mean_info',\n",
       "       'top2_info', 'top_quartile_mean_info', 'top_half_mean_info',\n",
       "       'mean_novo', 'max_novo', 'min_novo', 'median_novo',\n",
       "       'harmonic_mean_novo', 'top2_novo', 'top_quartile_mean_novo',\n",
       "       'top_half_mean_novo', 'mean_relv', 'max_relv', 'min_relv',\n",
       "       'median_relv', 'harmonic_mean_relv', 'top2_relv',\n",
       "       'top_quartile_mean_relv', 'top_half_mean_relv', 'mean_imsc', 'max_imsc',\n",
       "       'min_imsc', 'median_imsc', 'harmonic_mean_imsc', 'top2_imsc',\n",
       "       'top_quartile_mean_imsc', 'top_half_mean_imsc',\n",
       "       'mem_action_count_total', 'mem_action_prop_ADD', 'mem_action_prop_NONE',\n",
       "       'mem_action_prop_UPDATE', 'mem_action_entropy',\n",
       "       'logical_relation_count_total',\n",
       "       'logical_relation_prop_backward_entailment',\n",
       "       'logical_relation_prop_contradiction',\n",
       "       'logical_relation_prop_equivalent',\n",
       "       'logical_relation_prop_forward_entailment',\n",
       "       'logical_relation_prop_neutral', 'logical_relation_entropy', 'conv_id',\n",
       "       'utterance_id', 'corpus', 'info', 'novo', 'relv', 'imsc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_soft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d5c475ed-4f24-4a6d-818c-c0f08a2c7519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating aspect: info\n",
      "aggregation_method: mean, mae: 0.665273059400253, spearman: 0.36446038031141015\n",
      "aggregation_method: max, mae: 0.7114058355437667, spearman: 0.4624200574302934\n",
      "aggregation_method: min, mae: 1.2302387267904509, spearman: 0.05670590333789154\n",
      "aggregation_method: median, mae: 0.6575596816976127, spearman: 0.386545643136077\n",
      "aggregation_method: harmonic, mae: 0.7867185410106841, spearman: 0.3125557621138622\n",
      "aggregation_method: top2, mae: 0.5856763925729443, spearman: 0.5369037676295215\n",
      "aggregation_method: top, mae: 0.6707780725022104, spearman: 0.4201286905534938\n",
      "aggregation_method: top, mae: 0.6505784445439619, spearman: 0.405148416363418\n",
      "evaluating aspect: novo\n",
      "aggregation_method: mean, mae: 0.6102529180362778, spearman: 0.3931714787526952\n",
      "aggregation_method: max, mae: 0.7661803713527852, spearman: 0.4145203603519862\n",
      "aggregation_method: min, mae: 1.2103006189213086, spearman: 0.14177407718139978\n",
      "aggregation_method: median, mae: 0.6015473032714412, spearman: 0.41114249864503505\n",
      "aggregation_method: harmonic, mae: 0.7447665840503445, spearman: 0.34807822580471215\n",
      "aggregation_method: top2, mae: 0.6265694076038903, spearman: 0.5029313458871149\n",
      "aggregation_method: top, mae: 0.7141909814323607, spearman: 0.38308734947416945\n",
      "aggregation_method: top, mae: 0.6412625688487757, spearman: 0.3984732874600942\n",
      "evaluating aspect: relv\n",
      "aggregation_method: mean, mae: 0.4145671729148091, spearman: 0.16441592396577792\n",
      "aggregation_method: max, mae: 0.36357206012378424, spearman: 0.26303994194895874\n",
      "aggregation_method: min, mae: 0.8002652519893899, spearman: 0.09902531172401652\n",
      "aggregation_method: median, mae: 0.3978779840848806, spearman: 0.24445889780151173\n",
      "aggregation_method: harmonic, mae: 0.4665596336872505, spearman: 0.15860656855101393\n",
      "aggregation_method: top2, mae: 0.33837312113174184, spearman: 0.34191315942391676\n",
      "aggregation_method: top, mae: 0.36445623342175065, spearman: 0.23369109468830915\n",
      "aggregation_method: top, mae: 0.36034798534798534, spearman: 0.20977405485256923\n",
      "evaluating aspect: imsc\n",
      "aggregation_method: mean, mae: 0.4735973517043106, spearman: 0.5457843372802331\n",
      "aggregation_method: max, mae: 0.5863837312113174, spearman: 0.530807362712137\n",
      "aggregation_method: min, mae: 0.7536693191865605, spearman: 0.261796792058957\n",
      "aggregation_method: median, mae: 0.5069849690539345, spearman: 0.5020130438911018\n",
      "aggregation_method: harmonic, mae: 0.5086448289265032, spearman: 0.5117011448442433\n",
      "aggregation_method: top2, mae: 0.5019451812555261, spearman: 0.5696471761692777\n",
      "aggregation_method: top, mae: 0.5279398762157382, spearman: 0.5385968273932011\n",
      "aggregation_method: top, mae: 0.48737689513551585, spearman: 0.5481673433046078\n"
     ]
    }
   ],
   "source": [
    "cols = [\"info\", \"novo\", \"relv\", \"imsc\"]\n",
    "df = cl_soft[cl_soft.n_claims >=2]\n",
    "for aspect in cols:\n",
    "    print(f\"evaluating aspect: {aspect}\")\n",
    "    related_cols = [col for col in cl_soft.columns if aspect in col and col != aspect]\n",
    "    gt = df[aspect]\n",
    "    for col in related_cols:\n",
    "        agg_method = col.split(\"_\")[0]\n",
    "        pred = df[col]\n",
    "        print(f\"aggregation_method: {agg_method}, mae: {mean_absolute_error(gt, pred)}, spearman: {float(pd.Series(gt).corr(pd.Series(pred), method='spearman'))}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "18cc88bb-a1cd-47cd-a6b3-3ea2097d1298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>predictor</th>\n",
       "      <th>agg_label</th>\n",
       "      <th>n</th>\n",
       "      <th>spearman_all</th>\n",
       "      <th>kendall_all</th>\n",
       "      <th>mae_all</th>\n",
       "      <th>rmse_all</th>\n",
       "      <th>nmae_all</th>\n",
       "      <th>spearman_groupcv</th>\n",
       "      <th>mae_groupcv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>info</td>\n",
       "      <td>top_quartile_mean_info</td>\n",
       "      <td>top_quartile_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.439997</td>\n",
       "      <td>0.337425</td>\n",
       "      <td>0.638811</td>\n",
       "      <td>0.862951</td>\n",
       "      <td>0.212937</td>\n",
       "      <td>0.438520</td>\n",
       "      <td>0.638721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>info</td>\n",
       "      <td>top_half_mean_info</td>\n",
       "      <td>top_half_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.440851</td>\n",
       "      <td>0.328638</td>\n",
       "      <td>0.625427</td>\n",
       "      <td>0.826386</td>\n",
       "      <td>0.208476</td>\n",
       "      <td>0.436069</td>\n",
       "      <td>0.625388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>info</td>\n",
       "      <td>max_info</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.432830</td>\n",
       "      <td>0.338904</td>\n",
       "      <td>0.665729</td>\n",
       "      <td>0.898096</td>\n",
       "      <td>0.221910</td>\n",
       "      <td>0.433356</td>\n",
       "      <td>0.665584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>info</td>\n",
       "      <td>top2_info</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.426419</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.582425</td>\n",
       "      <td>0.789031</td>\n",
       "      <td>0.194142</td>\n",
       "      <td>0.429122</td>\n",
       "      <td>0.582327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>info</td>\n",
       "      <td>median_info</td>\n",
       "      <td>median</td>\n",
       "      <td>569</td>\n",
       "      <td>0.418223</td>\n",
       "      <td>0.323615</td>\n",
       "      <td>0.630053</td>\n",
       "      <td>0.835846</td>\n",
       "      <td>0.210018</td>\n",
       "      <td>0.418556</td>\n",
       "      <td>0.630060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info</td>\n",
       "      <td>mean_info</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.405317</td>\n",
       "      <td>0.293602</td>\n",
       "      <td>0.635163</td>\n",
       "      <td>0.827260</td>\n",
       "      <td>0.211721</td>\n",
       "      <td>0.406064</td>\n",
       "      <td>0.635269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>info</td>\n",
       "      <td>max_novo</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.391090</td>\n",
       "      <td>0.302735</td>\n",
       "      <td>0.881664</td>\n",
       "      <td>1.135110</td>\n",
       "      <td>0.293888</td>\n",
       "      <td>0.395999</td>\n",
       "      <td>0.881590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>info</td>\n",
       "      <td>top2_novo</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.391630</td>\n",
       "      <td>0.291340</td>\n",
       "      <td>0.780609</td>\n",
       "      <td>1.030375</td>\n",
       "      <td>0.260203</td>\n",
       "      <td>0.395923</td>\n",
       "      <td>0.780528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>info</td>\n",
       "      <td>max_relv</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.385572</td>\n",
       "      <td>0.310699</td>\n",
       "      <td>1.296661</td>\n",
       "      <td>1.584604</td>\n",
       "      <td>0.432220</td>\n",
       "      <td>0.389467</td>\n",
       "      <td>1.296622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>info</td>\n",
       "      <td>mem_action_count_total</td>\n",
       "      <td>mem</td>\n",
       "      <td>569</td>\n",
       "      <td>0.374611</td>\n",
       "      <td>0.271326</td>\n",
       "      <td>2.136028</td>\n",
       "      <td>3.417264</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.387547</td>\n",
       "      <td>2.135693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>info</td>\n",
       "      <td>harmonic_mean_info</td>\n",
       "      <td>harmonic_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.366231</td>\n",
       "      <td>0.260465</td>\n",
       "      <td>0.715629</td>\n",
       "      <td>0.917316</td>\n",
       "      <td>0.238543</td>\n",
       "      <td>0.365458</td>\n",
       "      <td>0.715829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>info</td>\n",
       "      <td>top2_relv</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.345728</td>\n",
       "      <td>0.273579</td>\n",
       "      <td>1.255946</td>\n",
       "      <td>1.545422</td>\n",
       "      <td>0.418649</td>\n",
       "      <td>0.359603</td>\n",
       "      <td>1.255886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>info</td>\n",
       "      <td>max_imsc</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.340342</td>\n",
       "      <td>0.260511</td>\n",
       "      <td>0.794610</td>\n",
       "      <td>1.060951</td>\n",
       "      <td>0.264870</td>\n",
       "      <td>0.358783</td>\n",
       "      <td>0.794064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>info</td>\n",
       "      <td>top2_imsc</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.333799</td>\n",
       "      <td>0.247040</td>\n",
       "      <td>0.746690</td>\n",
       "      <td>0.996046</td>\n",
       "      <td>0.248897</td>\n",
       "      <td>0.353435</td>\n",
       "      <td>0.746184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>info</td>\n",
       "      <td>mean_imsc</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.333588</td>\n",
       "      <td>0.235695</td>\n",
       "      <td>0.760437</td>\n",
       "      <td>0.984892</td>\n",
       "      <td>0.253479</td>\n",
       "      <td>0.351750</td>\n",
       "      <td>0.760089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>info</td>\n",
       "      <td>logical_relation_count_total</td>\n",
       "      <td>logical</td>\n",
       "      <td>569</td>\n",
       "      <td>0.337163</td>\n",
       "      <td>0.246442</td>\n",
       "      <td>1.446983</td>\n",
       "      <td>2.215410</td>\n",
       "      <td>0.482328</td>\n",
       "      <td>0.338760</td>\n",
       "      <td>1.446685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>info</td>\n",
       "      <td>mean_novo</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.320631</td>\n",
       "      <td>0.228391</td>\n",
       "      <td>0.764098</td>\n",
       "      <td>0.991061</td>\n",
       "      <td>0.254699</td>\n",
       "      <td>0.327296</td>\n",
       "      <td>0.764156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>info</td>\n",
       "      <td>min_imsc</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.267916</td>\n",
       "      <td>0.204502</td>\n",
       "      <td>0.880375</td>\n",
       "      <td>1.133871</td>\n",
       "      <td>0.293458</td>\n",
       "      <td>0.280396</td>\n",
       "      <td>0.880128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>info</td>\n",
       "      <td>mean_relv</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.253570</td>\n",
       "      <td>0.182724</td>\n",
       "      <td>1.158516</td>\n",
       "      <td>1.468968</td>\n",
       "      <td>0.386172</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>1.158451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>info</td>\n",
       "      <td>min_relv</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.232598</td>\n",
       "      <td>0.175429</td>\n",
       "      <td>1.156532</td>\n",
       "      <td>1.455939</td>\n",
       "      <td>0.385511</td>\n",
       "      <td>0.249632</td>\n",
       "      <td>1.156445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aspect                     predictor          agg_label    n  spearman_all  \\\n",
       "6    info        top_quartile_mean_info  top_quartile_mean  569      0.439997   \n",
       "7    info            top_half_mean_info      top_half_mean  569      0.440851   \n",
       "1    info                      max_info                max  569      0.432830   \n",
       "5    info                     top2_info               top2  569      0.426419   \n",
       "3    info                   median_info             median  569      0.418223   \n",
       "0    info                     mean_info               mean  569      0.405317   \n",
       "9    info                      max_novo                max  569      0.391090   \n",
       "11   info                     top2_novo               top2  569      0.391630   \n",
       "13   info                      max_relv                max  569      0.385572   \n",
       "20   info        mem_action_count_total                mem  569      0.374611   \n",
       "4    info            harmonic_mean_info      harmonic_mean  569      0.366231   \n",
       "15   info                     top2_relv               top2  569      0.345728   \n",
       "17   info                      max_imsc                max  569      0.340342   \n",
       "19   info                     top2_imsc               top2  569      0.333799   \n",
       "16   info                     mean_imsc               mean  569      0.333588   \n",
       "21   info  logical_relation_count_total            logical  569      0.337163   \n",
       "8    info                     mean_novo               mean  569      0.320631   \n",
       "18   info                      min_imsc                min  569      0.267916   \n",
       "12   info                     mean_relv               mean  569      0.253570   \n",
       "14   info                      min_relv                min  569      0.232598   \n",
       "\n",
       "    kendall_all   mae_all  rmse_all  nmae_all  spearman_groupcv  mae_groupcv  \n",
       "6      0.337425  0.638811  0.862951  0.212937          0.438520     0.638721  \n",
       "7      0.328638  0.625427  0.826386  0.208476          0.436069     0.625388  \n",
       "1      0.338904  0.665729  0.898096  0.221910          0.433356     0.665584  \n",
       "5      0.323100  0.582425  0.789031  0.194142          0.429122     0.582327  \n",
       "3      0.323615  0.630053  0.835846  0.210018          0.418556     0.630060  \n",
       "0      0.293602  0.635163  0.827260  0.211721          0.406064     0.635269  \n",
       "9      0.302735  0.881664  1.135110  0.293888          0.395999     0.881590  \n",
       "11     0.291340  0.780609  1.030375  0.260203          0.395923     0.780528  \n",
       "13     0.310699  1.296661  1.584604  0.432220          0.389467     1.296622  \n",
       "20     0.271326  2.136028  3.417264  0.712009          0.387547     2.135693  \n",
       "4      0.260465  0.715629  0.917316  0.238543          0.365458     0.715829  \n",
       "15     0.273579  1.255946  1.545422  0.418649          0.359603     1.255886  \n",
       "17     0.260511  0.794610  1.060951  0.264870          0.358783     0.794064  \n",
       "19     0.247040  0.746690  0.996046  0.248897          0.353435     0.746184  \n",
       "16     0.235695  0.760437  0.984892  0.253479          0.351750     0.760089  \n",
       "21     0.246442  1.446983  2.215410  0.482328          0.338760     1.446685  \n",
       "8      0.228391  0.764098  0.991061  0.254699          0.327296     0.764156  \n",
       "18     0.204502  0.880375  1.133871  0.293458          0.280396     0.880128  \n",
       "12     0.182724  1.158516  1.468968  0.386172          0.275967     1.158451  \n",
       "14     0.175429  1.156532  1.455939  0.385511          0.249632     1.156445  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>predictor</th>\n",
       "      <th>agg_label</th>\n",
       "      <th>n</th>\n",
       "      <th>spearman_all</th>\n",
       "      <th>kendall_all</th>\n",
       "      <th>mae_all</th>\n",
       "      <th>rmse_all</th>\n",
       "      <th>nmae_all</th>\n",
       "      <th>spearman_groupcv</th>\n",
       "      <th>mae_groupcv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>novo</td>\n",
       "      <td>top_half_mean_novo</td>\n",
       "      <td>top_half_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.418594</td>\n",
       "      <td>0.311189</td>\n",
       "      <td>0.714100</td>\n",
       "      <td>0.942087</td>\n",
       "      <td>0.238033</td>\n",
       "      <td>0.421299</td>\n",
       "      <td>0.714568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>novo</td>\n",
       "      <td>median_novo</td>\n",
       "      <td>median</td>\n",
       "      <td>569</td>\n",
       "      <td>0.411559</td>\n",
       "      <td>0.313473</td>\n",
       "      <td>0.687786</td>\n",
       "      <td>0.917269</td>\n",
       "      <td>0.229262</td>\n",
       "      <td>0.415434</td>\n",
       "      <td>0.687966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>novo</td>\n",
       "      <td>top_quartile_mean_novo</td>\n",
       "      <td>top_quartile_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.405384</td>\n",
       "      <td>0.306965</td>\n",
       "      <td>0.762419</td>\n",
       "      <td>0.991132</td>\n",
       "      <td>0.254140</td>\n",
       "      <td>0.405079</td>\n",
       "      <td>0.762938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>novo</td>\n",
       "      <td>top2_novo</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.398220</td>\n",
       "      <td>0.295842</td>\n",
       "      <td>0.704364</td>\n",
       "      <td>0.929198</td>\n",
       "      <td>0.234788</td>\n",
       "      <td>0.398106</td>\n",
       "      <td>0.704852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>novo</td>\n",
       "      <td>max_info</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.407082</td>\n",
       "      <td>0.313847</td>\n",
       "      <td>0.617077</td>\n",
       "      <td>0.838667</td>\n",
       "      <td>0.205692</td>\n",
       "      <td>0.392910</td>\n",
       "      <td>0.617398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>novo</td>\n",
       "      <td>max_novo</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.393598</td>\n",
       "      <td>0.304251</td>\n",
       "      <td>0.796866</td>\n",
       "      <td>1.024749</td>\n",
       "      <td>0.265622</td>\n",
       "      <td>0.389409</td>\n",
       "      <td>0.797413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>novo</td>\n",
       "      <td>mean_info</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.392067</td>\n",
       "      <td>0.280284</td>\n",
       "      <td>0.642390</td>\n",
       "      <td>0.818977</td>\n",
       "      <td>0.214130</td>\n",
       "      <td>0.381414</td>\n",
       "      <td>0.642189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novo</td>\n",
       "      <td>mean_novo</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.377111</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.693554</td>\n",
       "      <td>0.901665</td>\n",
       "      <td>0.231185</td>\n",
       "      <td>0.377818</td>\n",
       "      <td>0.693682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>novo</td>\n",
       "      <td>top2_info</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.384126</td>\n",
       "      <td>0.284837</td>\n",
       "      <td>0.569156</td>\n",
       "      <td>0.762223</td>\n",
       "      <td>0.189719</td>\n",
       "      <td>0.371767</td>\n",
       "      <td>0.569343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>novo</td>\n",
       "      <td>max_relv</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.379403</td>\n",
       "      <td>0.301377</td>\n",
       "      <td>1.161599</td>\n",
       "      <td>1.417924</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.364330</td>\n",
       "      <td>1.162564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>novo</td>\n",
       "      <td>harmonic_mean_novo</td>\n",
       "      <td>harmonic_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.342070</td>\n",
       "      <td>0.242209</td>\n",
       "      <td>0.782678</td>\n",
       "      <td>0.985994</td>\n",
       "      <td>0.260893</td>\n",
       "      <td>0.340130</td>\n",
       "      <td>0.782689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>novo</td>\n",
       "      <td>max_imsc</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.334373</td>\n",
       "      <td>0.251235</td>\n",
       "      <td>0.731547</td>\n",
       "      <td>0.963523</td>\n",
       "      <td>0.243849</td>\n",
       "      <td>0.318162</td>\n",
       "      <td>0.731159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>novo</td>\n",
       "      <td>top2_relv</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.325746</td>\n",
       "      <td>0.253882</td>\n",
       "      <td>1.120885</td>\n",
       "      <td>1.382879</td>\n",
       "      <td>0.373628</td>\n",
       "      <td>0.314837</td>\n",
       "      <td>1.121828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>novo</td>\n",
       "      <td>mem_action_count_total</td>\n",
       "      <td>mem</td>\n",
       "      <td>569</td>\n",
       "      <td>0.319582</td>\n",
       "      <td>0.218689</td>\n",
       "      <td>2.189953</td>\n",
       "      <td>3.470238</td>\n",
       "      <td>0.729984</td>\n",
       "      <td>0.311444</td>\n",
       "      <td>2.190092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>novo</td>\n",
       "      <td>mean_imsc</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.306113</td>\n",
       "      <td>0.214083</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.925242</td>\n",
       "      <td>0.243957</td>\n",
       "      <td>0.302166</td>\n",
       "      <td>0.731252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>novo</td>\n",
       "      <td>logical_relation_count_total</td>\n",
       "      <td>logical</td>\n",
       "      <td>569</td>\n",
       "      <td>0.297839</td>\n",
       "      <td>0.209736</td>\n",
       "      <td>1.554218</td>\n",
       "      <td>2.296790</td>\n",
       "      <td>0.518073</td>\n",
       "      <td>0.297944</td>\n",
       "      <td>1.554043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>novo</td>\n",
       "      <td>top2_imsc</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.302424</td>\n",
       "      <td>0.218075</td>\n",
       "      <td>0.709461</td>\n",
       "      <td>0.925408</td>\n",
       "      <td>0.236487</td>\n",
       "      <td>0.288929</td>\n",
       "      <td>0.709030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>novo</td>\n",
       "      <td>min_imsc</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.249488</td>\n",
       "      <td>0.190577</td>\n",
       "      <td>0.890129</td>\n",
       "      <td>1.111093</td>\n",
       "      <td>0.296710</td>\n",
       "      <td>0.244328</td>\n",
       "      <td>0.889356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>novo</td>\n",
       "      <td>min_info</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.233217</td>\n",
       "      <td>0.181022</td>\n",
       "      <td>1.061013</td>\n",
       "      <td>1.325152</td>\n",
       "      <td>0.353671</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>1.060702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>novo</td>\n",
       "      <td>min_novo</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.210518</td>\n",
       "      <td>0.160873</td>\n",
       "      <td>1.091125</td>\n",
       "      <td>1.354837</td>\n",
       "      <td>0.363708</td>\n",
       "      <td>0.214281</td>\n",
       "      <td>1.090893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aspect                     predictor          agg_label    n  spearman_all  \\\n",
       "11   novo            top_half_mean_novo      top_half_mean  569      0.418594   \n",
       "7    novo                   median_novo             median  569      0.411559   \n",
       "10   novo        top_quartile_mean_novo  top_quartile_mean  569      0.405384   \n",
       "9    novo                     top2_novo               top2  569      0.398220   \n",
       "1    novo                      max_info                max  569      0.407082   \n",
       "5    novo                      max_novo                max  569      0.393598   \n",
       "0    novo                     mean_info               mean  569      0.392067   \n",
       "4    novo                     mean_novo               mean  569      0.377111   \n",
       "3    novo                     top2_info               top2  569      0.384126   \n",
       "13   novo                      max_relv                max  569      0.379403   \n",
       "8    novo            harmonic_mean_novo      harmonic_mean  569      0.342070   \n",
       "17   novo                      max_imsc                max  569      0.334373   \n",
       "15   novo                     top2_relv               top2  569      0.325746   \n",
       "20   novo        mem_action_count_total                mem  569      0.319582   \n",
       "16   novo                     mean_imsc               mean  569      0.306113   \n",
       "21   novo  logical_relation_count_total            logical  569      0.297839   \n",
       "19   novo                     top2_imsc               top2  569      0.302424   \n",
       "18   novo                      min_imsc                min  569      0.249488   \n",
       "2    novo                      min_info                min  569      0.233217   \n",
       "6    novo                      min_novo                min  569      0.210518   \n",
       "\n",
       "    kendall_all   mae_all  rmse_all  nmae_all  spearman_groupcv  mae_groupcv  \n",
       "11     0.311189  0.714100  0.942087  0.238033          0.421299     0.714568  \n",
       "7      0.313473  0.687786  0.917269  0.229262          0.415434     0.687966  \n",
       "10     0.306965  0.762419  0.991132  0.254140          0.405079     0.762938  \n",
       "9      0.295842  0.704364  0.929198  0.234788          0.398106     0.704852  \n",
       "1      0.313847  0.617077  0.838667  0.205692          0.392910     0.617398  \n",
       "5      0.304251  0.796866  1.024749  0.265622          0.389409     0.797413  \n",
       "0      0.280284  0.642390  0.818977  0.214130          0.381414     0.642189  \n",
       "4      0.271100  0.693554  0.901665  0.231185          0.377818     0.693682  \n",
       "3      0.284837  0.569156  0.762223  0.189719          0.371767     0.569343  \n",
       "13     0.301377  1.161599  1.417924  0.387200          0.364330     1.162564  \n",
       "8      0.242209  0.782678  0.985994  0.260893          0.340130     0.782689  \n",
       "17     0.251235  0.731547  0.963523  0.243849          0.318162     0.731159  \n",
       "15     0.253882  1.120885  1.382879  0.373628          0.314837     1.121828  \n",
       "20     0.218689  2.189953  3.470238  0.729984          0.311444     2.190092  \n",
       "16     0.214083  0.731870  0.925242  0.243957          0.302166     0.731252  \n",
       "21     0.209736  1.554218  2.296790  0.518073          0.297944     1.554043  \n",
       "19     0.218075  0.709461  0.925408  0.236487          0.288929     0.709030  \n",
       "18     0.190577  0.890129  1.111093  0.296710          0.244328     0.889356  \n",
       "2      0.181022  1.061013  1.325152  0.353671          0.221821     1.060702  \n",
       "6      0.160873  1.091125  1.354837  0.363708          0.214281     1.090893  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>predictor</th>\n",
       "      <th>agg_label</th>\n",
       "      <th>n</th>\n",
       "      <th>spearman_all</th>\n",
       "      <th>kendall_all</th>\n",
       "      <th>mae_all</th>\n",
       "      <th>rmse_all</th>\n",
       "      <th>nmae_all</th>\n",
       "      <th>spearman_groupcv</th>\n",
       "      <th>mae_groupcv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relv</td>\n",
       "      <td>max_relv</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.456717</td>\n",
       "      <td>0.364255</td>\n",
       "      <td>0.521470</td>\n",
       "      <td>0.824088</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.461389</td>\n",
       "      <td>0.522336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>relv</td>\n",
       "      <td>top_quartile_mean_relv</td>\n",
       "      <td>top_quartile_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.440802</td>\n",
       "      <td>0.349652</td>\n",
       "      <td>0.522056</td>\n",
       "      <td>0.823709</td>\n",
       "      <td>0.174019</td>\n",
       "      <td>0.449767</td>\n",
       "      <td>0.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>relv</td>\n",
       "      <td>top2_relv</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.428576</td>\n",
       "      <td>0.336245</td>\n",
       "      <td>0.504774</td>\n",
       "      <td>0.803190</td>\n",
       "      <td>0.168258</td>\n",
       "      <td>0.437997</td>\n",
       "      <td>0.505624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>relv</td>\n",
       "      <td>top_half_mean_relv</td>\n",
       "      <td>top_half_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.422771</td>\n",
       "      <td>0.331879</td>\n",
       "      <td>0.519334</td>\n",
       "      <td>0.817268</td>\n",
       "      <td>0.173111</td>\n",
       "      <td>0.437560</td>\n",
       "      <td>0.520164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relv</td>\n",
       "      <td>median_relv</td>\n",
       "      <td>median</td>\n",
       "      <td>569</td>\n",
       "      <td>0.389855</td>\n",
       "      <td>0.302913</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>0.828502</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.404683</td>\n",
       "      <td>0.544811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>relv</td>\n",
       "      <td>harmonic_mean_relv</td>\n",
       "      <td>harmonic_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.379047</td>\n",
       "      <td>0.278519</td>\n",
       "      <td>0.589706</td>\n",
       "      <td>0.838008</td>\n",
       "      <td>0.196569</td>\n",
       "      <td>0.399216</td>\n",
       "      <td>0.590025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relv</td>\n",
       "      <td>mean_relv</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.378044</td>\n",
       "      <td>0.277887</td>\n",
       "      <td>0.555258</td>\n",
       "      <td>0.806607</td>\n",
       "      <td>0.185086</td>\n",
       "      <td>0.396807</td>\n",
       "      <td>0.555708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relv</td>\n",
       "      <td>min_relv</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.363697</td>\n",
       "      <td>0.281306</td>\n",
       "      <td>0.810808</td>\n",
       "      <td>1.075384</td>\n",
       "      <td>0.270269</td>\n",
       "      <td>0.383368</td>\n",
       "      <td>0.810757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>relv</td>\n",
       "      <td>min_imsc</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.313578</td>\n",
       "      <td>0.244870</td>\n",
       "      <td>1.243673</td>\n",
       "      <td>1.470957</td>\n",
       "      <td>0.414558</td>\n",
       "      <td>0.316477</td>\n",
       "      <td>1.242982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>relv</td>\n",
       "      <td>mean_imsc</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.312320</td>\n",
       "      <td>0.224152</td>\n",
       "      <td>0.965825</td>\n",
       "      <td>1.163107</td>\n",
       "      <td>0.321942</td>\n",
       "      <td>0.311868</td>\n",
       "      <td>0.965180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>relv</td>\n",
       "      <td>max_imsc</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.301726</td>\n",
       "      <td>0.228870</td>\n",
       "      <td>0.738518</td>\n",
       "      <td>0.993317</td>\n",
       "      <td>0.246173</td>\n",
       "      <td>0.292713</td>\n",
       "      <td>0.738580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relv</td>\n",
       "      <td>max_info</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.294644</td>\n",
       "      <td>0.220194</td>\n",
       "      <td>0.749883</td>\n",
       "      <td>1.019798</td>\n",
       "      <td>0.249961</td>\n",
       "      <td>0.282884</td>\n",
       "      <td>0.749615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>relv</td>\n",
       "      <td>top2_imsc</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.282814</td>\n",
       "      <td>0.205168</td>\n",
       "      <td>0.797979</td>\n",
       "      <td>1.030371</td>\n",
       "      <td>0.265993</td>\n",
       "      <td>0.272869</td>\n",
       "      <td>0.797765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relv</td>\n",
       "      <td>mean_info</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.190356</td>\n",
       "      <td>1.116704</td>\n",
       "      <td>1.307825</td>\n",
       "      <td>0.372235</td>\n",
       "      <td>0.260046</td>\n",
       "      <td>1.115981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relv</td>\n",
       "      <td>top2_info</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.268349</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.842912</td>\n",
       "      <td>1.082280</td>\n",
       "      <td>0.280971</td>\n",
       "      <td>0.255046</td>\n",
       "      <td>0.842389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relv</td>\n",
       "      <td>max_novo</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.223445</td>\n",
       "      <td>0.167882</td>\n",
       "      <td>0.729965</td>\n",
       "      <td>1.044460</td>\n",
       "      <td>0.243322</td>\n",
       "      <td>0.211821</td>\n",
       "      <td>0.730233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>relv</td>\n",
       "      <td>mem_action_count_total</td>\n",
       "      <td>mem</td>\n",
       "      <td>569</td>\n",
       "      <td>0.208637</td>\n",
       "      <td>0.121772</td>\n",
       "      <td>2.309578</td>\n",
       "      <td>3.348912</td>\n",
       "      <td>0.769859</td>\n",
       "      <td>0.198443</td>\n",
       "      <td>2.309466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relv</td>\n",
       "      <td>top2_novo</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.188368</td>\n",
       "      <td>0.132742</td>\n",
       "      <td>0.792882</td>\n",
       "      <td>1.088191</td>\n",
       "      <td>0.264294</td>\n",
       "      <td>0.182349</td>\n",
       "      <td>0.792909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relv</td>\n",
       "      <td>mean_novo</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.158991</td>\n",
       "      <td>0.109885</td>\n",
       "      <td>1.035168</td>\n",
       "      <td>1.266987</td>\n",
       "      <td>0.345056</td>\n",
       "      <td>0.150577</td>\n",
       "      <td>1.034839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>relv</td>\n",
       "      <td>logical_relation_count_total</td>\n",
       "      <td>logical</td>\n",
       "      <td>569</td>\n",
       "      <td>0.147679</td>\n",
       "      <td>0.091513</td>\n",
       "      <td>2.005185</td>\n",
       "      <td>2.555564</td>\n",
       "      <td>0.668395</td>\n",
       "      <td>0.135741</td>\n",
       "      <td>2.004724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aspect                     predictor          agg_label    n  spearman_all  \\\n",
       "9    relv                      max_relv                max  569      0.456717   \n",
       "14   relv        top_quartile_mean_relv  top_quartile_mean  569      0.440802   \n",
       "13   relv                     top2_relv               top2  569      0.428576   \n",
       "15   relv            top_half_mean_relv      top_half_mean  569      0.422771   \n",
       "11   relv                   median_relv             median  569      0.389855   \n",
       "12   relv            harmonic_mean_relv      harmonic_mean  569      0.379047   \n",
       "8    relv                     mean_relv               mean  569      0.378044   \n",
       "10   relv                      min_relv                min  569      0.363697   \n",
       "18   relv                      min_imsc                min  569      0.313578   \n",
       "16   relv                     mean_imsc               mean  569      0.312320   \n",
       "17   relv                      max_imsc                max  569      0.301726   \n",
       "1    relv                      max_info                max  569      0.294644   \n",
       "19   relv                     top2_imsc               top2  569      0.282814   \n",
       "0    relv                     mean_info               mean  569      0.271200   \n",
       "3    relv                     top2_info               top2  569      0.268349   \n",
       "5    relv                      max_novo                max  569      0.223445   \n",
       "20   relv        mem_action_count_total                mem  569      0.208637   \n",
       "7    relv                     top2_novo               top2  569      0.188368   \n",
       "4    relv                     mean_novo               mean  569      0.158991   \n",
       "21   relv  logical_relation_count_total            logical  569      0.147679   \n",
       "\n",
       "    kendall_all   mae_all  rmse_all  nmae_all  spearman_groupcv  mae_groupcv  \n",
       "9      0.364255  0.521470  0.824088  0.173823          0.461389     0.522336  \n",
       "14     0.349652  0.522056  0.823709  0.174019          0.449767     0.522900  \n",
       "13     0.336245  0.504774  0.803190  0.168258          0.437997     0.505624  \n",
       "15     0.331879  0.519334  0.817268  0.173111          0.437560     0.520164  \n",
       "11     0.302913  0.544200  0.828502  0.181400          0.404683     0.544811  \n",
       "12     0.278519  0.589706  0.838008  0.196569          0.399216     0.590025  \n",
       "8      0.277887  0.555258  0.806607  0.185086          0.396807     0.555708  \n",
       "10     0.281306  0.810808  1.075384  0.270269          0.383368     0.810757  \n",
       "18     0.244870  1.243673  1.470957  0.414558          0.316477     1.242982  \n",
       "16     0.224152  0.965825  1.163107  0.321942          0.311868     0.965180  \n",
       "17     0.228870  0.738518  0.993317  0.246173          0.292713     0.738580  \n",
       "1      0.220194  0.749883  1.019798  0.249961          0.282884     0.749615  \n",
       "19     0.205168  0.797979  1.030371  0.265993          0.272869     0.797765  \n",
       "0      0.190356  1.116704  1.307825  0.372235          0.260046     1.115981  \n",
       "3      0.192300  0.842912  1.082280  0.280971          0.255046     0.842389  \n",
       "5      0.167882  0.729965  1.044460  0.243322          0.211821     0.730233  \n",
       "20     0.121772  2.309578  3.348912  0.769859          0.198443     2.309466  \n",
       "7      0.132742  0.792882  1.088191  0.264294          0.182349     0.792909  \n",
       "4      0.109885  1.035168  1.266987  0.345056          0.150577     1.034839  \n",
       "21     0.091513  2.005185  2.555564  0.668395          0.135741     2.004724  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>predictor</th>\n",
       "      <th>agg_label</th>\n",
       "      <th>n</th>\n",
       "      <th>spearman_all</th>\n",
       "      <th>kendall_all</th>\n",
       "      <th>mae_all</th>\n",
       "      <th>rmse_all</th>\n",
       "      <th>nmae_all</th>\n",
       "      <th>spearman_groupcv</th>\n",
       "      <th>mae_groupcv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>imsc</td>\n",
       "      <td>mean_imsc</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.583238</td>\n",
       "      <td>0.442882</td>\n",
       "      <td>0.514463</td>\n",
       "      <td>0.699422</td>\n",
       "      <td>0.171488</td>\n",
       "      <td>0.572524</td>\n",
       "      <td>0.514006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>imsc</td>\n",
       "      <td>median_imsc</td>\n",
       "      <td>median</td>\n",
       "      <td>569</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>0.447850</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.739159</td>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.559337</td>\n",
       "      <td>0.536054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>imsc</td>\n",
       "      <td>harmonic_mean_imsc</td>\n",
       "      <td>harmonic_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.423554</td>\n",
       "      <td>0.537684</td>\n",
       "      <td>0.724701</td>\n",
       "      <td>0.179228</td>\n",
       "      <td>0.556139</td>\n",
       "      <td>0.537138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>imsc</td>\n",
       "      <td>top_half_mean_imsc</td>\n",
       "      <td>top_half_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.570993</td>\n",
       "      <td>0.438492</td>\n",
       "      <td>0.523593</td>\n",
       "      <td>0.726167</td>\n",
       "      <td>0.174531</td>\n",
       "      <td>0.554979</td>\n",
       "      <td>0.523432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>imsc</td>\n",
       "      <td>top_quartile_mean_imsc</td>\n",
       "      <td>top_quartile_mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.550369</td>\n",
       "      <td>0.428760</td>\n",
       "      <td>0.550469</td>\n",
       "      <td>0.760947</td>\n",
       "      <td>0.183490</td>\n",
       "      <td>0.532491</td>\n",
       "      <td>0.550367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>imsc</td>\n",
       "      <td>top2_imsc</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.538926</td>\n",
       "      <td>0.415341</td>\n",
       "      <td>0.533245</td>\n",
       "      <td>0.734308</td>\n",
       "      <td>0.177748</td>\n",
       "      <td>0.519590</td>\n",
       "      <td>0.533168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>imsc</td>\n",
       "      <td>max_imsc</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.530101</td>\n",
       "      <td>0.421510</td>\n",
       "      <td>0.589192</td>\n",
       "      <td>0.806109</td>\n",
       "      <td>0.196397</td>\n",
       "      <td>0.509413</td>\n",
       "      <td>0.589182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>imsc</td>\n",
       "      <td>min_imsc</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>0.364204</td>\n",
       "      <td>0.700029</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>0.233343</td>\n",
       "      <td>0.443644</td>\n",
       "      <td>0.699347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>imsc</td>\n",
       "      <td>max_relv</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.359850</td>\n",
       "      <td>0.283545</td>\n",
       "      <td>1.187786</td>\n",
       "      <td>1.399855</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>0.353348</td>\n",
       "      <td>1.188812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>imsc</td>\n",
       "      <td>top2_relv</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.333717</td>\n",
       "      <td>0.257375</td>\n",
       "      <td>1.148243</td>\n",
       "      <td>1.361800</td>\n",
       "      <td>0.382748</td>\n",
       "      <td>0.324722</td>\n",
       "      <td>1.149246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imsc</td>\n",
       "      <td>max_info</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.285191</td>\n",
       "      <td>0.216177</td>\n",
       "      <td>0.746719</td>\n",
       "      <td>0.954295</td>\n",
       "      <td>0.248906</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>0.746963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>imsc</td>\n",
       "      <td>mem_action_count_total</td>\n",
       "      <td>mem</td>\n",
       "      <td>569</td>\n",
       "      <td>0.265294</td>\n",
       "      <td>0.177844</td>\n",
       "      <td>2.330023</td>\n",
       "      <td>3.600375</td>\n",
       "      <td>0.776674</td>\n",
       "      <td>0.245980</td>\n",
       "      <td>2.330077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imsc</td>\n",
       "      <td>top2_info</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.250713</td>\n",
       "      <td>0.181841</td>\n",
       "      <td>0.690949</td>\n",
       "      <td>0.892322</td>\n",
       "      <td>0.230316</td>\n",
       "      <td>0.234696</td>\n",
       "      <td>0.691116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>imsc</td>\n",
       "      <td>mean_relv</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.232601</td>\n",
       "      <td>0.163230</td>\n",
       "      <td>1.042929</td>\n",
       "      <td>1.278560</td>\n",
       "      <td>0.347643</td>\n",
       "      <td>0.231108</td>\n",
       "      <td>1.043861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>imsc</td>\n",
       "      <td>min_relv</td>\n",
       "      <td>min</td>\n",
       "      <td>569</td>\n",
       "      <td>0.226728</td>\n",
       "      <td>0.169156</td>\n",
       "      <td>0.990949</td>\n",
       "      <td>1.264733</td>\n",
       "      <td>0.330316</td>\n",
       "      <td>0.219445</td>\n",
       "      <td>0.991352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>imsc</td>\n",
       "      <td>max_novo</td>\n",
       "      <td>max</td>\n",
       "      <td>569</td>\n",
       "      <td>0.216042</td>\n",
       "      <td>0.165054</td>\n",
       "      <td>0.931019</td>\n",
       "      <td>1.156052</td>\n",
       "      <td>0.310340</td>\n",
       "      <td>0.208505</td>\n",
       "      <td>0.931860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>imsc</td>\n",
       "      <td>logical_relation_count_total</td>\n",
       "      <td>logical</td>\n",
       "      <td>569</td>\n",
       "      <td>0.204597</td>\n",
       "      <td>0.141624</td>\n",
       "      <td>1.722408</td>\n",
       "      <td>2.449983</td>\n",
       "      <td>0.574136</td>\n",
       "      <td>0.203604</td>\n",
       "      <td>1.722100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imsc</td>\n",
       "      <td>mean_info</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.189305</td>\n",
       "      <td>0.128506</td>\n",
       "      <td>0.719529</td>\n",
       "      <td>0.912738</td>\n",
       "      <td>0.239843</td>\n",
       "      <td>0.182877</td>\n",
       "      <td>0.719412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>imsc</td>\n",
       "      <td>top2_novo</td>\n",
       "      <td>top2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.186599</td>\n",
       "      <td>0.134826</td>\n",
       "      <td>0.870972</td>\n",
       "      <td>1.094998</td>\n",
       "      <td>0.290324</td>\n",
       "      <td>0.182779</td>\n",
       "      <td>0.871696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imsc</td>\n",
       "      <td>mean_novo</td>\n",
       "      <td>mean</td>\n",
       "      <td>569</td>\n",
       "      <td>0.088848</td>\n",
       "      <td>0.058808</td>\n",
       "      <td>0.836217</td>\n",
       "      <td>1.058845</td>\n",
       "      <td>0.278739</td>\n",
       "      <td>0.090738</td>\n",
       "      <td>0.836619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aspect                     predictor          agg_label    n  spearman_all  \\\n",
       "12   imsc                     mean_imsc               mean  569      0.583238   \n",
       "15   imsc                   median_imsc             median  569      0.566800   \n",
       "16   imsc            harmonic_mean_imsc      harmonic_mean  569      0.566941   \n",
       "19   imsc            top_half_mean_imsc      top_half_mean  569      0.570993   \n",
       "18   imsc        top_quartile_mean_imsc  top_quartile_mean  569      0.550369   \n",
       "17   imsc                     top2_imsc               top2  569      0.538926   \n",
       "13   imsc                      max_imsc                max  569      0.530101   \n",
       "14   imsc                      min_imsc                min  569      0.455205   \n",
       "9    imsc                      max_relv                max  569      0.359850   \n",
       "11   imsc                     top2_relv               top2  569      0.333717   \n",
       "1    imsc                      max_info                max  569      0.285191   \n",
       "20   imsc        mem_action_count_total                mem  569      0.265294   \n",
       "3    imsc                     top2_info               top2  569      0.250713   \n",
       "8    imsc                     mean_relv               mean  569      0.232601   \n",
       "10   imsc                      min_relv                min  569      0.226728   \n",
       "5    imsc                      max_novo                max  569      0.216042   \n",
       "21   imsc  logical_relation_count_total            logical  569      0.204597   \n",
       "0    imsc                     mean_info               mean  569      0.189305   \n",
       "7    imsc                     top2_novo               top2  569      0.186599   \n",
       "4    imsc                     mean_novo               mean  569      0.088848   \n",
       "\n",
       "    kendall_all   mae_all  rmse_all  nmae_all  spearman_groupcv  mae_groupcv  \n",
       "12     0.442882  0.514463  0.699422  0.171488          0.572524     0.514006  \n",
       "15     0.447850  0.536585  0.739159  0.178862          0.559337     0.536054  \n",
       "16     0.423554  0.537684  0.724701  0.179228          0.556139     0.537138  \n",
       "19     0.438492  0.523593  0.726167  0.174531          0.554979     0.523432  \n",
       "18     0.428760  0.550469  0.760947  0.183490          0.532491     0.550367  \n",
       "17     0.415341  0.533245  0.734308  0.177748          0.519590     0.533168  \n",
       "13     0.421510  0.589192  0.806109  0.196397          0.509413     0.589182  \n",
       "14     0.364204  0.700029  0.929762  0.233343          0.443644     0.699347  \n",
       "9      0.283545  1.187786  1.399855  0.395929          0.353348     1.188812  \n",
       "11     0.257375  1.148243  1.361800  0.382748          0.324722     1.149246  \n",
       "1      0.216177  0.746719  0.954295  0.248906          0.275053     0.746963  \n",
       "20     0.177844  2.330023  3.600375  0.776674          0.245980     2.330077  \n",
       "3      0.181841  0.690949  0.892322  0.230316          0.234696     0.691116  \n",
       "8      0.163230  1.042929  1.278560  0.347643          0.231108     1.043861  \n",
       "10     0.169156  0.990949  1.264733  0.330316          0.219445     0.991352  \n",
       "5      0.165054  0.931019  1.156052  0.310340          0.208505     0.931860  \n",
       "21     0.141624  1.722408  2.449983  0.574136          0.203604     1.722100  \n",
       "0      0.128506  0.719529  0.912738  0.239843          0.182877     0.719412  \n",
       "7      0.134826  0.870972  1.094998  0.290324          0.182779     0.871696  \n",
       "4      0.058808  0.836217  1.058845  0.278739          0.090738     0.836619  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from collections import defaultdict\n",
    "\n",
    "def _pairmask(a: pd.Series, b: pd.Series) -> np.ndarray:\n",
    "    # finite pairwise mask\n",
    "    return np.isfinite(a.values) & np.isfinite(b.values)\n",
    "\n",
    "def _spearman(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    rho, _ = spearmanr(a, b)\n",
    "    return 0.0 if np.isnan(rho) else float(rho)\n",
    "\n",
    "def _kendall(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    tau, _ = kendalltau(a, b)\n",
    "    return 0.0 if np.isnan(tau) else float(tau)\n",
    "\n",
    "def _normalize_mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    # MAE normalized by target range (assumes 1..4 scale)\n",
    "    denom = (np.nanmax(y_true) - np.nanmin(y_true))\n",
    "    if not np.isfinite(denom) or denom == 0:\n",
    "        return np.nan\n",
    "    return mean_absolute_error(y_true, y_pred) / denom\n",
    "\n",
    "def _residualize(y: pd.Series, x: pd.Series) -> pd.Series:\n",
    "    # simple linear residual y ~ a + b*x (ignores NaNs properly)\n",
    "    m = _pairmask(y, x)\n",
    "    if m.sum() < 3:\n",
    "        return y.copy()\n",
    "    X = np.c_[np.ones(m.sum()), x.values[m]]\n",
    "    beta = np.linalg.lstsq(X, y.values[m], rcond=None)[0]\n",
    "    yhat = beta[0] + beta[1] * x.values\n",
    "    res = y - yhat\n",
    "    return res\n",
    "\n",
    "def evaluate_aggregations(\n",
    "    df: pd.DataFrame,\n",
    "    aspect: str = \"info\",                         # target column name at utterance level (float 1..4)\n",
    "    group_col: Optional[str] = None,              # e.g., \"conv_id\" for grouped CV Spearman/MAE\n",
    "    length_col: Optional[str] = \"utterance_token_count\",  # optional residualization\n",
    "    consider_cols: Optional[List[str]] = None,    # optional whitelist of predictor columns\n",
    "    drop_cols_exact: Optional[List[str]] = None,  # columns to explicitly drop\n",
    "    regex_include: str = r\"^(mean|max|min|std|top\\d+|prop_[^ ]+|.*cig.*|.*entropy.*|.*density.*|.*count.*)_\",\n",
    "    regex_suffix_aspect: Optional[str] = None,    # if your columns end with f\"_{aspect}\", set to None; otherwise pattern\n",
    "    n_splits: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare how each aggregated claim-level predictor column maps to an utterance-level target (e.g., 'info').\n",
    "    Returns a DataFrame with metrics per predictor.\n",
    "    \"\"\"\n",
    "    assert aspect in df.columns, f\"Target column '{aspect}' not found.\"\n",
    "    y = pd.to_numeric(df[aspect], errors=\"coerce\")\n",
    "\n",
    "    # Candidate predictors\n",
    "    if consider_cols is None:\n",
    "        # pick numeric columns that *reference* the aspect somewhere or are general aggregates\n",
    "        # Heuristic: include columns whose names either contain f\"_{aspect}\" or match general aggregate patterns\n",
    "        patt = re.compile(regex_include)\n",
    "        cols = []\n",
    "        for c in df.columns:\n",
    "            if c == aspect:\n",
    "                continue\n",
    "            if drop_cols_exact and c in drop_cols_exact:\n",
    "                continue\n",
    "            if c == group_col:\n",
    "                continue\n",
    "            if df[c].dtype.kind in \"biufc\":  # numeric-ish\n",
    "                # Keep if name hints at being an aggregation or contains the aspect name\n",
    "                if (f\"_{aspect}\" in c) or patt.search(c):\n",
    "                    cols.append(c)\n",
    "        candidate_cols = cols\n",
    "    else:\n",
    "        candidate_cols = [c for c in consider_cols if c in df.columns]\n",
    "\n",
    "    # Optional residualize target on length to see beyond length bias\n",
    "    if length_col is not None and length_col in df.columns:\n",
    "        y_base = _residualize(y, pd.to_numeric(df[length_col], errors=\"coerce\"))\n",
    "    else:\n",
    "        y_base = y.copy()\n",
    "\n",
    "    records: List[Dict] = []\n",
    "\n",
    "    # Group-aware CV function (for Spearman/MAE)\n",
    "    def cv_scores(pred_col: str) -> Tuple[float, float]:\n",
    "        pred = pd.to_numeric(df[pred_col], errors=\"coerce\")\n",
    "        m = _pairmask(y_base, pred)\n",
    "        if m.sum() < 5:\n",
    "            return np.nan, np.nan\n",
    "        if group_col is None or df[group_col].nunique() < n_splits:\n",
    "            # simple hold-outless (single) metrics on all valid pairs\n",
    "            return (\n",
    "                _spearman(y_base.values[m], pred.values[m]),\n",
    "                mean_absolute_error(y.values[m], pred.values[m]),\n",
    "            )\n",
    "        # grouped CV\n",
    "        sp_folds, mae_folds = [], []\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        groups = df[group_col].astype(str).values[m]\n",
    "        yb = y_base.values[m]\n",
    "        yt = y.values[m]\n",
    "        xb = pred.values[m]\n",
    "        for tr, te in gkf.split(xb, yb, groups):\n",
    "            sp_folds.append(_spearman(yb[te], xb[te]))\n",
    "            mae_folds.append(mean_absolute_error(yt[te], xb[te]))\n",
    "        return float(np.mean(sp_folds)), float(np.mean(mae_folds))\n",
    "\n",
    "    for col in candidate_cols:\n",
    "        pred = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        m = _pairmask(y_base, pred)\n",
    "        if m.sum() < 5:\n",
    "            continue\n",
    "\n",
    "        # Plain (no CV) metrics on all valid rows\n",
    "        sp_all = _spearman(y_base.values[m], pred.values[m])\n",
    "        kd_all = _kendall(y_base.values[m], pred.values[m])\n",
    "        mae_all = mean_absolute_error(y.values[m], pred.values[m])\n",
    "        rmse_all = mean_squared_error(y.values[m], pred.values[m], squared=True) ** 0.5\n",
    "        nmae_all = _normalize_mae(y.values[m], pred.values[m])\n",
    "\n",
    "        # Grouped CV summary (or same as all if no group)\n",
    "        sp_cv, mae_cv = cv_scores(col)\n",
    "\n",
    "        # try to extract an aggregation label: prefix before the aspect\n",
    "        # e.g., 'mean_info' -> agg='mean'; 'max_info' -> 'max'; 'prop_info_ge_3' -> 'prop'\n",
    "        agg_label = None\n",
    "        if f\"_{aspect}\" in col:\n",
    "            agg_label = col.split(f\"_{aspect}\", 1)[0]\n",
    "        else:\n",
    "            # fallback: prefix before first underscore\n",
    "            agg_label = col.split(\"_\", 1)[0] if \"_\" in col else col\n",
    "\n",
    "        records.append({\n",
    "            \"aspect\": aspect,\n",
    "            \"predictor\": col,\n",
    "            \"agg_label\": agg_label,\n",
    "            \"n\": int(m.sum()),\n",
    "            \"spearman_all\": sp_all,\n",
    "            \"kendall_all\": kd_all,\n",
    "            \"mae_all\": mae_all,\n",
    "            \"rmse_all\": rmse_all,\n",
    "            \"nmae_all\": nmae_all,\n",
    "            \"spearman_groupcv\": sp_cv,\n",
    "            \"mae_groupcv\": mae_cv,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame.from_records(records).sort_values(\n",
    "        [\"spearman_groupcv\", \"spearman_all\", \"mae_groupcv\"], ascending=[False, False, True]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "aspects = [\"info\", \"novo\", \"relv\", \"imsc\"]  # or just [\"info\"]\n",
    "for a in aspects:\n",
    "    res = evaluate_aggregations(cl_soft, aspect=a, group_col=\"conv_id\",\n",
    "                                drop_cols_exact=[\"conv_id\", \"utterance_id\"])\n",
    "    display(res.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2783b721-2259-4f97-87a3-b9f6da283bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['has_claims', 'n_claims', 'n_changes', 'p_changes',\n",
       "       'utterance_token_count', 'claim_density', 'change_density', 'mean_info',\n",
       "       'max_info', 'min_info', 'median_info', 'harmonic_mean_info',\n",
       "       'top2_info', 'top_quartile_mean_info', 'top_half_mean_info',\n",
       "       'mean_novo', 'max_novo', 'min_novo', 'median_novo',\n",
       "       'harmonic_mean_novo', 'top2_novo', 'top_quartile_mean_novo',\n",
       "       'top_half_mean_novo', 'mean_relv', 'max_relv', 'min_relv',\n",
       "       'median_relv', 'harmonic_mean_relv', 'top2_relv',\n",
       "       'top_quartile_mean_relv', 'top_half_mean_relv', 'mean_imsc', 'max_imsc',\n",
       "       'min_imsc', 'median_imsc', 'harmonic_mean_imsc', 'top2_imsc',\n",
       "       'top_quartile_mean_imsc', 'top_half_mean_imsc',\n",
       "       'mem_action_count_total', 'mem_action_prop_ADD', 'mem_action_prop_NONE',\n",
       "       'mem_action_prop_UPDATE', 'mem_action_entropy',\n",
       "       'logical_relation_count_total',\n",
       "       'logical_relation_prop_backward_entailment',\n",
       "       'logical_relation_prop_contradiction',\n",
       "       'logical_relation_prop_equivalent',\n",
       "       'logical_relation_prop_forward_entailment',\n",
       "       'logical_relation_prop_neutral', 'logical_relation_entropy', 'conv_id',\n",
       "       'utterance_id', 'corpus', 'info', 'novo', 'relv', 'imsc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_soft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f5da6cda-6338-4c02-9992-c6798ba8372e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_claims</th>\n",
       "      <th>n_claims</th>\n",
       "      <th>n_changes</th>\n",
       "      <th>p_changes</th>\n",
       "      <th>utterance_token_count</th>\n",
       "      <th>claim_density</th>\n",
       "      <th>change_density</th>\n",
       "      <th>mean_info</th>\n",
       "      <th>max_info</th>\n",
       "      <th>min_info</th>\n",
       "      <th>...</th>\n",
       "      <th>logical_relation_prop_forward_entailment</th>\n",
       "      <th>logical_relation_prop_neutral</th>\n",
       "      <th>logical_relation_entropy</th>\n",
       "      <th>conv_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>info</th>\n",
       "      <th>novo</th>\n",
       "      <th>relv</th>\n",
       "      <th>imsc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>insq_2228</td>\n",
       "      <td>2306</td>\n",
       "      <td>insq</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.442823e-12</td>\n",
       "      <td>insq_2228</td>\n",
       "      <td>2308</td>\n",
       "      <td>insq</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>121</td>\n",
       "      <td>0.041322</td>\n",
       "      <td>0.041322</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-1.442823e-12</td>\n",
       "      <td>insq_2228</td>\n",
       "      <td>2310</td>\n",
       "      <td>insq</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>153</td>\n",
       "      <td>0.071895</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>5.435644e-01</td>\n",
       "      <td>insq_2228</td>\n",
       "      <td>2312</td>\n",
       "      <td>insq</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.442823e-12</td>\n",
       "      <td>insq_2228</td>\n",
       "      <td>2314</td>\n",
       "      <td>insq</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>fora_2239</td>\n",
       "      <td>EGI_Conversations_196_2239_340</td>\n",
       "      <td>fora</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.442823e-12</td>\n",
       "      <td>fora_2239</td>\n",
       "      <td>EGI_Conversations_196_2239_341</td>\n",
       "      <td>fora</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>fora_2239</td>\n",
       "      <td>EGI_Conversations_196_2239_342</td>\n",
       "      <td>fora</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>162</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>fora_2239</td>\n",
       "      <td>EGI_Conversations_196_2239_343</td>\n",
       "      <td>fora</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.442823e-12</td>\n",
       "      <td>fora_2239</td>\n",
       "      <td>EGI_Conversations_196_2239_345</td>\n",
       "      <td>fora</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     has_claims  n_claims  n_changes  p_changes  utterance_token_count  \\\n",
       "0             1         3          3   1.000000                     35   \n",
       "1             1         1          1   1.000000                     12   \n",
       "2             1         5          5   1.000000                    121   \n",
       "3             1        11         10   0.909091                    153   \n",
       "4             1         1          1   1.000000                     11   \n",
       "..          ...       ...        ...        ...                    ...   \n",
       "564           1         2          2   1.000000                     11   \n",
       "565           1         2          2   1.000000                     27   \n",
       "566           0         0          0   0.000000                      7   \n",
       "567           1         8          7   0.875000                    162   \n",
       "568           1         2          2   1.000000                     27   \n",
       "\n",
       "     claim_density  change_density  mean_info  max_info  min_info  ...  \\\n",
       "0         0.085714        0.085714   2.333333       3.0       2.0  ...   \n",
       "1         0.083333        0.083333   3.000000       3.0       3.0  ...   \n",
       "2         0.041322        0.041322   2.200000       3.0       2.0  ...   \n",
       "3         0.071895        0.065359   2.090909       3.0       1.0  ...   \n",
       "4         0.090909        0.090909   3.000000       3.0       3.0  ...   \n",
       "..             ...             ...        ...       ...       ...  ...   \n",
       "564       0.181818        0.181818   1.500000       2.0       1.0  ...   \n",
       "565       0.074074        0.074074   1.000000       1.0       1.0  ...   \n",
       "566       0.000000        0.000000   1.000000       1.0       1.0  ...   \n",
       "567       0.049383        0.043210   2.375000       3.0       1.0  ...   \n",
       "568       0.074074        0.074074   2.000000       3.0       1.0  ...   \n",
       "\n",
       "     logical_relation_prop_forward_entailment  logical_relation_prop_neutral  \\\n",
       "0                                         0.0                       0.333333   \n",
       "1                                         0.0                       1.000000   \n",
       "2                                         0.0                       0.600000   \n",
       "3                                         0.0                       0.636364   \n",
       "4                                         0.0                       0.000000   \n",
       "..                                        ...                            ...   \n",
       "564                                       0.0                       0.500000   \n",
       "565                                       0.0                       0.500000   \n",
       "566                                       1.0                       1.000000   \n",
       "567                                       0.0                       0.250000   \n",
       "568                                       0.0                       0.500000   \n",
       "\n",
       "     logical_relation_entropy    conv_id                    utterance_id  \\\n",
       "0                1.000000e+00  insq_2228                            2306   \n",
       "1               -1.442823e-12  insq_2228                            2308   \n",
       "2               -1.442823e-12  insq_2228                            2310   \n",
       "3                5.435644e-01  insq_2228                            2312   \n",
       "4               -1.442823e-12  insq_2228                            2314   \n",
       "..                        ...        ...                             ...   \n",
       "564              1.000000e+00  fora_2239  EGI_Conversations_196_2239_340   \n",
       "565             -1.442823e-12  fora_2239  EGI_Conversations_196_2239_341   \n",
       "566              1.000000e+00  fora_2239  EGI_Conversations_196_2239_342   \n",
       "567              1.000000e+00  fora_2239  EGI_Conversations_196_2239_343   \n",
       "568             -1.442823e-12  fora_2239  EGI_Conversations_196_2239_345   \n",
       "\n",
       "     corpus      info  novo  relv  imsc  \n",
       "0      insq  1.800000  2.80  2.80  2.80  \n",
       "1      insq  1.800000  1.40  2.20  2.00  \n",
       "2      insq  3.000000  3.20  3.20  2.40  \n",
       "3      insq  3.400000  2.60  3.40  3.40  \n",
       "4      insq  1.400000  2.20  2.80  2.40  \n",
       "..      ...       ...   ...   ...   ...  \n",
       "564    fora  2.000000  1.50  2.75  3.00  \n",
       "565    fora  2.000000  2.25  3.00  2.25  \n",
       "566    fora  1.000000  1.00  1.00  2.00  \n",
       "567    fora  2.333333  3.00  3.25  2.75  \n",
       "568    fora  1.000000  2.25  3.00  3.00  \n",
       "\n",
       "[569 rows x 58 columns]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7880cd2-642d-4c3e-a639-9e096524cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cl_soft\n",
    "df = df[['mean_novo', 'max_novo', 'min_novo', 'median_novo', 'harmonic_mean_novo',\n",
    "       'top_half_mean_novo', 'top2_novo', 'top_quartile_mean_novo', 'top2_relv', 'mean_relv', 'max_relv', 'min_relv',\n",
    "       'median_relv', 'harmonic_mean_relv', 'top_quartile_mean_relv', 'top_half_mean_relv', 'top2_imsc',\n",
    "       'mean_imsc', 'max_imsc', 'min_imsc', 'median_imsc',\n",
    "       'harmonic_mean_imsc', 'top_quartile_mean_imsc', 'top_half_mean_imsc', \"info\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "dcbbe0d0-b616-4c5d-a3ed-fa132cb74993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell builds a heatmap of MAE for predicting utterance-level \"info\"\n",
    "# using cross-aspect combiners (over novo/relv/imsc) Ã— claim-level aggregation methods.\n",
    "#\n",
    "# It expects a DataFrame named `cl_soft` already in the environment with columns like:\n",
    "# 'mean_info', 'max_info', ..., 'mean_novo', 'max_novo', ..., etc., and target 'info'.\n",
    "#\n",
    "# Outputs:\n",
    "# - A DataFrame of MAE values (rows: cross-aspect combiners, cols: claim-level agg methods)\n",
    "# - A matplotlib heatmap saved to /mnt/data/cross_aspect_agg_mae_heatmap.png\n",
    "# - A CSV of the MAE grid saved to /mnt/data/cross_aspect_agg_mae_grid.csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- sanity: check cl_soft exists ---\n",
    "# try:\n",
    "#     cl_soft\n",
    "# except NameError as e:\n",
    "#     raise SystemExit(\"`cl_soft` DataFrame not found in the environment. Please define it and rerun.\")\n",
    "\n",
    "# df = cl_soft.copy()\n",
    "\n",
    "# Target\n",
    "if \"info\" not in df.columns:\n",
    "    raise SystemExit(\"The target column 'info' was not found in cl_soft.\")\n",
    "\n",
    "y = pd.to_numeric(df[\"info\"], errors=\"coerce\").values\n",
    "\n",
    "# Per-aspect columns pattern mapping\n",
    "claim_agg_map = {\n",
    "    \"mean\": \"mean_{a}\",\n",
    "    \"max\": \"max_{a}\",\n",
    "    \"min\": \"min_{a}\",\n",
    "    \"median\": \"median_{a}\",\n",
    "    \"harmonic\": \"harmonic_mean_{a}\",\n",
    "    \"top2\": \"top2_{a}\",\n",
    "    # \"top1\": \"top1_{a}\",\n",
    "    \"top_quartile_mean\": \"top_quartile_mean_{a}\",\n",
    "    \"top_half_mean\": \"top_half_mean_{a}\",\n",
    "}\n",
    "\n",
    "# Limit to methods actually present for all three aspects\n",
    "aspects = [\"novo\", \"relv\", \"imsc\"]\n",
    "\n",
    "available_methods = []\n",
    "missing_report = {}\n",
    "for method, patt in claim_agg_map.items():\n",
    "    needed = [patt.format(a=a) for a in aspects]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if not missing:\n",
    "        available_methods.append(method)\n",
    "    else:\n",
    "        missing_report[method] = missing\n",
    "\n",
    "# If nothing available, stop\n",
    "if not available_methods:\n",
    "    raise SystemExit(f\"No complete method had all required columns. Missing: {missing_report}\")\n",
    "\n",
    "# helper: numeric matrix for a given method with shape (N, 3) for (novo, relv, imsc)\n",
    "def get_matrix_for_method(method: str) -> np.ndarray:\n",
    "    cols = [claim_agg_map[method].format(a=a) for a in aspects]\n",
    "    M = df[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(dtype=float)\n",
    "    return M\n",
    "\n",
    "# cross-aspect combiners (operate row-wise across the 3 aspect columns)\n",
    "def comb_min(M):\n",
    "    return np.nanmin(M, axis=1)\n",
    "\n",
    "def comb_max(M):\n",
    "    return np.nanmax(M, axis=1)\n",
    "\n",
    "def comb_mean(M):\n",
    "    return np.nanmean(M, axis=1)\n",
    "\n",
    "def comb_median(M):\n",
    "    return np.nanmedian(M, axis=1)\n",
    "\n",
    "def comb_geometric(M):\n",
    "    # geometric mean over positives; otherwise fall back to NaN for that row\n",
    "    # (Scores are 1..4 typically, so positives; but we check anyway)\n",
    "    Mp = np.where(M > 0, M, np.nan)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        return np.exp(np.nanmean(np.log(Mp), axis=1))\n",
    "\n",
    "def comb_harmonic(M):\n",
    "    # harmonic mean over positives; skip NaNs\n",
    "    Mp = np.where(M > 0, M, np.nan)\n",
    "    inv = 1.0 / Mp\n",
    "    k = np.sum(np.isfinite(Mp), axis=1)\n",
    "    s = np.nansum(inv, axis=1)\n",
    "    out = np.full(M.shape[0], np.nan, dtype=float)\n",
    "    mask = (k > 0) & np.isfinite(s) & (s > 0)\n",
    "    out[mask] = k[mask] / s[mask]\n",
    "    return out\n",
    "\n",
    "def comb_power2(M):\n",
    "    # Power mean with p=2\n",
    "    return (np.nanmean(M**2, axis=1))**0.5\n",
    "\n",
    "def comb_softmax(M):\n",
    "    # Softmax-weighted average across aspects with fixed temperature\n",
    "    # This keeps it unsupervised. tau=1.0\n",
    "    tau = 1.0\n",
    "    # handle NaNs: replace with very small to avoid dominating; then restore NaNs via mask\n",
    "    R = M.copy()\n",
    "    nanmask = ~np.isfinite(R)\n",
    "    R[nanmask] = np.nanmin(R) if np.isfinite(np.nanmin(R)) else 0.0\n",
    "    ex = np.exp((R - np.nanmax(R, axis=1, keepdims=True)) / tau)\n",
    "    ex[nanmask] = 0.0\n",
    "    w = ex / np.sum(ex, axis=1, keepdims=True)\n",
    "    out = np.sum(w * R, axis=1)\n",
    "    # rows that were all NaN remain NaN\n",
    "    all_nan = np.all(~np.isfinite(M), axis=1)\n",
    "    out[all_nan] = np.nan\n",
    "    return out\n",
    "\n",
    "comb_funcs = {\n",
    "    \"min\": comb_min,\n",
    "    \"max\": comb_max,\n",
    "    \"mean\": comb_mean,\n",
    "    \"median\": comb_median,\n",
    "    \"geometric\": comb_geometric,\n",
    "    \"harmonic\": comb_harmonic,\n",
    "    \"power2\": comb_power2,\n",
    "    \"softmax\": comb_softmax,\n",
    "}\n",
    "\n",
    "# Compute MAE grid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae_grid = pd.DataFrame(index=sorted(comb_funcs.keys()), columns=sorted(available_methods), dtype=float)\n",
    "\n",
    "for method in sorted(available_methods):\n",
    "    M = get_matrix_for_method(method)\n",
    "    for comb_name, comb in comb_funcs.items():\n",
    "        pred = comb(M)\n",
    "        mask = np.isfinite(pred) & np.isfinite(y)\n",
    "        if mask.sum() == 0:\n",
    "            mae = np.nan\n",
    "        else:\n",
    "            mae = mean_absolute_error(y[mask], pred[mask])\n",
    "        mae_grid.loc[comb_name, method] = mae\n",
    "\n",
    "# Save grid\n",
    "# os.makedirs(\"/mnt/data\", exist_ok=True)\n",
    "# csv_path = \"/mnt/data/cross_aspect_agg_mae_grid.csv\"\n",
    "# mae_grid.to_csv(csv_path)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use a lighter diverging colormap where lower values are green (better)\n",
    "cmap = plt.cm.GnBu   # green (low) â†’ yellow â†’ red (high)\n",
    "vmin, vmax = np.nanmin(mae_grid.values), np.nanmax(mae_grid.values)\n",
    "\n",
    "im = plt.imshow(mae_grid.values, aspect=\"auto\", cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "plt.xticks(ticks=np.arange(mae_grid.shape[1]), labels=mae_grid.columns, rotation=45, ha=\"right\")\n",
    "plt.yticks(ticks=np.arange(mae_grid.shape[0]), labels=mae_grid.index)\n",
    "plt.title(\"MAE: Cross-Aspect Combiner Ã— Claim-Level Aggregation Method\\n(target = info; green = lower/better)\")\n",
    "\n",
    "cbar = plt.colorbar(im, shrink=0.8)\n",
    "cbar.set_label(\"MAE (lower is better)\")\n",
    "\n",
    "# annotate values\n",
    "mid = (vmin + vmax) / 2.0\n",
    "for i in range(mae_grid.shape[0]):\n",
    "    for j in range(mae_grid.shape[1]):\n",
    "        val = mae_grid.values[i, j]\n",
    "        if np.isfinite(val):\n",
    "            # Use white text on darker (higher) regions, black on lighter (lower) ones\n",
    "            txt_color = \"white\" if val > mid else \"black\"\n",
    "            plt.text(j, i, f\"{val:.3f}\", ha=\"center\", va=\"center\", fontsize=8, color=txt_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# csv_path, png_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce5509-dcd5-4ec6-ab0c-9fc2e7e95a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "new_analysis_cell",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_runs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a deep copy of runs to avoid modifying the original data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m runs_for_analysis \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(llm_runs)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use the first run gpt-5_summary as the ground truth\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Note: We assume gpt-5_summary is present in the runs\u001b[39;00m\n\u001b[1;32m      9\u001b[0m summary_gt_source \u001b[38;5;241m=\u001b[39m runs_for_analysis[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-5_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_runs' is not defined"
     ]
    }
   ],
   "source": [
    "# Analysis against gpt-5_summary as ground truth\n",
    "import copy\n",
    "\n",
    "# Create a deep copy of runs to avoid modifying the original data\n",
    "runs_for_analysis = copy.deepcopy(llm_runs)\n",
    "\n",
    "# Use the first run gpt-5_summary as the ground truth\n",
    "# Note: We assume gpt-5_summary is present in the runs\n",
    "summary_gt_source = runs_for_analysis[0][\"gpt-5_summary\"]\n",
    "ids = runs_for_analysis[0][\"ids\"]\n",
    "\n",
    "# Construct new Ground Truth dictionary\n",
    "summary_gt = {\n",
    "    \"ids\": ids,\n",
    "    \"soft\": summary_gt_source,\n",
    "    \"hard\": summary_gt_source\n",
    "}\n",
    "\n",
    "# Remove gpt-5_summary from the runs to avoid comparing it to itself\n",
    "for run in runs_for_analysis:\n",
    "    if \"gpt-5_summary\" in run:\n",
    "        del run[\"gpt-5_summary\"]\n",
    "    # Ensure hard/soft are removed if they exist\n",
    "    if \"hard\" in run: del run[\"hard\"]\n",
    "    if \"soft\" in run: del run[\"soft\"]\n",
    "\n",
    "# Evaluate using the new ground truth\n",
    "per_run_summary_vs_gpt5, agg_summary_vs_gpt5 = evaluate_llm_runs(runs_for_analysis, summary_gt)\n",
    "\n",
    "print(\"Evaluation against gpt-5_summary (Run 0) as Ground Truth\")\n",
    "# Display only requested metrics\n",
    "display(agg_summary_vs_gpt5[[\"method\", \"aspect\", \"mae_soft_mean\", \"pearson_soft_mean\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59120c6-9a1e-4958-810d-398db918e52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
