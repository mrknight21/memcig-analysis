{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda0037d-2af3-4af0-90f5-05ee45c493da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0de1b5-1bae-4b25-a5b1-ba6294aa74b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394afafd-914a-4e41-b098-b90f1211dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def filter_latest_by_user(records):\n",
    "    \"\"\"\n",
    "    Given a list of dicts with keys 'user_id' and 'date_time',\n",
    "    returns a new list containing only the latest record per unique user_id.\n",
    "    \"\"\"\n",
    "    latest_by_user = {}\n",
    "    for record in records:\n",
    "        user = record['user_id']\n",
    "        dt = datetime.strptime(record['date_time'], '%Y-%m-%d %H:%M:%S')\n",
    "        if user not in latest_by_user or dt > latest_by_user[user]['dt']:\n",
    "            latest_by_user[user] = {'record': record, 'dt': dt}\n",
    "    return [entry['record'] for entry in latest_by_user.values()]\n",
    "\n",
    "# Example:\n",
    "# filtered_list = filter_latest_by_user(data)\n",
    "# print(filtered_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad7911b-dc1e-4a81-bcf6-5320e521da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import krippendorff\n",
    "\n",
    "def calculate_ordinal_kpf_alpha(reocrds):\n",
    "    rows = []\n",
    "    for rec in reocrds:\n",
    "        uid = rec['user_id']\n",
    "        for t in rec['target_utterances']:\n",
    "            rows.append({\n",
    "                'utterance_id': t['utterance_id'],\n",
    "                'user_id':       uid,\n",
    "                'label':         t['labels']['informativeness']\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    matrix = df.pivot(index='utterance_id',\n",
    "                      columns='user_id',\n",
    "                      values='label')\n",
    "    # krippendorff.alpha wants a 2D array shape (n_categories, n_subjects),\n",
    "    # so we transpose: categories × items, filling missing with np.nan\n",
    "    data = matrix.T.values\n",
    "\n",
    "    α = krippendorff.alpha(\n",
    "        reliability_data = data,\n",
    "        level_of_measurement = 'ordinal'\n",
    "    )\n",
    "    return α\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d82b26-f006-4544-8a08-4a67f4cb4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def compute_weighted_accuracy(truths, preds):\n",
    "    maxd = 3\n",
    "    w = lambda i,j: 1 - abs(i-j)/maxd\n",
    "    return sum(w(t,p) for t,p in zip(truths,preds)) / len(truths)\n",
    "\n",
    "def compute_performance(records, ground_truth):\n",
    "    # map utterance_id → predicted label\n",
    "    weighted_accs = []\n",
    "    for rec in records:\n",
    "        preds  = [t['labels']['informativeness'] for t in rec['target_utterances']]\n",
    "        truths = [ground_truth[t['utterance_id']] for t in rec['target_utterances']]\n",
    "        weighted_accs.append(compute_weighted_accuracy(truths, preds))\n",
    "    return sum(weighted_accs)/len(weighted_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a911fdd6-1655-4ed9-9b6c-5338aee2fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_annotation_details(utt_id, records, ground_truth, utterances):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - utt_id (int or str)\n",
    "      - records: list of annotation dicts\n",
    "      - ground_truth: dict mapping utt_id -> true label\n",
    "      - utterances: list of dicts with keys 'utterance_id' and 'text'\n",
    "    Prints:\n",
    "      - the utterance text\n",
    "      - the ground truth label\n",
    "      - each annotator's vote (using their latest submission)\n",
    "    \"\"\"\n",
    "    # Build a lookup for utterance text\n",
    "    utt_map = {int(u['utterance_id']): u['utterance_text'] for u in utterances}\n",
    "    \n",
    "    # Fetch and print text\n",
    "    text = utt_map.get(int(utt_id), \"[Text not found]\")\n",
    "    print(f\"Utterance {utt_id} text:\\n{text}\\n\")\n",
    "    \n",
    "    # Print ground truth\n",
    "    gt = ground_truth.get(str(utt_id), \"[No ground truth]\")\n",
    "    print(f\"Ground truth informativeness: {gt}\\n\")\n",
    "    \n",
    "    records\n",
    "    \n",
    "    # Collect and print annotator votes\n",
    "    print(\"Annotator votes:\")\n",
    "    for rec in records:\n",
    "        preds = {\n",
    "            t['utterance_id']: t['labels']['informativeness']\n",
    "            for t in rec['target_utterances']\n",
    "        }\n",
    "        if str(utt_id) in preds:\n",
    "            print(f\" - {rec['user_id']}: {preds[str(utt_id)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9fd9a35-aa32-4edc-b9a4-142d90e63eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prescreen_data = load_json(\"infogain_annotation.prescreen.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8e7202-f05e-4905-8d18-dddbaa6564b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ground_truth = load_json(\"../static/prescreen_sample.json\")\n",
    "ground_truth_ans = {str(gt['utterance_id']): gt['answers']['informativeness'] for gt in ground_truth['target_utterances']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fa7bea-87ef-41a9-80b9-9cafdb51bc13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prescreen_data = [p for p in prescreen_data if len(p['target_utterances']) == 6 and p[\"user_id\"] not in [\"maychill\", \"test\", \"123\", \"345\", \"678\", \"67fd46db088825e305648e1a\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9ac928-b5ea-46f8-a772-8d0212f56b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prescreen_data = filter_latest_by_user(prescreen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabaa39d-cbf9-4fdb-9ab9-059a394829c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff’s α (ordinal): 0.8684054694229113\n"
     ]
    }
   ],
   "source": [
    "α = calculate_ordinal_kpf_alpha(prescreen_data)\n",
    "print(\"Krippendorff’s α (ordinal):\", α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e437ec72-564a-4ffb-90dd-cb83640c0388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy = 0.806\n"
     ]
    }
   ],
   "source": [
    "acc = compute_performance(prescreen_data, ground_truth_ans)\n",
    "print(f\"Mean Accuracy = {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf78034-0c7c-42c7-9f29-aa910fb0d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   utterance_id  n_annotators  adjusted_accuracy\n",
      "0           101             6           0.444444\n",
      "1           102             6           1.000000\n",
      "2           103             6           1.000000\n",
      "3           104             6           0.888889\n",
      "4           105             6           0.555556\n",
      "5           106             6           0.944444\n"
     ]
    }
   ],
   "source": [
    "# 3) build a flat DataFrame of all (utterance_id, user_id, pred, truth)\n",
    "rows = []\n",
    "for rec in prescreen_data:\n",
    "    for t in rec['target_utterances']:\n",
    "        qid  = int(t['utterance_id'])\n",
    "        pred = t['labels']['informativeness']\n",
    "        true = ground_truth_ans[str(qid)]\n",
    "        rows.append({'utterance_id': qid,\n",
    "                     'user_id'     : rec['user_id'],\n",
    "                     'pred'        : pred,\n",
    "                     'truth'       : true})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 4) compute the per-annotation weight\n",
    "maxd = df['truth'].max() - df['truth'].min()  # should be 3 for labels 1–4\n",
    "df['w_acc'] = 1 - (df.eval('abs(pred - truth)') / maxd)\n",
    "\n",
    "# 5) group by utterance_id to get adjusted accuracy\n",
    "breakdown = df.groupby('utterance_id').agg(\n",
    "    n_annotators     = ('user_id', 'nunique'),\n",
    "    adjusted_accuracy = ('w_acc',  'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b2d810-8f44-4465-8647-09f731c02fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance 101 text:\n",
      "It’s misleading to suggest that <strong>aid</strong> inherently undermines <strong>accountability</strong>. Take the substantial reductions in <strong>HIV/AIDS</strong> cases, for instance—<strong>aid</strong> supported government-run programs that dramatically reduced the spread of the disease. What matters is how <strong>aid</strong> is designed and monitored. Blanket critiques overlook the difference between strategic investment and careless spending.\n",
      "\n",
      "Ground truth informativeness: 2\n",
      "\n",
      "Annotator votes:\n",
      " - 67edb75b0744561f1cf6ff9d: 4\n",
      " - 677c4ac5878568e38dae405a: 4\n",
      " - 67aa21a12b1a6e9475bc61b2: 4\n",
      " - Nakita: 4\n",
      " - 63af557b3d4f219c3226b7d6: 2\n",
      " - 654455ad0b76675379c66db4: 4\n"
     ]
    }
   ],
   "source": [
    "print_annotation_details(\n",
    "    utt_id=101,\n",
    "    records=prescreen_data,\n",
    "    ground_truth=ground_truth_ans,\n",
    "    utterances=ground_truth[\"target_utterances\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a36be-5ef6-4022-98c4-617604920220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
